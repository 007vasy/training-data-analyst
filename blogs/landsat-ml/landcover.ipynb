{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Creating high-resolution Landcover data using Machine Learning </h1>\n",
    "\n",
    "In this notebook, we train a TensorFlow model to fit Landsat 8 bands to a low-resolution landcover map. Then, we use that model on the high-resolution Landsat data to create a high-resolution landcover map. In essence, we are using TensorFlow to <a href=\"https://gisclimatechange.ucar.edu/question/63\">statistically downscale</a> the landcover data (note that the term \"downscaling\" is counterintuitive -- downscaling an image increases its resolution or upsamples it).\n",
    "\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing using Cloud Dataflow </h2>\n",
    "\n",
    "Cloud Dataflow can scale up and simplify preprocessing in Cloud ML.  We'll need to read the Geotiffs and then merge them in such a way that all the data corresponding to a pixel becomes a single TFRecord. We'll also need to scale the pixel values to lie in the range [0,1]. If you do this sort of thing naively, you'll run out of memory or burn through your wallet -- the total size of the images alone is 25 GB.  Trying to fit it all in memory would require machines with 3-4 times more RAM.\n",
    "\n",
    "We'll use Cloud Dataflow to distribute the preprocessing onto an autoscaled cluster of machines with 3 GB of RAM each.  (Note: change 'nrows' to something like 3 to get a small dataset to work with, and make sure that the number of workers is within your quota for simultaneous Compute Engine VMs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml/sdk/cloudml-0.1.6-alpha.dataflow.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Direct usage of TextFileSink is deprecated. Please use 'textio.WriteToText()' instead of directly instantiating a TextFileSink object.\n"
     ]
    }
   ],
   "source": [
    "# a Python generator that packs all the training data line-by-line\n",
    "def get_next_line(x):\n",
    "  '''\n",
    "      return (lineno, linedata, featnames)\n",
    "      where linedata is a 2D array with first dimension being feature# and second dimension column in image \n",
    "  '''  \n",
    "  import osgeo.gdal as gdal\n",
    "  import struct\n",
    "  import os\n",
    "  import subprocess\n",
    "  \n",
    "  # The gdal library can not read from CloudStorage, so this class downloads the data to local VM\n",
    "  class LandsatReader():\n",
    "   def __init__(self, gsfile, destdir='./'):\n",
    "      self.gsfile = gsfile\n",
    "      self.dest = os.path.join(destdir, os.path.basename(self.gsfile))\n",
    "      if os.path.exists(self.dest):\n",
    "        print 'Using already existing {}'.format(self.dest)\n",
    "      else:\n",
    "        print 'Getting {0} to {1} '.format(self.gsfile, self.dest)\n",
    "        ret = subprocess.check_call(['gsutil', 'cp', self.gsfile, self.dest])\n",
    "      self.dataset = gdal.Open( self.dest, gdal.GA_ReadOnly )\n",
    "   def __exit__(self, exc_type=None, exc_val=None, exc_tb=None):\n",
    "      os.remove( self.dest ) # cleanup  \n",
    "   def ds(self):\n",
    "      return self.dataset\n",
    "\n",
    "  # open all the necessary files\n",
    "  input_dir = 'gs://mdh-test/landsat-ml/'\n",
    "  featnames = ['b{}'.format(band) for band in xrange(1,8)] # 8\n",
    "  filenames = [os.path.join(input_dir, 'landsat8-{}.tif'.format(band)) for band in featnames]\n",
    "  filenames.append(os.path.join(input_dir, 'srtm-elevation.tif')); featnames.append('elev')\n",
    "  filenames.append(os.path.join(input_dir, 'mcd12-labels.tif')); featnames.append('landcover')\n",
    "  readers = [LandsatReader(filename) for filename in filenames]\n",
    "  bands = [reader.ds().GetRasterBand(1) for reader in readers] \n",
    "  print \"Opened \", filenames\n",
    "      \n",
    "  # read one row of each the images and yield them\n",
    "  ncols = bands[0].XSize\n",
    "  nrows = bands[0].YSize\n",
    "  print \"Reading \", nrows, \"x\", ncols, \" images corresponding to \", featnames\n",
    "  packformat = 'f' * ncols\n",
    "  for line in xrange(0, nrows):  #WARN! Change 'nrows' here to 10 to get a small (0.05% of whole) dataset to work with.\n",
    "        line_data = [struct.unpack(packformat, band.ReadRaster(0, line, ncols, 1, ncols, 1, gdal.GDT_Float32)) for band in bands]\n",
    "        yield (line, line_data, featnames)\n",
    "      \n",
    "def get_features_from_line(args):\n",
    "  '''\n",
    "      return (1, dict)  or (0, dict)\n",
    "      where the first number is 1 or 0 depending on whether this row belongs to training (1)\n",
    "      or eval (0) partition.\n",
    "      dict is the set of features formed from pixels from all the bands\n",
    "  ''' \n",
    "  (line, line_data, featnames) = args\n",
    "  if line_data:\n",
    "    ncols = len(line_data[0])\n",
    "    for col in xrange(0, ncols): # ncols\n",
    "          featdict = {'rowcol': '{},{}'.format(line,col)}\n",
    "          for f in xrange(0, len(featnames)):\n",
    "            featdict[featnames[f]] = line_data[f][col]\n",
    "          featdict['landcover'] = '{}'.format(int(featdict['landcover']+0.5))\n",
    "          yield ( 0 if (line+col)%3==0 else 1, featdict )    # 1/3 are eval\n",
    "\n",
    "def get_partition(group_and_featdict, nparts):\n",
    "  (is_train, featdict) = group_and_featdict\n",
    "  return is_train # 0 or 1\n",
    "\n",
    "def get_featdict(group_and_featdict):\n",
    "  (is_train, featdict) = group_and_featdict\n",
    "  return featdict\n",
    "\n",
    "def run():\n",
    "  import os\n",
    "  import numpy as np\n",
    "  import apache_beam as beam\n",
    "  import google.cloud.ml as ml\n",
    "  import google.cloud.ml.io as io\n",
    "  import google.cloud.ml.features as features\n",
    "\n",
    "  # Change as needed\n",
    "  BUCKET = 'cloud-training-demos-ml'\n",
    "  PROJECT = 'cloud-training-demos' \n",
    "  OUTPUT_DIR = 'gs://{0}/landcover/preproc'.format(BUCKET); RUNNER = 'DataflowPipelineRunner'\n",
    "  #OUTPUT_DIR = './preproc'; RUNNER = 'DirectPipelineRunner'\n",
    "  \n",
    "  pipeline = beam.Pipeline(argv=['--project', PROJECT,\n",
    "                               '--runner', RUNNER,\n",
    "                               '--job_name', 'landcover',\n",
    "                               '--extra_package', ml.sdk_location,\n",
    "                               '--max_num_workers', '10',\n",
    "                               '--no_save_main_session', 'True',  # to prevent pickling and uploading Datalab itself!\n",
    "                               '--setup_file', './preproc/setup.py',  # for gdal installation on the cloud -- see CUSTOM_COMMANDS in setup.py\n",
    "                               '--staging_location', 'gs://{0}/landcover/staging'.format(BUCKET),\n",
    "                               '--temp_location', 'gs://{0}/landcover/temp'.format(BUCKET)])\n",
    "        \n",
    "  print ml.sdk_location\n",
    "  (evalg, traing) = (pipeline \n",
    "     | beam.Create([0]) # make the generator function like a source\n",
    "     | beam.FlatMap(get_next_line)\n",
    "     | beam.FlatMap(get_features_from_line) # (is_train, featdict)\n",
    "     | beam.Partition(get_partition, 2)\n",
    "  )  # eval, train both contain (is_train, featdict)\n",
    "  eval = evalg | 'eval_features' >> beam.Map(get_featdict)\n",
    "  train = traing | 'train_features' >> beam.Map(get_featdict)\n",
    "  \n",
    "  class LandcoverFeatures(object):\n",
    "    columns = ('rowcol', 'b1', 'b2', 'landcover')\n",
    "    key = features.key('rowcol')\n",
    "    landcover = features.target('landcover').discrete()  # classification problem\n",
    "    inputbands = [\n",
    "      features.numeric('b1').scale(),\n",
    "      features.numeric('b2').scale(),\n",
    "      features.numeric('b3').scale(),\n",
    "      features.numeric('b4').scale(),\n",
    "      features.numeric('b5').scale(),\n",
    "      features.numeric('b6').scale(),\n",
    "      features.numeric('b7').scale(),\n",
    "      #features.numeric('el').discretize(buckets=[1,5001,50], sparse=True),  # elevation\n",
    "    ]\n",
    "  feature_set = LandcoverFeatures()\n",
    "  (metadata, train_features, eval_features) = ((train, eval) |\n",
    "   'Preprocess' >> ml.Preprocess(feature_set))\n",
    "  (metadata\n",
    "     | 'SaveMetadata'\n",
    "     >> io.SaveMetadata(os.path.join(OUTPUT_DIR, 'metadata.yaml')))\n",
    "  (train_features\n",
    "     | 'WriteTraining'\n",
    "     >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_train')))\n",
    "  (eval_features\n",
    "     | 'WriteEval'\n",
    "     >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_eval')))\n",
    "  pipeline.run()\n",
    "  \n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos-ml/landcover/preproc/features_eval-00000-of-00003.tfrecord.gz\r\n",
      "gs://cloud-training-demos-ml/landcover/preproc/features_eval-00001-of-00003.tfrecord.gz\r\n",
      "gs://cloud-training-demos-ml/landcover/preproc/features_eval-00002-of-00003.tfrecord.gz\r\n",
      "gs://cloud-training-demos-ml/landcover/preproc/features_train-00000-of-00003.tfrecord.gz\r\n",
      "gs://cloud-training-demos-ml/landcover/preproc/features_train-00001-of-00003.tfrecord.gz\r\n",
      "gs://cloud-training-demos-ml/landcover/preproc/features_train-00002-of-00003.tfrecord.gz\r\n",
      "gs://cloud-training-demos-ml/landcover/preproc/metadata.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://cloud-training-demos-ml/landcover/preproc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create ML model using TensorFlow </h2>\n",
    "\n",
    "I cheated here. I simply took the <a href=\"https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/iris\">Cloud ML sample for Iris classification</a> and copied it into my repo.  The only change I had to make was to three fields, changing:\n",
    "<pre>\n",
    "KEY_FEATURE_COLUMN = 'key'\n",
    "TARGET_FEATURE_COLUMN = 'species'\n",
    "REAL_VALUED_FEATURE_COLUMNS = 'measurements'\n",
    "</pre>\n",
    "to\n",
    "<pre>\n",
    "KEY_FEATURE_COLUMN = 'key'\n",
    "TARGET_FEATURE_COLUMN = 'landcover'\n",
    "REAL_VALUED_FEATURE_COLUMNS = 'inputbands'\n",
    "</pre>\n",
    "Essentially, my new values match what I had in the class LandcoverFeatures during preprocessing (see above).  This is needed because that's what now encoded in the tfrecord files the preprocessing step wrote out.\n",
    "\n",
    "The model itself is a neural network with 2 hidden layers. The Iris sample uses the tf.learn API. It is a classification network, and the sample does all the saving, exporting, distribution, etc. I'm not going to worry too much about it. The samples are quite useful in that way. You can use the Iris sample for classification and the Census sample for regression -- you won't have to change much provided your inputs are similar. In my case, all my inputs are like the Iris sample in that they are all real-valued columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landcover:\r\n",
      "total 8\r\n",
      "-rw-r--r-- 1 root root  746 Nov  3 18:28 setup.py\r\n",
      "drwxr-xr-x 2 root root 4096 Nov  3 23:07 trainer\r\n",
      "\r\n",
      "landcover/trainer:\r\n",
      "total 24\r\n",
      "-rw-r--r-- 1 root root  677 Nov  3 21:54 __init__.py\r\n",
      "-rw-r--r-- 1 root root 9176 Nov  3 23:07 task.py\r\n",
      "-rw-r--r-- 1 root root 5553 Nov  3 22:51 util.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lR landcover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train model using Cloud ML </h2>\n",
    "\n",
    "Let's train the model locally on a subset of the data to ensure that we get things right. Then, we can train on the cloud with all of the data.\n",
    "\n",
    "<br/>\n",
    "<h4> Local training </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landcover/\n",
      "landcover/trainer/\n",
      "landcover/trainer/task.py\n",
      "landcover/trainer/util.py\n",
      "landcover/trainer/__init__.py\n",
      "landcover/setup.py\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf /content/training-data-analyst/blogs/landsat-ml/landcover_trained\n",
    "tar cvfz landcover.tgz landcover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master: WARNING:tensorflow:Please specify metrics using MetricSpec. Using bare functions or (key, fn) tuples is deprecated and support for it will be removed on Oct 1, 2016.<br/>master: WARNING:tensorflow:Please specify metrics using MetricSpec. Using bare functions or (key, fn) tuples is deprecated and support for it will be removed on Oct 1, 2016.<br/>master: <br/>master: INFO:tensorflow:Restored model from /content/training-data-analyst/blogs/landsat-ml/landcover_trained/train<br/>master: INFO:tensorflow:Eval steps [0,100) for training step 1000.<br/>master: INFO:tensorflow:Results after 10 steps (0.003 sec/batch): loss = 0.742407, training/hptuning/metric = 0.776667, accuracy = 0.776667.<br/>master: INFO:tensorflow:Results after 20 steps (0.001 sec/batch): loss = 0.663186, training/hptuning/metric = 0.771667, accuracy = 0.771667.<br/>master: INFO:tensorflow:Results after 30 steps (0.001 sec/batch): loss = 0.495592, training/hptuning/metric = 0.844444, accuracy = 0.844444.<br/>master: INFO:tensorflow:Results after 40 steps (0.001 sec/batch): loss = 0.42915, training/hptuning/metric = 0.8675, accuracy = 0.8675.<br/>master: INFO:tensorflow:Results after 50 steps (0.001 sec/batch): loss = 0.496906, training/hptuning/metric = 0.828667, accuracy = 0.828667.<br/>master: INFO:tensorflow:Results after 60 steps (0.001 sec/batch): loss = 0.524924, training/hptuning/metric = 0.801667, accuracy = 0.801667.<br/>master: INFO:tensorflow:Results after 70 steps (0.001 sec/batch): loss = 0.529801, training/hptuning/metric = 0.799524, accuracy = 0.799524.<br/>master: INFO:tensorflow:Results after 80 steps (0.001 sec/batch): loss = 0.546676, training/hptuning/metric = 0.780833, accuracy = 0.780833.<br/>master: INFO:tensorflow:Results after 90 steps (0.001 sec/batch): loss = 0.547263, training/hptuning/metric = 0.783704, accuracy = 0.783704.<br/>master: INFO:tensorflow:Results after 100 steps (0.001 sec/batch): loss = 0.543411, training/hptuning/metric = 0.785, accuracy = 0.785.<br/>master: W tensorflow/core/kernels/queue_base.cc:294] _7_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed<br/>master: W tensorflow/core/kernels/queue_base.cc:294] _7_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed<br/>master: W tensorflow/core/kernels/queue_base.cc:294] _6_input_producer: Skipping cancelled enqueue attempt with queue not closed<br/>master: INFO:tensorflow:Saving evaluation summary for 1000 step: loss = 0.543411, training/hptuning/metric = 0.785, accuracy = 0.785<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mlalpha train\n",
    "package_uris: /content/training-data-analyst/blogs/landsat-ml/landcover.tgz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths: gs://cloud-training-demos-ml/landcover/preproc/features_train-0000*\n",
    "  eval_data_paths: gs://cloud-training-demos-ml/landcover/preproc/features_eval-0000*\n",
    "  metadata_path: gs://cloud-training-demos-ml/landcover/preproc/metadata.yaml\n",
    "  output_path: /content/training-data-analyst/blogs/landsat-ml/landcover_trained\n",
    "  max_steps:  1000\n",
    "  batch_size: 1000\n",
    "  layer1_size: 30\n",
    "  layer2_size: 10\n",
    "  learning_rate: 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2016 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
