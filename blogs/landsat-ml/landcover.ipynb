{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Creating high-resolution Landcover data using Machine Learning </h1>\n",
    "\n",
    "In this notebook, we train a TensorFlow model to fit Landsat 8 bands to a low-resolution landcover map. Then, we use that model on the high-resolution Landsat data to create a high-resolution landcover map. In essence, we are using TensorFlow to <a href=\"https://gisclimatechange.ucar.edu/question/63\">statistically downscale</a> the landcover data (note that the term \"downscaling\" is counterintuitive -- downscaling an image increases its resolution or upsamples it).\n",
    "\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "<h2> Workflow </h2>\n",
    "We will read corresponding pixels out of a set of mosaiced-and-cloud-corrected Landsat GeoTiff images and correlate them with a low-resolution landcover map that has been upsampled to match the Landsat imagery.  This dataset of pixel values is what is used in training.  In prediction, we take the same set of Landsat images and use the trained model to come up with a high-resolution landcover map.\n",
    "\n",
    "This is the basic workflow:\n",
    "<img src=\"landcover_features.png\" style='width: 100%;' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing using Cloud Dataflow </h2>\n",
    "\n",
    "Cloud Dataflow can scale up and simplify preprocessing in Cloud ML.  We'll need to read the Geotiffs and then merge them in such a way that all the data corresponding to a pixel becomes a single TFRecord. We'll scale the pixel values to lie in the range [-1,1]. If you do this sort of thing naively, you'll run out of memory or burn through your wallet -- the total size of the images alone is 25 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a Python generator that packs all the training data line-by-line\n",
    "def get_next_line(SMALL_SAMPLE):\n",
    "  '''\n",
    "      return (lineno, linedata, featnames)\n",
    "      where linedata is a 2D array with first dimension being feature# and second dimension column in image \n",
    "  '''  \n",
    "  import osgeo.gdal as gdal\n",
    "  import struct\n",
    "  import os\n",
    "  import subprocess\n",
    "  \n",
    "  # The gdal library can not read from CloudStorage, so this class downloads the data to local VM\n",
    "  class LandsatReader():\n",
    "   def __init__(self, gsfile, destdir='./'):\n",
    "      self.gsfile = gsfile\n",
    "      self.dest = os.path.join(destdir, os.path.basename(self.gsfile))\n",
    "      if os.path.exists(self.dest):\n",
    "        print 'Using already existing {}'.format(self.dest)\n",
    "      else:\n",
    "        print 'Getting {0} to {1} '.format(self.gsfile, self.dest)\n",
    "        ret = subprocess.check_call(['gsutil', 'cp', self.gsfile, self.dest])\n",
    "      self.dataset = gdal.Open( self.dest, gdal.GA_ReadOnly )\n",
    "   def __exit__(self, exc_type=None, exc_val=None, exc_tb=None):\n",
    "      os.remove( self.dest ) # cleanup  \n",
    "   def ds(self):\n",
    "      return self.dataset\n",
    "\n",
    "  # open all the necessary files\n",
    "  input_dir = 'gs://mdh-test/landsat-ml/'\n",
    "  featnames = ['b{}'.format(band) for band in xrange(1,8)] # 8\n",
    "  filenames = [os.path.join(input_dir, 'landsat8-{}.tif'.format(band)) for band in featnames]\n",
    "  filenames.append(os.path.join(input_dir, 'srtm-elevation.tif')); featnames.append('elev')\n",
    "  filenames.append(os.path.join(input_dir, 'mcd12-labels.tif')); featnames.append('landcover')\n",
    "  readers = [LandsatReader(filename) for filename in filenames]\n",
    "  bands = [reader.ds().GetRasterBand(1) for reader in readers] \n",
    "  print \"Opened \", filenames\n",
    "      \n",
    "  # read one row of each the images and yield them\n",
    "  ncols = bands[0].XSize\n",
    "  nrows = bands[0].YSize\n",
    "  if SMALL_SAMPLE:\n",
    "    nrows_to_read = 200\n",
    "    ncols_to_read = 1000\n",
    "  else:\n",
    "    nrows_to_read = nrows\n",
    "    ncols_to_read = ncols\n",
    "  print \"Reading \", nrows_to_read, \"x\", ncols_to_read, \" from \", nrows, 'x', ncols, ' images corresponding to ', featnames\n",
    "  packformat = 'f' * ncols\n",
    "  for line in xrange(0, nrows_to_read):\n",
    "        line_data = [struct.unpack(packformat, band.ReadRaster(0, line, ncols, 1, ncols, 1, gdal.GDT_Float32)) for band in bands]\n",
    "        yield (line, (line_data, featnames, ncols_to_read))\n",
    "      \n",
    "def get_features_from_line(args):\n",
    "  '''\n",
    "      return (1, dict)  or (0, dict)\n",
    "      where the first number is 1 or 0 depending on whether this row belongs to training (1)\n",
    "      or eval (0) partition.\n",
    "      dict is the set of features formed from pixels from all the bands\n",
    "  ''' \n",
    "  # line, [(line_data, featnames, ncols_to_read)] = args\n",
    "  line = args[0]\n",
    "  for (line_data, featnames, ncols_to_read) in args[1]:\n",
    "    if line_data:\n",
    "       for col in xrange(0, ncols_to_read):\n",
    "          featdict = {'rowcol': '{},{}'.format(line,col)}\n",
    "          for f in xrange(0, len(featnames)):\n",
    "            featdict[featnames[f]] = line_data[f][col]\n",
    "          featdict['landcover'] = '{}'.format(int(featdict['landcover']+0.5))\n",
    "          yield ( 0 if (line+col)%3==0 else 1, featdict )    # 1/3 are eval\n",
    "\n",
    "def get_partition(group_and_featdict, nparts):\n",
    "  (is_train, featdict) = group_and_featdict\n",
    "  return is_train # 0 or 1\n",
    "\n",
    "def get_featdict(group_and_featdict):\n",
    "  (is_train, featdict) = group_and_featdict\n",
    "  return featdict\n",
    "\n",
    "def run_preprocessing(BUCKET=None, PROJECT=None):\n",
    "  import os\n",
    "  import numpy as np\n",
    "  import apache_beam as beam\n",
    "  import google.cloud.ml as ml\n",
    "  import google.cloud.ml.io as io\n",
    "  import google.cloud.ml.features as features\n",
    "\n",
    "  # small sample locally; full dataset on cloud\n",
    "  if BUCKET is None or PROJECT is None:\n",
    "    SMALL_SAMPLE = True\n",
    "    OUTPUT_DIR = './landcover_preproc'\n",
    "    RUNNER = 'DirectPipelineRunner'\n",
    "  else:\n",
    "    SMALL_SAMPLE = False\n",
    "    OUTPUT_DIR = 'gs://{0}/landcoverml/preproc'.format(BUCKET)\n",
    "    RUNNER = 'DataflowPipelineRunner'\n",
    "  #\n",
    "  \n",
    "  pipeline = beam.Pipeline(argv=['--project', PROJECT,\n",
    "                               '--runner', RUNNER,\n",
    "                               '--job_name', 'landcover',\n",
    "                               '--extra_package', ml.sdk_location,\n",
    "                               '--max_num_workers', '50',\n",
    "                               '--no_save_main_session', 'True',  # to prevent pickling and uploading Datalab itself!\n",
    "                               '--setup_file', './preproc/setup.py',  # for gdal installation on the cloud -- see CUSTOM_COMMANDS in setup.py\n",
    "                               '--staging_location', 'gs://{0}/landcoverml/staging'.format(BUCKET),\n",
    "                               '--temp_location', 'gs://{0}/landcoverml/temp'.format(BUCKET)])\n",
    "        \n",
    "  print ml.sdk_location\n",
    "  \n",
    "  (evalg, traing) = (pipeline \n",
    "     | beam.Create([SMALL_SAMPLE]) # make the generator function like a source\n",
    "     | beam.FlatMap(get_next_line) # (line, (line_data, featnames, ncols_to_read))\n",
    "     | beam.GroupByKey() # line, [(line_data, featnames, ncols_to_read)]\n",
    "     | beam.FlatMap(get_features_from_line) # (is_train, featdict)\n",
    "     | beam.Partition(get_partition, 2)\n",
    "  )  # eval, train both contain (is_train, featdict)\n",
    "  eval = evalg | 'eval_features' >> beam.Map(get_featdict)\n",
    "  train = traing | 'train_features' >> beam.Map(get_featdict)\n",
    "  \n",
    "  class LandcoverFeatures(object):\n",
    "    key = features.key('rowcol')\n",
    "    landcover = features.target('landcover').discrete()  # classification problem\n",
    "    inputbands = [\n",
    "      features.numeric('b1').scale(),\n",
    "      features.numeric('b2').scale(),\n",
    "      features.numeric('b3').scale(),\n",
    "      features.numeric('b4').scale(),\n",
    "      features.numeric('b5').scale(),\n",
    "      features.numeric('b6').scale(),\n",
    "      features.numeric('b7').scale(),\n",
    "      #features.numeric('el').discretize(buckets=[1,5001,50], sparse=True),  # elevation\n",
    "    ]\n",
    "  feature_set = LandcoverFeatures()\n",
    "  (metadata, train_features, eval_features) = ((train, eval) |\n",
    "   'Preprocess' >> ml.Preprocess(feature_set, input_format='json'))\n",
    "  (metadata\n",
    "     | 'SaveMetadata'\n",
    "     >> io.SaveMetadata(os.path.join(OUTPUT_DIR, 'metadata.yaml')))\n",
    "  (train_features\n",
    "     | 'WriteTraining'\n",
    "     >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_train')))\n",
    "  (eval_features\n",
    "     | 'WriteEval'\n",
    "     >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_eval')))\n",
    "  pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create ML model using TensorFlow </h2>\n",
    "\n",
    "I cheated here. I simply took the <a href=\"https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/iris\">Cloud ML sample for Iris classification</a> and copied it into my repo.  The only change I had to make was to three fields, changing:\n",
    "<pre>\n",
    "KEY_FEATURE_COLUMN = 'key'\n",
    "TARGET_FEATURE_COLUMN = 'species'\n",
    "REAL_VALUED_FEATURE_COLUMNS = 'measurements'\n",
    "</pre>\n",
    "to\n",
    "<pre>\n",
    "KEY_FEATURE_COLUMN = 'key'\n",
    "TARGET_FEATURE_COLUMN = 'landcover'\n",
    "REAL_VALUED_FEATURE_COLUMNS = 'inputbands'\n",
    "</pre>\n",
    "Essentially, my new values match what I had in the class LandcoverFeatures during preprocessing (see above).  This is needed because that's what now encoded in the tfrecord files the preprocessing step wrote out.\n",
    "\n",
    "The model itself is a neural network with 2 hidden layers. The Iris sample uses the tf.learn API. It is a classification network, and the sample does all the saving, exporting, distribution, etc. All my inputs are like the Iris sample in that they are all real-valued columns. Like in the Iris example, the target takes only one value -- a landcover that is brushland can not also be forest. So, I'm relatively safe in reusing the Iris model as-is.  Of course, I should probably do some feature engineering, by calculating normalized differences, for example. But for now, the Iris sample will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landcover:\r\n",
      "total 8\r\n",
      "-rw-r--r-- 1 root root  746 Nov  3 18:28 setup.py\r\n",
      "drwxr-xr-x 2 root root 4096 Nov  3 23:07 trainer\r\n",
      "\r\n",
      "landcover/trainer:\r\n",
      "total 24\r\n",
      "-rw-r--r-- 1 root root  677 Nov  3 21:54 __init__.py\r\n",
      "-rw-r--r-- 1 root root 9176 Nov  3 23:07 task.py\r\n",
      "-rw-r--r-- 1 root root 5553 Nov  3 22:51 util.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lR landcover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train model locally using Cloud ML </h2>\n",
    "\n",
    "Let's train the model locally on a subset of the data to ensure that we get things right. Then, we can train on the cloud with all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml/sdk/cloudml-0.1.6-alpha.dataflow.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Direct usage of TextFileSink is deprecated. Please use 'textio.WriteToText()' instead of directly instantiating a TextFileSink object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using already existing ./landsat8-b1.tif\n",
      "Using already existing ./landsat8-b2.tif\n",
      "Using already existing ./landsat8-b3.tif\n",
      "Using already existing ./landsat8-b4.tif\n",
      "Using already existing ./landsat8-b5.tif\n",
      "Using already existing ./landsat8-b6.tif\n",
      "Using already existing ./landsat8-b7.tif\n",
      "Using already existing ./srtm-elevation.tif\n",
      "Using already existing ./mcd12-labels.tif\n",
      "Opened  ['gs://mdh-test/landsat-ml/landsat8-b1.tif', 'gs://mdh-test/landsat-ml/landsat8-b2.tif', 'gs://mdh-test/landsat-ml/landsat8-b3.tif', 'gs://mdh-test/landsat-ml/landsat8-b4.tif', 'gs://mdh-test/landsat-ml/landsat8-b5.tif', 'gs://mdh-test/landsat-ml/landsat8-b6.tif', 'gs://mdh-test/landsat-ml/landsat8-b7.tif', 'gs://mdh-test/landsat-ml/srtm-elevation.tif', 'gs://mdh-test/landsat-ml/mcd12-labels.tif']\n",
      "Reading  200 x 1000  from  16384 x 16384  images corresponding to  ['b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'elev', 'landcover']\n"
     ]
    }
   ],
   "source": [
    "# process a small sample (200k points) by running the preprocessing locally\n",
    "# if your Datalab instance can't handle this, reduce the sample size by changing the preprocessing code\n",
    "# (look for nrows_read and change it from 200 to perhaps 20)\n",
    "# alternately, if you followed the codelab instructions to launch Datalab on a GCE, change the machine\n",
    "# type in instance_details.sh to n1-highmem-2   (should take about 5 minutes on n1-highmem-2)\n",
    "import shutil\n",
    "shutil.rmtree('landcover_preproc', ignore_errors=True)\n",
    "run_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 7004\r\n",
      "-rw-r--r-- 1 root root 2421530 Dec  6 21:17 features_eval-00000-of-00001.tfrecord.gz\r\n",
      "-rw-r--r-- 1 root root 4739884 Dec  6 21:17 features_train-00000-of-00001.tfrecord.gz\r\n",
      "-rw-r--r-- 1 root root    2099 Dec  6 21:16 metadata.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /content/training-data-analyst/blogs/landsat-ml/landcover_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landcover/\n",
      "landcover/trainer/\n",
      "landcover/trainer/task.py\n",
      "landcover/trainer/util.py\n",
      "landcover/trainer/__init__.py\n",
      "landcover/setup.py\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf /content/training-data-analyst/blogs/landsat-ml/landcover_trained\n",
    "tar cvfz landcover.tgz landcover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master: INFO:tensorflow:Transforming feature_column _RealValuedColumn(column_name='inputbands', dimension=7, default_value=None, dtype=tf.float32, normalizer=None)<br/>master: WARNING:tensorflow:Please specify metrics using MetricSpec. Using bare functions or (key, fn) tuples is deprecated and support for it will be removed on Oct 1, 2016.<br/>master: WARNING:tensorflow:Please specify metrics using MetricSpec. Using bare functions or (key, fn) tuples is deprecated and support for it will be removed on Oct 1, 2016.<br/>master: INFO:tensorflow:Restored model from /content/training-data-analyst/blogs/landsat-ml/landcover_trained/train<br/>master: INFO:tensorflow:Eval steps [0,100) for training step 2000.<br/>master: INFO:tensorflow:Results after 10 steps (0.003 sec/batch): loss = 0.86316, training/hptuning/metric = 0.68, accuracy = 0.68.<br/>master: INFO:tensorflow:Results after 20 steps (0.001 sec/batch): loss = 0.938792, training/hptuning/metric = 0.641667, accuracy = 0.641667.<br/>master: INFO:tensorflow:Results after 30 steps (0.001 sec/batch): loss = 0.936572, training/hptuning/metric = 0.655556, accuracy = 0.655556.<br/>master: INFO:tensorflow:Results after 40 steps (0.001 sec/batch): loss = 0.89947, training/hptuning/metric = 0.664167, accuracy = 0.664167.<br/>master: INFO:tensorflow:Results after 50 steps (0.001 sec/batch): loss = 0.903345, training/hptuning/metric = 0.666, accuracy = 0.666.<br/>master: INFO:tensorflow:Results after 60 steps (0.001 sec/batch): loss = 0.894385, training/hptuning/metric = 0.673889, accuracy = 0.673889.<br/>master: INFO:tensorflow:Results after 70 steps (0.001 sec/batch): loss = 0.897772, training/hptuning/metric = 0.67, accuracy = 0.67.<br/>master: INFO:tensorflow:Results after 80 steps (0.001 sec/batch): loss = 0.892392, training/hptuning/metric = 0.67, accuracy = 0.67.<br/>master: INFO:tensorflow:Results after 90 steps (0.001 sec/batch): loss = 0.894256, training/hptuning/metric = 0.667778, accuracy = 0.667778.<br/>master: INFO:tensorflow:Results after 100 steps (0.001 sec/batch): loss = 0.892895, training/hptuning/metric = 0.665667, accuracy = 0.665667.<br/>master: W tensorflow/core/kernels/queue_base.cc:294] _9_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed<br/>master: W tensorflow/core/kernels/queue_base.cc:294] _9_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed<br/>master: W tensorflow/core/kernels/queue_base.cc:294] _11_input_producer: Skipping cancelled enqueue attempt with queue not closed<br/>master: INFO:tensorflow:Saving evaluation summary for 2000 step: loss = 0.892895, training/hptuning/metric = 0.665667, accuracy = 0.665667<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mlalpha train\n",
    "package_uris: /content/training-data-analyst/blogs/landsat-ml/landcover.tgz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths: /content/training-data-analyst/blogs/landsat-ml/landcover_preproc/features_train-*\n",
    "  eval_data_paths: /content/training-data-analyst/blogs/landsat-ml/landcover_preproc/features_eval-*\n",
    "  metadata_path: /content/training-data-analyst/blogs/landsat-ml/landcover_preproc/metadata.yaml\n",
    "  output_path: /content/training-data-analyst/blogs/landsat-ml/landcover_trained\n",
    "  max_steps:  2000\n",
    "  batch_size: 10000\n",
    "  layer1_size: 30\n",
    "  layer2_size: 10\n",
    "  learning_rate: 0.01\n",
    "  min_eval_frequency: 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Predict with locally trained model</h3>\n",
    "\n",
    "We can use the preprocessed features to evaluate how well the trained model performs. The evaluation workflow will use the model for prediction, so if we save the predictions from the model when it is being evaluated, we can use those predictions for downscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_eval-00000-of-00001.tfrecord.gz   metadata.yaml\r\n",
      "features_train-00000-of-00001.tfrecord.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls /content/training-data-analyst/blogs/landsat-ml/landcover_preproc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Direct usage of TextFileSink is deprecated. Please use 'textio.WriteToText()' instead of directly instantiating a TextFileSink object.\n",
      "WARNING:root:Direct usage of TextFileSink is deprecated. Please use 'textio.WriteToText()' instead of directly instantiating a TextFileSink object.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.direct_runner.DirectPipelineResult at 0x7f3fd40eff50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.analysis as analysis\n",
    "import google.cloud.ml.io as io\n",
    "import json\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = '/content/training-data-analyst/blogs/landsat-ml/landcover_eval'\n",
    "pipeline = beam.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "eval_features = (pipeline | 'ReadEval' >> io.LoadFeatures('/content/training-data-analyst/blogs/landsat-ml/landcover_preproc/features_eval*'))\n",
    "trained_model = pipeline | 'LoadModel' >> io.LoadModel('/content/training-data-analyst/blogs/landsat-ml/landcover_trained/model')\n",
    "evaluations = (eval_features | 'Evaluate' >> ml.Evaluate(trained_model) |\n",
    "    beam.Map('ExtractEvaluationResults', lambda (example, prediction): prediction))\n",
    "eval_data_sink = beam.io.TextFileSink(os.path.join(OUTPUT_DIR, 'eval'), shard_name_template='')\n",
    "evaluations | beam.io.textio.WriteToText(os.path.join(OUTPUT_DIR, 'eval'), shard_name_template='')\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'score': [0.9201728701591492, 0.005093369632959366, 6.252119055716321e-05, 0.011211490258574486, 0.055562522262334824, 0.007897143252193928], u'target': '1', u'key': '0,0', u'label': '1'}\r\n",
      "{u'score': [0.9550836682319641, 0.0030277296900749207, 2.1692805603379384e-05, 0.009645896032452583, 0.02988329716026783, 0.0023376329336315393], u'target': '1', u'key': '0,3', u'label': '1'}\r\n"
     ]
    }
   ],
   "source": [
    "!head -2 /content/training-data-analyst/blogs/landsat-ml/landcover_eval/eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output includes the key (the pixel location), and the label (the prediction) for that pixel. That is enough for us to be able to do the downscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Using fallback coder for typehint: <type 'function'>.\n",
      "WARNING:root:Direct usage of TextFileSink is deprecated. Please use 'textio.WriteToText()' instead of directly instantiating a TextFileSink object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '10': 1, '12': 2, '5': 3, '9': 5, '8': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.direct_runner.DirectPipelineResult at 0x7f400b226290>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.analysis as analysis\n",
    "import google.cloud.ml.io as io\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUTPUT_DIR = '/content/training-data-analyst/blogs/landsat-ml/landcover_eval'\n",
    "pipeline = beam.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "# analysis\n",
    "def read_metadata(filename):\n",
    "  with open(filename, 'r') as stream:\n",
    "    try:\n",
    "        return yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "metadata = read_metadata('/content/training-data-analyst/blogs/landsat-ml/landcover_preproc/metadata.yaml')\n",
    "lookup = metadata['columns']['landcover']['vocab']\n",
    "print lookup\n",
    "def make_data_for_analysis(values):\n",
    "  return {\n",
    "      'target': lookup[values['target']],\n",
    "      'predicted': lookup[values['label']],\n",
    "      'score': np.max(values['score']), # not needed\n",
    "  }\n",
    "\n",
    "metadata = pipeline | io.LoadMetadata('/content/training-data-analyst/blogs/landsat-ml/landcover_preproc/metadata.yaml')\n",
    "analysis_source = evaluations | beam.Map('CreateAnalysisSource', make_data_for_analysis)\n",
    "confusion_matrix, precision_recall, logloss = (analysis_source |\n",
    "    'Analyze Model' >> analysis.AnalyzeModel(metadata))\n",
    "confusion_matrix_file = os.path.join(OUTPUT_DIR, 'analyze_cm.json')\n",
    "confusion_matrix_sink = beam.io.TextFileSink(confusion_matrix_file, shard_name_template='')\n",
    "confusion_matrix | beam.io.Write('WriteConfusionMatrix', confusion_matrix_sink)\n",
    "\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"5e22cd4b-eab0-4c7d-9233-0a1b273ee2d8\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5e22cd4b-eab0-4c7d-9233-0a1b273ee2d8\", [{\"y\": [\"lc_09\", \"lc_08\", \"lc_10\", \"lc_12\", \"lc_05\", \"lc_01\"], \"x\": [\"lc_09\", \"lc_08\", \"lc_10\", \"lc_12\", \"lc_05\", \"lc_01\"], \"z\": [[5347, 1712, 246, 4, 0, 4693], [1990, 18316, 1462, 6, 0, 1307], [297, 1610, 5389, 2, 0, 79], [29, 81, 28, 7, 0, 3], [84, 1, 0, 0, 0, 1880], [2355, 183, 10, 0, 0, 19546]], \"type\": \"heatmap\", \"colorscale\": \"YlGnBu\"}], {\"title\": \"Confusion Matrix\", \"xaxis\": {\"title\": \"Predicted value\"}, \"yaxis\": {\"title\": \"True Value\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datalab.mlalpha\n",
    "import yaml\n",
    "with ml.util._file.open_local_or_gcs(confusion_matrix_file, 'r') as f:\n",
    "  data = [yaml.load(line) for line in f.read().rstrip().split('\\n')]\n",
    "  for line in data:\n",
    "    line['target'] = 'lc_{:02d}'.format(int(line['target']))\n",
    "    line['predicted'] = 'lc_{:02d}'.format(int(line['predicted']))\n",
    "datalab.mlalpha.ConfusionMatrix([d['predicted'] for d in data],\n",
    "                           [d['target'] for d in data],\n",
    "                           [d['count'] for d in data]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to get a little confused between categories 1 and 9; categories 5 and 12 are poorly recognized. Let's not get too hung up on this, though, because this is on a very small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train on full dataset on the cloud</h2>\n",
    "\n",
    "Let's preprocess the complete dataset.  <b>These steps will take several hours and have billing implications</b>.\n",
    "\n",
    "Specify your bucket and project as appropriate. Make sure that the bucket you use is a single-region bucket (when you create a bucket, there is an option to specify this). If you already have a bucket and it is not a single-region one, you should create a separate single-region bucket for Cloud ML jobs to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml/sdk/cloudml-0.1.6-alpha.dataflow.tar.gz\n",
      "WARNING:root:Direct usage of TextFileSink is deprecated. Please use 'textio.WriteToText()' instead of directly instantiating a TextFileSink object.\n",
      "running sdist\n",
      "running egg_info\n",
      "writing requirements to landcover.egg-info/requires.txt\n",
      "writing landcover.egg-info/PKG-INFO\n",
      "writing top-level names to landcover.egg-info/top_level.txt\n",
      "writing dependency_links to landcover.egg-info/dependency_links.txt\n",
      "reading manifest file 'landcover.egg-info/SOURCES.txt'\n",
      "writing manifest file 'landcover.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating landcover-0.0.1\n",
      "creating landcover-0.0.1/landcover.egg-info\n",
      "copying files to landcover-0.0.1...\n",
      "copying setup.py -> landcover-0.0.1\n",
      "copying landcover.egg-info/PKG-INFO -> landcover-0.0.1/landcover.egg-info\n",
      "copying landcover.egg-info/SOURCES.txt -> landcover-0.0.1/landcover.egg-info\n",
      "copying landcover.egg-info/dependency_links.txt -> landcover-0.0.1/landcover.egg-info\n",
      "copying landcover.egg-info/requires.txt -> landcover-0.0.1/landcover.egg-info\n",
      "copying landcover.egg-info/top_level.txt -> landcover-0.0.1/landcover.egg-info\n",
      "Writing landcover-0.0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'landcover-0.0.1' (and everything under it)\n",
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "\u001b[33mDEPRECATION: pip install --download has been deprecated and will be removed in the future. Pip now has a download command that should be used instead.\u001b[0m\n",
      "Collecting google-cloud-dataflow==0.4.2\n",
      "  Using cached google-cloud-dataflow-0.4.2.zip\n",
      "  Saved /tmp/tmpKdWQmm/google-cloud-dataflow-0.4.2.zip\n",
      "Successfully downloaded google-cloud-dataflow\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "  11911414  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00000-of-00017.tfrecord.gz\n",
      "   1399595  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00001-of-00017.tfrecord.gz\n",
      "    722332  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00002-of-00017.tfrecord.gz\n",
      "  61908282  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00003-of-00017.tfrecord.gz\n",
      "  62458050  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00004-of-00017.tfrecord.gz\n",
      " 233743299  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00005-of-00017.tfrecord.gz\n",
      "   5783557  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00006-of-00017.tfrecord.gz\n",
      " 130264412  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00007-of-00017.tfrecord.gz\n",
      "  23244901  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00008-of-00017.tfrecord.gz\n",
      "1065181613  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00009-of-00017.tfrecord.gz\n",
      " 277395943  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00010-of-00017.tfrecord.gz\n",
      "  28719629  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00011-of-00017.tfrecord.gz\n",
      "1050966190  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00012-of-00017.tfrecord.gz\n",
      "  78715409  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00013-of-00017.tfrecord.gz\n",
      " 130639900  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00014-of-00017.tfrecord.gz\n",
      "    310832  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00015-of-00017.tfrecord.gz\n",
      "   2787529  2016-11-04T18:12:29Z  gs://cloud-training-demos-ml/landcover/preproc/features_eval-00016-of-00017.tfrecord.gz\n",
      " 443549311  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00000-of-00024.tfrecord.gz\n",
      "   1192011  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00001-of-00024.tfrecord.gz\n",
      "  35677789  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00002-of-00024.tfrecord.gz\n",
      "1038657615  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00003-of-00024.tfrecord.gz\n",
      " 199716840  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00004-of-00024.tfrecord.gz\n",
      "  45670895  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00005-of-00024.tfrecord.gz\n",
      "   4878803  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00006-of-00024.tfrecord.gz\n",
      "  16880155  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00007-of-00024.tfrecord.gz\n",
      " 295734304  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00008-of-00024.tfrecord.gz\n",
      "   2526618  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00009-of-00024.tfrecord.gz\n",
      "1046941812  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00010-of-00024.tfrecord.gz\n",
      "   1436843  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00011-of-00024.tfrecord.gz\n",
      "  25896955  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00012-of-00024.tfrecord.gz\n",
      " 199163307  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00013-of-00024.tfrecord.gz\n",
      "1053759092  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00014-of-00024.tfrecord.gz\n",
      "  22568405  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00015-of-00024.tfrecord.gz\n",
      "   2890711  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00016-of-00024.tfrecord.gz\n",
      " 481782889  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00017-of-00024.tfrecord.gz\n",
      "  39446244  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00018-of-00024.tfrecord.gz\n",
      " 107611947  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00019-of-00024.tfrecord.gz\n",
      " 154284477  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00020-of-00024.tfrecord.gz\n",
      "    747465  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00021-of-00024.tfrecord.gz\n",
      "1035094056  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00022-of-00024.tfrecord.gz\n",
      "    790420  2016-11-04T18:12:59Z  gs://cloud-training-demos-ml/landcover/preproc/features_train-00023-of-00024.tfrecord.gz\n",
      "      2243  2016-11-04T16:57:07Z  gs://cloud-training-demos-ml/landcover/preproc/metadata.yaml\n",
      "TOTAL: 42 objects, 9423054094 bytes (8.78 GiB)\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -l gs://cloud-training-demos-ml/landcover/preproc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tar up the Python package and make it available on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landcover/\n",
      "landcover/setup.py\n",
      "landcover/trainer/\n",
      "landcover/trainer/__init__.py\n",
      "landcover/trainer/task.py\n",
      "landcover/trainer/util.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Copying file://landcover.tgz [Content-Type=application/x-tar]...\n",
      "/ [0 files][    0.0 B/  5.0 KiB]                                                \r",
      "/ [1 files][  5.0 KiB/  5.0 KiB]                                                \r\n",
      "Operation completed over 1 objects/5.0 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "BUCKET=cloud-training-demos-ml\n",
    "gsutil -m rm -rf gs://$BUCKET/landcover/trained\n",
    "tar cvfz landcover.tgz landcover\n",
    "gsutil cp landcover.tgz gs://$BUCKET/landcover/source/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as the local training except:\n",
    "<ol>\n",
    "<li> --cloud parameter to do the training on the Cloud in a distributed way rather on a single machine.\n",
    "<li> all the data paths point to Cloud Storage, where our preprocessing code wrote its output\n",
    "<li> max_steps is much larger.  This is because a step is only one batch.  The entire dataset is 178m points, and since batchsize is 10000, we need\n",
    "17,800 steps for a single epoch (or pass through training data).  So, the 890000 here is approximately 50 epochs of training.\n",
    "<li> The evaluation frequency has been upped to 17,800 for the same reason (so that we evaluate approximately once every epoch)\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job \"trainer_task_161121_170420\" was submitted successfully.<br/>Run \"%mlalpha jobs --name trainer_task_161121_170420\" to view the status of the job.</p><p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=cloud-training-demos&resource=ml.googleapis.com%2Fjob_id%2Ftrainer_task_161121_170420\" target=\"_blank\">here</a> to view cloud log. <br/>Start TensorBoard by running \"%tensorboard start --logdir=&lt;YourLogDir&gt;\".</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mlalpha train --cloud\n",
    "package_uris: gs://cloud-training-demos-ml/landcover/source/landcover.tgz\n",
    "python_module: trainer.task\n",
    "scale_tier: STANDARD_1\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths: gs://cloud-training-demos-ml/landcover/preproc/features_train-*\n",
    "  eval_data_paths: gs://cloud-training-demos-ml/landcover/preproc/features_eval-*\n",
    "  metadata_path: gs://cloud-training-demos-ml/landcover/preproc/metadata.yaml\n",
    "  output_path: gs://cloud-training-demos-ml/landcover/trained\n",
    "  max_steps:  890000\n",
    "  batch_size: 10000\n",
    "  layer1_size: 30\n",
    "  layer2_size: 10\n",
    "  learning_rate: 0.01\n",
    "  min_eval_frequency: 17800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>createTime: '2016-11-21T17:04:14Z'\n",
       "endTime: '2016-11-21T17:49:53Z'\n",
       "jobId: trainer_task_161121_170420\n",
       "startTime: '2016-11-21T17:04:44Z'\n",
       "state: SUCCEEDED\n",
       "trainingInput:\n",
       "  args: [--metadata_path, 'gs://cloud-training-demos-ml/landcover/preproc/metadata.yaml',\n",
       "    --min_eval_frequency, '17800', --batch_size, '10000', --eval_data_paths, 'gs://cloud-training-demos-ml/landcover/preproc/features_eval-*',\n",
       "    --layer1_size, '30', --output_path, 'gs://cloud-training-demos-ml/landcover/trained',\n",
       "    --train_data_paths, 'gs://cloud-training-demos-ml/landcover/preproc/features_train-*',\n",
       "    --max_steps, '890000', --learning_rate, '0.01', --layer2_size, '10']\n",
       "  packageUris: ['gs://cloud-training-demos-ml/landcover/source/landcover.tgz']\n",
       "  pythonModule: trainer.task\n",
       "  region: us-central1\n",
       "  scaleTier: STANDARD_1\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mlalpha jobs --name trainer_task_161121_170420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "gs://cloud-training-demos-ml/landcover/trained/model/\n",
      "gs://cloud-training-demos-ml/landcover/trained/model/checkpoint\n",
      "gs://cloud-training-demos-ml/landcover/trained/model/export\n",
      "gs://cloud-training-demos-ml/landcover/trained/model/export.meta\n",
      "gs://cloud-training-demos-ml/landcover/trained/model/metadata.yaml\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://cloud-training-demos-ml/landcover/trained/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Run prediction </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!python predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos-ml/landcover/prediction/eval\n",
      "gs://cloud-training-demos-ml/landcover/prediction/tmp/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://cloud-training-demos-ml/landcover/prediction/landcover.TIF...\n",
      "/ [1 objects]                                                                   \r\n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil rm gs://cloud-training-demos-ml/landcover/prediction/landcover.TIF\n",
    "gsutil ls gs://cloud-training-demos-ml/landcover/prediction/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Confusion matrix </h2>\n",
    "\n",
    "Let's display the confusion matrix on the full prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos-ml/landcover/trained/model/metadata.yaml...\n",
      "- [1 files][  2.2 KiB/  2.2 KiB]                                                \n",
      "Operation completed over 1 objects/2.2 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://cloud-training-demos-ml/landcover/trained/model/metadata.yaml /tmp/metadata.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11': 3, '10': 2, '13': 5, '12': 4, '15': 7, '14': 6, '16': 8, '1': 1, '0': 0, '3': 10, '2': 9, '5': 12, '4': 11, '7': 14, '6': 13, '9': 16, '8': 15}\n"
     ]
    }
   ],
   "source": [
    "# analysis\n",
    "def read_metadata(filename): \n",
    "  import yaml\n",
    "  with open(filename, 'r') as stream:\n",
    "    try:\n",
    "        return yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "metadata = read_metadata('/tmp/metadata.yaml')\n",
    "lookup = metadata['columns']['landcover']['vocab']\n",
    "print lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos-ml/landcover/trained/model/metadata.yaml...\n",
      "/ [1 files][  2.2 KiB/  2.2 KiB]                                                \n",
      "Operation completed over 1 objects/2.2 KiB.                                      \n",
      "WARNING:root:Direct usage of TextFileSink is deprecated. Please use 'textio.WriteToText()' instead of directly instantiating a TextFileSink object.\n",
      "running sdist\n",
      "running egg_info\n",
      "writing requirements to landcover.egg-info/requires.txt\n",
      "writing landcover.egg-info/PKG-INFO\n",
      "writing top-level names to landcover.egg-info/top_level.txt\n",
      "writing dependency_links to landcover.egg-info/dependency_links.txt\n",
      "reading manifest file 'landcover.egg-info/SOURCES.txt'\n",
      "writing manifest file 'landcover.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating landcover-0.0.1\n",
      "creating landcover-0.0.1/landcover.egg-info\n",
      "copying files to landcover-0.0.1...\n",
      "copying setup.py -> landcover-0.0.1\n",
      "copying landcover.egg-info/PKG-INFO -> landcover-0.0.1/landcover.egg-info\n",
      "copying landcover.egg-info/SOURCES.txt -> landcover-0.0.1/landcover.egg-info\n",
      "copying landcover.egg-info/dependency_links.txt -> landcover-0.0.1/landcover.egg-info\n",
      "copying landcover.egg-info/requires.txt -> landcover-0.0.1/landcover.egg-info\n",
      "copying landcover.egg-info/top_level.txt -> landcover-0.0.1/landcover.egg-info\n",
      "Writing landcover-0.0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'landcover-0.0.1' (and everything under it)\n",
      "\u001b[31mDEPRECATION: pip install --download has been deprecated and will be removed in the future. Pip now has a download command that should be used instead.\u001b[0m\n",
      "Collecting google-cloud-dataflow==0.4.2\n",
      "  Using cached google-cloud-dataflow-0.4.2.zip\n",
      "  Saved /tmp/tmpzJJEK0/google-cloud-dataflow-0.4.2.zip\n",
      "Successfully downloaded google-cloud-dataflow\n"
     ]
    }
   ],
   "source": [
    "!python confusion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Retrying after exception reading gcs file: [Errno 2] Not found: gs://cloud-training-demos-ml/landcover/prediction/confusion/analyze_cm.json\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] Not found: gs://cloud-training-demos-ml/landcover/prediction/confusion/analyze_cm.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c0dd583a60bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconfusion_matrix_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gs://cloud-training-demos-ml/landcover/prediction/confusion/analyze_cm.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_local_or_gcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/cloud/ml/util/_file.pyc\u001b[0m in \u001b[0;36mopen_local_or_gcs\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Retrying after exception reading gcs file: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgcsio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGcsIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/io/gcsio.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, filename, mode, read_buffer_size, mime_type)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m       return GcsBufferedReader(self.client, filename, mode=mode,\n\u001b[0;32m--> 114\u001b[0;31m                                buffer_size=read_buffer_size)\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m       return GcsBufferedWriter(self.client, filename, mode=mode,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/io/gcsio.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, client, path, mode, buffer_size)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHttpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhttp_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhttp_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Not found: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         logging.error('HTTP error while requesting file %s: %s', self.path,\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] Not found: gs://cloud-training-demos-ml/landcover/prediction/confusion/analyze_cm.json"
     ]
    }
   ],
   "source": [
    "import datalab.mlalpha\n",
    "import google.cloud.ml as ml\n",
    "import yaml\n",
    "confusion_matrix_file = 'gs://cloud-training-demos-ml/landcover/prediction/confusion/analyze_cm.json'\n",
    "with ml.util._file.open_local_or_gcs(confusion_matrix_file, 'r') as f:\n",
    "  data = [yaml.load(line) for line in f.read().rstrip().split('\\n')]\n",
    "  for line in data:\n",
    "    line['target'] = 'lc_{:02d}'.format(int(line['target']))\n",
    "    line['predicted'] = 'lc_{:02d}'.format(int(line['predicted']))\n",
    "datalab.mlalpha.ConfusionMatrix([d['predicted'] for d in data],\n",
    "                           [d['target'] for d in data],\n",
    "                           [d['count'] for d in data]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create downscaled Landcover image </h2>\n",
    "\n",
    "We'll read the original Landcover image and replace the pixel values by the predictions from the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml/sdk/cloudml-0.1.6-alpha.dataflow.tar.gz\n",
      "in-memory array created ...\n",
      "WARNING:root:Using fallback coder for typehint: Union[].\n",
      "running sdist\n",
      "running egg_info\n",
      "writing requirements to landcover.egg-info/requires.txt\n",
      "writing landcover.egg-info/PKG-INFO\n",
      "writing top-level names to landcover.egg-info/top_level.txt\n",
      "writing dependency_links to landcover.egg-info/dependency_links.txt\n",
      "reading manifest file 'landcover.egg-info/SOURCES.txt'\n",
      "writing manifest file 'landcover.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating landcover-0.0.1\n",
      "creating landcover-0.0.1/landcover.egg-info\n",
      "copying files to landcover-0.0.1...\n",
      "copying setup.py -> landcover-0.0.1\n",
      "copying landcover.egg-info/PKG-INFO -> landcover-0.0.1/landcover.egg-info\n",
      "copying landcover.egg-info/SOURCES.txt -> landcover-0.0.1/landcover.egg-info\n",
      "copying landcover.egg-info/dependency_links.txt -> landcover-0.0.1/landcover.egg-info\n",
      "copying landcover.egg-info/requires.txt -> landcover-0.0.1/landcover.egg-info\n",
      "copying landcover.egg-info/top_level.txt -> landcover-0.0.1/landcover.egg-info\n",
      "Writing landcover-0.0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'landcover-0.0.1' (and everything under it)\n",
      "\u001b[33mDEPRECATION: pip install --download has been deprecated and will be removed in the future. Pip now has a download command that should be used instead.\u001b[0m\n",
      "Collecting google-cloud-dataflow==0.4.2\n",
      "  Using cached google-cloud-dataflow-0.4.2.zip\n",
      "  Saved /tmp/tmpMQaAba/google-cloud-dataflow-0.4.2.zip\n",
      "Successfully downloaded google-cloud-dataflow\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Killed\n"
     ]
    }
   ],
   "source": [
    "!python create_downscaled.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2016 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
