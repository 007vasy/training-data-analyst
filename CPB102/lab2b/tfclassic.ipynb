{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2b. Working with low-level TensorFlow </h1>\n",
    "\n",
    "This notebook is Lab2b of CPB 102, Google's course on Machine Learning using Cloud ML.\n",
    "\n",
    "In this notebook, we will work with relatively low-level TensorFlow functions to implement a linear regression model. We will use this notebook to demonstrate early stopping -- a technique whereby training is stopped once the error on the validation dataset starts to increase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0-rc2\n"
     ]
    }
   ],
   "source": [
    "import datalab.bigquery as bq\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to read data and compute error is similar to Lab2a, but we simply load the dataset into memory rather than come up with an input_fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickuplon</th>\n",
       "      <th>pickuplat</th>\n",
       "      <th>dropofflon</th>\n",
       "      <th>dropofflat</th>\n",
       "      <th>passengers</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-74.003375</td>\n",
       "      <td>40.743642</td>\n",
       "      <td>-73.993685</td>\n",
       "      <td>40.728487</td>\n",
       "      <td>1</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-73.990978</td>\n",
       "      <td>40.745340</td>\n",
       "      <td>-73.979782</td>\n",
       "      <td>40.752980</td>\n",
       "      <td>2</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.011400</td>\n",
       "      <td>40.701761</td>\n",
       "      <td>-73.978215</td>\n",
       "      <td>40.764434</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.970957</td>\n",
       "      <td>40.758473</td>\n",
       "      <td>-73.979440</td>\n",
       "      <td>40.759398</td>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.990905</td>\n",
       "      <td>40.749596</td>\n",
       "      <td>-73.974205</td>\n",
       "      <td>40.756526</td>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickuplon  pickuplat  dropofflon  dropofflat  passengers  fare_amount\n",
       "0 -74.003375  40.743642  -73.993685   40.728487           1          6.9\n",
       "1 -73.990978  40.745340  -73.979782   40.752980           2          5.7\n",
       "2 -74.011400  40.701761  -73.978215   40.764434           1         20.0\n",
       "3 -73.970957  40.758473  -73.979440   40.759398           1          4.9\n",
       "4 -73.990905  40.749596  -73.974205   40.756526           1          9.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_dataset(filename):\n",
    "  return pd.read_csv(filename, header=None, names=['pickuplon','pickuplat','dropofflon','dropofflat','passengers','fare_amount'])\n",
    "\n",
    "df_train = read_dataset('../lab1a/taxi-train.csv')\n",
    "df_valid = read_dataset('../lab1a/taxi-valid.csv')\n",
    "df_test = read_dataset('../lab1a/taxi-test.csv')\n",
    "df_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FEATURE_COLS = np.arange(0,5)\n",
    "TARGET_COL   = 'fare_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rmse(actual, predicted):\n",
    "  return np.sqrt(np.mean((actual-predicted)**2))\n",
    "\n",
    "def print_rmse(model):\n",
    "  print \"Train RMSE = {0}\".format(compute_rmse(df_train[TARGET_COL], model.predict(df_train.iloc[:,FEATURE_COLS].values)))\n",
    "  print \"Valid RMSE = {0}\".format(compute_rmse(df_valid[TARGET_COL], model.predict(df_valid.iloc[:,FEATURE_COLS].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Linear Regression </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1 train_error=10.4740593948 valid_err=15.1333942323\n",
      "Model written to /tmp/trained_model-1\n",
      "iter=101 train_error=9.31031243471 valid_err=13.4961987771\n",
      "Model written to /tmp/trained_model-101\n",
      "iter=201 train_error=8.39193264165 valid_err=12.2002376379\n",
      "Model written to /tmp/trained_model-201\n",
      "iter=301 train_error=7.7169462126 valid_err=11.2414351895\n",
      "Model written to /tmp/trained_model-301\n",
      "iter=401 train_error=7.25939013787 valid_err=10.5837084169\n",
      "Model written to /tmp/trained_model-401\n",
      "iter=501 train_error=6.97592067564 valid_err=10.1681273848\n",
      "Model written to /tmp/trained_model-501\n",
      "iter=601 train_error=6.81620878642 valid_err=9.9265273249\n",
      "Model written to /tmp/trained_model-601\n",
      "iter=701 train_error=6.73450035362 valid_err=9.7967053956\n",
      "Model written to /tmp/trained_model-701\n",
      "iter=801 train_error=6.69655993007 valid_err=9.73157710121\n",
      "Model written to /tmp/trained_model-801\n",
      "iter=901 train_error=6.68056697177 valid_err=9.70058509883\n",
      "Model written to /tmp/trained_model-901\n",
      "iter=1001 train_error=6.67445360462 valid_err=9.68629113024\n",
      "Model written to /tmp/trained_model-1001\n",
      "iter=1101 train_error=6.67233611086 valid_err=9.67974552492\n",
      "Model written to /tmp/trained_model-1101\n",
      "iter=1201 train_error=6.67167303174 valid_err=9.67671307029\n",
      "Model written to /tmp/trained_model-1201\n",
      "iter=1301 train_error=6.67148705636 valid_err=9.67528519327\n",
      "Model written to /tmp/trained_model-1301\n",
      "iter=1401 train_error=6.67144033563 valid_err=9.67460923567\n",
      "Model written to /tmp/trained_model-1401\n",
      "iter=1501 train_error=6.67142827862 valid_err=9.67429359207\n",
      "Model written to /tmp/trained_model-1501\n",
      "iter=1601 train_error=6.67142405865 valid_err=9.67415065234\n",
      "Model written to /tmp/trained_model-1601\n",
      "iter=1701 train_error=6.67142737434 valid_err=9.67408875104\n",
      "Model written to /tmp/trained_model-1701\n",
      "iter=1801 train_error=6.67142737434 valid_err=9.67406305046\n",
      "Model written to /tmp/trained_model-1801\n",
      "iter=1901 train_error=6.67142737434 valid_err=9.67405260009\n",
      "Model written to /tmp/trained_model-1901\n",
      "iter=2001 train_error=6.67142616864 valid_err=9.6740484127\n",
      "Model written to /tmp/trained_model-2001\n",
      "iter=2101 train_error=6.67142556578 valid_err=9.67404618215\n",
      "Model written to /tmp/trained_model-2101\n",
      "iter=2201 train_error=6.67142737434 valid_err=9.67404458534\n",
      "Model written to /tmp/trained_model-2201\n",
      "iter=2301 train_error=6.67142677149 valid_err=9.67404345676\n",
      "Model written to /tmp/trained_model-2301\n",
      "iter=2401 train_error=6.67142767576 valid_err=9.67404235585\n",
      "Model written to /tmp/trained_model-2401\n",
      "iter=2501 train_error=6.67142797719 valid_err=9.67404128261\n",
      "Model written to /tmp/trained_model-2501\n",
      "iter=2601 train_error=6.67142737434 valid_err=9.67404009946\n",
      "Model written to /tmp/trained_model-2601\n",
      "iter=2701 train_error=6.67142677149 valid_err=9.67403880645\n",
      "Model written to /tmp/trained_model-2701\n",
      "iter=2801 train_error=6.67142647006 valid_err=9.67403726611\n",
      "Model written to /tmp/trained_model-2801\n",
      "iter=2901 train_error=6.67142767576 valid_err=9.67403569857\n",
      "Model written to /tmp/trained_model-2901\n",
      "iter=3001 train_error=6.67142858004 valid_err=9.67403440626\n",
      "Model written to /tmp/trained_model-3001\n",
      "iter=3101 train_error=6.67142858004 valid_err=9.67403292174\n",
      "Model written to /tmp/trained_model-3101\n",
      "iter=3201 train_error=6.67142797719 valid_err=9.67403121763\n",
      "Model written to /tmp/trained_model-3201\n",
      "iter=3301 train_error=6.67142677149 valid_err=9.67402934902\n",
      "Model written to /tmp/trained_model-3301\n",
      "iter=3401 train_error=6.67142677149 valid_err=9.6740273435\n",
      "Model written to /tmp/trained_model-3401\n",
      "iter=3501 train_error=6.67142556578 valid_err=9.67402580535\n",
      "Model written to /tmp/trained_model-3501\n",
      "iter=3601 train_error=6.67142526436 valid_err=9.67402385562\n",
      "Model written to /tmp/trained_model-3601\n",
      "iter=3701 train_error=6.67142556578 valid_err=9.67402157697\n",
      "Model written to /tmp/trained_model-3701\n",
      "iter=3801 train_error=6.67142556578 valid_err=9.67401918919\n",
      "Model written to /tmp/trained_model-3801\n",
      "iter=3901 train_error=6.67142526436 valid_err=9.67401732338\n",
      "Model written to /tmp/trained_model-3901\n",
      "iter=4001 train_error=6.67142647006 valid_err=9.67401499173\n",
      "Model written to /tmp/trained_model-4001\n",
      "iter=4101 train_error=6.67142677149 valid_err=9.67401246882\n",
      "Model written to /tmp/trained_model-4101\n",
      "iter=4201 train_error=6.67142526436 valid_err=9.67401008375\n",
      "Model written to /tmp/trained_model-4201\n",
      "iter=4301 train_error=6.67142556578 valid_err=9.67400753499\n",
      "Model written to /tmp/trained_model-4301\n",
      "iter=4401 train_error=6.67142466151 valid_err=9.67400487747\n",
      "Model written to /tmp/trained_model-4401\n",
      "iter=4501 train_error=6.67142315438 valid_err=9.67400219345\n",
      "Model written to /tmp/trained_model-4501\n",
      "iter=4601 train_error=6.67142436008 valid_err=9.67399931871\n",
      "Model written to /tmp/trained_model-4601\n",
      "iter=4701 train_error=6.67142375723 valid_err=9.67399628083\n",
      "Model written to /tmp/trained_model-4701\n",
      "iter=4801 train_error=6.67142436008 valid_err=9.67399324411\n",
      "Model written to /tmp/trained_model-4801\n",
      "iter=4901 train_error=6.67142315438 valid_err=9.67399009917\n",
      "Model written to /tmp/trained_model-4901\n",
      "iter=5001 train_error=6.67142315438 valid_err=9.67398690082\n",
      "Model written to /tmp/trained_model-5001\n",
      "iter=5101 train_error=6.67142255153 valid_err=9.67398364911\n",
      "Model written to /tmp/trained_model-5101\n",
      "iter=5201 train_error=6.67142436008 valid_err=9.67398026219\n",
      "Model written to /tmp/trained_model-5201\n",
      "iter=5301 train_error=6.67142466151 valid_err=9.67397679483\n",
      "Model written to /tmp/trained_model-5301\n",
      "iter=5401 train_error=6.67142526436 valid_err=9.67397321985\n",
      "Model written to /tmp/trained_model-5401\n",
      "iter=5501 train_error=6.67142436008 valid_err=9.67396959193\n",
      "Model written to /tmp/trained_model-5501\n",
      "iter=5601 train_error=6.6714222501 valid_err=9.67396585664\n",
      "Model written to /tmp/trained_model-5601\n",
      "iter=5701 train_error=6.67142375723 valid_err=9.67396206863\n",
      "Model written to /tmp/trained_model-5701\n",
      "iter=5801 train_error=6.67142074297 valid_err=9.67395822796\n",
      "Model written to /tmp/trained_model-5801\n",
      "iter=5901 train_error=6.67142375723 valid_err=9.67395422585\n",
      "Model written to /tmp/trained_model-5901\n",
      "iter=6001 train_error=6.67142164725 valid_err=9.67395022578\n",
      "Model written to /tmp/trained_model-6001\n",
      "iter=6101 train_error=6.67142194867 valid_err=9.67394622773\n",
      "Model written to /tmp/trained_model-6101\n",
      "iter=6201 train_error=6.67142194867 valid_err=9.673942123\n",
      "Model written to /tmp/trained_model-6201\n",
      "iter=6301 train_error=6.67142134582 valid_err=9.6739381019\n",
      "Model written to /tmp/trained_model-6301\n",
      "iter=6401 train_error=6.67142044154 valid_err=9.67393391998\n",
      "Model written to /tmp/trained_model-6401\n",
      "iter=6501 train_error=6.67142014012 valid_err=9.67392979454\n",
      "Model written to /tmp/trained_model-6501\n",
      "iter=6601 train_error=6.67142014012 valid_err=9.6739255628\n",
      "Model written to /tmp/trained_model-6601\n",
      "iter=6701 train_error=6.67141833156 valid_err=9.67392149597\n",
      "Model written to /tmp/trained_model-6701\n",
      "iter=6801 train_error=6.67141953727 valid_err=9.67391729581\n",
      "Model written to /tmp/trained_model-6801\n",
      "iter=6901 train_error=6.67141803014 valid_err=9.67391312497\n",
      "Model written to /tmp/trained_model-6901\n",
      "iter=7001 train_error=6.67141833156 valid_err=9.67390909168\n",
      "Model written to /tmp/trained_model-7001\n",
      "iter=7101 train_error=6.67141833156 valid_err=9.67390503342\n",
      "Model written to /tmp/trained_model-7101\n",
      "iter=7201 train_error=6.67141772871 valid_err=9.67390108541\n",
      "Model written to /tmp/trained_model-7201\n",
      "iter=7301 train_error=6.67141712586 valid_err=9.6738971394\n",
      "Model written to /tmp/trained_model-7301\n",
      "iter=7401 train_error=6.6714153173 valid_err=9.67389333044\n",
      "Model written to /tmp/trained_model-7401\n",
      "iter=7501 train_error=6.67141592015 valid_err=9.67388963131\n",
      "Model written to /tmp/trained_model-7501\n",
      "iter=7601 train_error=6.67141803014 valid_err=9.67388596093\n",
      "Model written to /tmp/trained_model-7601\n",
      "iter=7701 train_error=6.67141712586 valid_err=9.67388256198\n",
      "Model written to /tmp/trained_model-7701\n",
      "iter=7801 train_error=6.67141622158 valid_err=9.67387908366\n",
      "Model written to /tmp/trained_model-7801\n",
      "iter=7901 train_error=6.67141712586 valid_err=9.67387593024\n",
      "Model written to /tmp/trained_model-7901\n",
      "iter=8001 train_error=6.67141772871 valid_err=9.67387283199\n",
      "Model written to /tmp/trained_model-8001\n",
      "iter=8101 train_error=6.67141561873 valid_err=9.67386984268\n",
      "Model written to /tmp/trained_model-8101\n",
      "iter=8201 train_error=6.67141622158 valid_err=9.67386709676\n",
      "Model written to /tmp/trained_model-8201\n",
      "iter=8301 train_error=6.67141592015 valid_err=9.67386448636\n",
      "Model written to /tmp/trained_model-8301\n",
      "iter=8401 train_error=6.67141622158 valid_err=9.67386209202\n",
      "Model written to /tmp/trained_model-8401\n",
      "iter=8501 train_error=6.67141381017 valid_err=9.67385988667\n",
      "Model written to /tmp/trained_model-8501\n",
      "iter=8601 train_error=6.67141350874 valid_err=9.67385776259\n",
      "Model written to /tmp/trained_model-8601\n",
      "iter=8701 train_error=6.67141381017 valid_err=9.67385580036\n",
      "Model written to /tmp/trained_model-8701\n",
      "iter=8801 train_error=6.67141441302 valid_err=9.6738541342\n",
      "Model written to /tmp/trained_model-8801\n",
      "iter=8901 train_error=6.67141381017 valid_err=9.67385260272\n",
      "Model written to /tmp/trained_model-8901\n",
      "iter=9001 train_error=6.67141381017 valid_err=9.67385117899\n",
      "Model written to /tmp/trained_model-9001\n",
      "iter=9101 train_error=6.67141381017 valid_err=9.67384991666\n",
      "Model written to /tmp/trained_model-9101\n",
      "iter=9201 train_error=6.67141320732 valid_err=9.67384881565\n",
      "Model written to /tmp/trained_model-9201\n",
      "iter=9301 train_error=6.67141200161 valid_err=9.67384784903\n",
      "Model written to /tmp/trained_model-9301\n",
      "iter=9401 train_error=6.67141139876 valid_err=9.67384701677\n",
      "Model written to /tmp/trained_model-9401\n",
      "iter=9501 train_error=6.67141079591 valid_err=9.67384639934\n",
      "Model written to /tmp/trained_model-9501\n",
      "iter=9601 train_error=6.67141079591 valid_err=9.67384575512\n",
      "Model written to /tmp/trained_model-9601\n",
      "iter=9701 train_error=6.67141139876 valid_err=9.67384529883\n",
      "Model written to /tmp/trained_model-9701\n",
      "iter=9801 train_error=6.67141019305 valid_err=9.67384489625\n",
      "Model written to /tmp/trained_model-9801\n",
      "iter=9901 train_error=6.67140898735 valid_err=9.67384446685\n",
      "Model written to /tmp/trained_model-9901\n",
      "Error on Test data = 8.68053949066\n"
     ]
    }
   ],
   "source": [
    "predictors = df_train.iloc[:,FEATURE_COLS].values\n",
    "targets = df_train[TARGET_COL].values\n",
    "prev_valid_error = 10000 # huge number\n",
    "modelprefix = '/tmp/trained_model'\n",
    "with tf.Session() as sess:\n",
    "  npredictors = len(FEATURE_COLS)\n",
    "  noutputs = 1\n",
    "  feature_data = tf.placeholder(\"float\", [None, npredictors])\n",
    "  target_data = tf.placeholder(\"float\", [None, noutputs])\n",
    "  weights = tf.Variable(tf.truncated_normal([npredictors, noutputs], stddev=0.01))\n",
    "  biases = tf.Variable(tf.ones([noutputs]))\n",
    "  model = (tf.matmul(feature_data, weights) + biases) # LINEAR REGRESSION\n",
    "  cost = tf.nn.l2_loss(model - target_data) # Square Error, not RMSE\n",
    "  saver = tf.train.Saver({'weights' : weights, 'biases' : biases})\n",
    "    \n",
    "  training_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "  tf.global_variables_initializer().run()\n",
    "  for iter in xrange(0, 10000):\n",
    "    _, trainerr = sess.run([training_step, cost], feed_dict = {\n",
    "        feature_data : predictors,\n",
    "        target_data : targets.reshape(len(predictors), noutputs)\n",
    "      })\n",
    "    if (iter%100 == 1):\n",
    "      # early stop if validation error doesn't keep dropping\n",
    "      preds = sess.run(model, feed_dict = {feature_data : df_valid.iloc[:,FEATURE_COLS].values})\n",
    "      trmse = np.sqrt(trainerr/len(predictors))\n",
    "      vrmse = compute_rmse(df_valid[TARGET_COL], preds[0])      \n",
    "      print 'iter={0} train_error={1} valid_err={2}'.format(iter, trmse, vrmse)\n",
    "      if vrmse > prev_valid_error:\n",
    "         print \"Early stop!\"\n",
    "         break  # out of iteration loop\n",
    "      else:\n",
    "         prev_valid_error = vrmse\n",
    "         # save the model so that we can read it\n",
    "         modelfile = saver.save(sess, modelprefix, global_step=iter)\n",
    "         print 'Model written to {0}'.format(modelfile)\n",
    "\n",
    "  preds = sess.run(model, feed_dict = {feature_data : df_test.iloc[:,FEATURE_COLS].values})\n",
    "  testrmse = compute_rmse(df_test[TARGET_COL], preds[0]) \n",
    "  print 'Error on Test data = {0}'.format(testrmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the training error can be driven down very low, but it doesn't actually reduce the validation error.  To help prevent over-fitting, the loop above makes use of \"early-stopping\", to stop the training when the validation error starts to increase.  In tf.learn, we didn't pass in a validation dataset, but we got similar performance on the validation set -- that's because tf.learn uses a different technique called regularization to help prevent over-fitting.\n",
    "\n",
    "Early stopping and regularization are not that critical in linear regression (because the model is quite simple), but are crucial once you start creating deep neural networks where there are thousands of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
