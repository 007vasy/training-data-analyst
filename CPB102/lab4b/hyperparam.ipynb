{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Hyperparameter tuning </h1>\n",
    "\n",
    "This notebook is Lab4b of CPB 102, Google's course on Machine Learning using Cloud ML.\n",
    "\n",
    "This notebook builds on Lab 4a, adding hyperparameter tuning to the feature engineering done in that lab.  To save time, we will start from the preprocessed data in that lab.\n",
    "\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./cloudml-0.1.4.tar.gz\n",
      "Collecting oauth2client==2.2.0 (from cloudml==0.1.4)\n",
      "Collecting six>=1.10.0 (from cloudml==0.1.4)\n",
      "  Using cached six-1.10.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-dataflow>=0.4.0 (from cloudml==0.1.4)\n",
      "Collecting bs4>=0.0.1 (from cloudml==0.1.4)\n",
      "Collecting numpy>=1.10.4 (from cloudml==0.1.4)\n",
      "  Using cached numpy-1.11.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting pillow>=3.2.0 (from cloudml==0.1.4)\n",
      "  Using cached Pillow-3.3.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting dpkt>=1.8.7 (from cloudml==0.1.4)\n",
      "  Using cached dpkt-1.8.8-py2-none-any.whl\n",
      "Collecting nltk>=3.2.1 (from cloudml==0.1.4)\n",
      "Collecting httplib2>=0.9.1 (from oauth2client==2.2.0->cloudml==0.1.4)\n",
      "Collecting rsa>=3.1.4 (from oauth2client==2.2.0->cloudml==0.1.4)\n",
      "  Using cached rsa-3.4.2-py2.py3-none-any.whl\n",
      "Collecting pyasn1>=0.1.7 (from oauth2client==2.2.0->cloudml==0.1.4)\n",
      "  Using cached pyasn1-0.1.9-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.0.5 (from oauth2client==2.2.0->cloudml==0.1.4)\n",
      "  Using cached pyasn1_modules-0.0.8-py2.py3-none-any.whl\n",
      "Collecting google-apitools>=0.5.2 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Using cached google_apitools-0.5.4-py2-none-any.whl\n",
      "Collecting dill>=0.2.5 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "Collecting protorpc>=0.9.1 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Using cached protorpc-0.11.1-py2-none-any.whl\n",
      "Collecting mock>=1.0.1 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Using cached mock-2.0.0-py2.py3-none-any.whl\n",
      "Collecting avro>=1.7.7 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "Collecting python-gflags>=2.0 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "Collecting pyyaml>=3.10 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "Collecting beautifulsoup4 (from bs4>=0.0.1->cloudml==0.1.4)\n",
      "  Using cached beautifulsoup4-4.5.1-py2-none-any.whl\n",
      "Collecting setuptools>=18.5 (from google-apitools>=0.5.2->google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Using cached setuptools-27.2.0-py2.py3-none-any.whl\n",
      "Collecting funcsigs>=1; python_version < \"3.3\" (from mock>=1.0.1->google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Using cached funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting pbr>=0.11 (from mock>=1.0.1->google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Using cached pbr-1.10.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: cloudml\n",
      "  Running setup.py bdist_wheel for cloudml: started\n",
      "  Running setup.py bdist_wheel for cloudml: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/24/e7/b6/341a6eb8b4b636be9f27d6c6de774d0227faae264846ff02f3\n",
      "Successfully built cloudml\n",
      "Installing collected packages: httplib2, pyasn1, rsa, pyasn1-modules, six, oauth2client, setuptools, google-apitools, dill, protorpc, funcsigs, pbr, mock, avro, python-gflags, pyyaml, google-cloud-dataflow, beautifulsoup4, bs4, numpy, pillow, dpkt, nltk, cloudml\n",
      "  Found existing installation: httplib2 0.9.2\n",
      "    Uninstalling httplib2-0.9.2:\n",
      "      Successfully uninstalled httplib2-0.9.2\n",
      "  Found existing installation: pyasn1 0.1.9\n",
      "    Uninstalling pyasn1-0.1.9:\n",
      "      Successfully uninstalled pyasn1-0.1.9\n",
      "  Found existing installation: rsa 3.4.2\n",
      "    Uninstalling rsa-3.4.2:\n",
      "      Successfully uninstalled rsa-3.4.2\n",
      "  Found existing installation: pyasn1-modules 0.0.8\n",
      "    Uninstalling pyasn1-modules-0.0.8:\n",
      "      Successfully uninstalled pyasn1-modules-0.0.8\n",
      "  Found existing installation: six 1.10.0\n",
      "    Uninstalling six-1.10.0:\n",
      "      Successfully uninstalled six-1.10.0\n",
      "  Found existing installation: oauth2client 2.2.0\n",
      "    Uninstalling oauth2client-2.2.0:\n",
      "      Successfully uninstalled oauth2client-2.2.0\n",
      "  Found existing installation: setuptools 27.2.0\n",
      "    Uninstalling setuptools-27.2.0:\n",
      "      Successfully uninstalled setuptools-27.2.0\n",
      "  Found existing installation: google-apitools 0.5.4\n",
      "    Uninstalling google-apitools-0.5.4:\n",
      "      Successfully uninstalled google-apitools-0.5.4\n",
      "  Found existing installation: dill 0.2.5\n",
      "    Uninstalling dill-0.2.5:\n",
      "      Successfully uninstalled dill-0.2.5\n",
      "  Found existing installation: protorpc 0.11.1\n",
      "    Uninstalling protorpc-0.11.1:\n",
      "      Successfully uninstalled protorpc-0.11.1\n",
      "  Found existing installation: funcsigs 1.0.2\n",
      "    Uninstalling funcsigs-1.0.2:\n",
      "      Successfully uninstalled funcsigs-1.0.2\n",
      "  Found existing installation: pbr 1.10.0\n",
      "    Uninstalling pbr-1.10.0:\n",
      "      Successfully uninstalled pbr-1.10.0\n",
      "  Found existing installation: mock 2.0.0\n",
      "    Uninstalling mock-2.0.0:\n",
      "      Successfully uninstalled mock-2.0.0\n",
      "  Found existing installation: avro 1.8.1\n",
      "    Uninstalling avro-1.8.1:\n",
      "      Successfully uninstalled avro-1.8.1\n",
      "  Found existing installation: python-gflags 3.0.7\n",
      "    Uninstalling python-gflags-3.0.7:\n",
      "      Successfully uninstalled python-gflags-3.0.7\n",
      "  Found existing installation: PyYAML 3.12\n",
      "    Uninstalling PyYAML-3.12:\n",
      "      Successfully uninstalled PyYAML-3.12\n",
      "  Found existing installation: google-cloud-dataflow 0.4.1\n",
      "    Uninstalling google-cloud-dataflow-0.4.1:\n",
      "      Successfully uninstalled google-cloud-dataflow-0.4.1\n",
      "  Found existing installation: beautifulsoup4 4.5.1\n",
      "    Uninstalling beautifulsoup4-4.5.1:\n",
      "      Successfully uninstalled beautifulsoup4-4.5.1\n",
      "  Found existing installation: bs4 0.0.1\n",
      "    Uninstalling bs4-0.0.1:\n",
      "      Successfully uninstalled bs4-0.0.1\n",
      "  Found existing installation: numpy 1.11.1\n",
      "    Uninstalling numpy-1.11.1:\n",
      "      Successfully uninstalled numpy-1.11.1\n",
      "  Found existing installation: Pillow 3.3.1\n",
      "    Uninstalling Pillow-3.3.1:\n",
      "      Successfully uninstalled Pillow-3.3.1\n",
      "  Found existing installation: dpkt 1.8.8\n",
      "    Uninstalling dpkt-1.8.8:\n",
      "      Successfully uninstalled dpkt-1.8.8\n",
      "  Found existing installation: nltk 3.2.1\n",
      "    Uninstalling nltk-3.2.1:\n",
      "      Successfully uninstalled nltk-3.2.1\n",
      "  Found existing installation: cloudml 0.1.4\n",
      "    Uninstalling cloudml-0.1.4:\n",
      "      Successfully uninstalled cloudml-0.1.4\n",
      "Successfully installed avro-1.8.1 beautifulsoup4-4.5.1 bs4-0.0.1 cloudml-0.1.4 dill-0.2.5 dpkt-1.8.8 funcsigs-1.0.2 google-apitools-0.5.4 google-cloud-dataflow-0.4.1 httplib2-0.9.2 mock-2.0.0 nltk-3.2.1 numpy-1.11.1 oauth2client-2.2.0 pbr-1.10.0 pillow-3.3.1 protorpc-0.11.1 pyasn1-0.1.9 pyasn1-modules-0.0.8 python-gflags-3.0.7 pyyaml-3.12 rsa-3.4.2 setuptools-27.2.0 six-1.10.0\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "# remember to \"Reset Session\" if you execute this cell -- this is needed to restart the Python kernel with updated package\n",
    "#gsutil cp gs://cloud-ml/sdk/cloudml-0.1.4.tar.gz .\n",
    "pip install --force-reinstall --upgrade cloudml-0.1.4.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Retreiving preprocessed data </h1>\n",
    "\n",
    "To save time, we'll go off the preprocessed data from Lab4a. Since we don't know whether you got Lab4a working completely, let's start off from my Lab4a results.  Change the BUCKET below to be yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos/taxifare/taxi_preproc4a_saved/features_eval [Content-Type=application/x-www-form-urlencoded]...\n",
      "Copying     ...ning-demos/taxifare/taxi_preproc4b/features_eval: 293.47 KiB/293.47 KiB    \r\n",
      "Copying gs://cloud-training-demos/taxifare/taxi_preproc4a_saved/features_train [Content-Type=application/x-www-form-urlencoded]...\n",
      "Copying     ...ing-demos/taxifare/taxi_preproc4b/features_train: 328.52 KiB/328.52 KiB    \r\n",
      "Copying gs://cloud-training-demos/taxifare/taxi_preproc4a_saved/info [Content-Type=text/plain]...\n",
      "Copying     ...loud-training-demos/taxifare/taxi_preproc4b/info: 461 B/461 B    \r\n",
      "Copying gs://cloud-training-demos/taxifare/taxi_preproc4a_saved/metadata.yaml [Content-Type=text/plain]...\n",
      "Copying     ...ning-demos/taxifare/taxi_preproc4b/metadata.yaml: 2.25 KiB/2.25 KiB    \r\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "BUCKET=cloud-training-demos\n",
    "SOURCE=gs://cloud-training-demos/taxifare/taxi_preproc4a_saved/*\n",
    "gsutil cp $SOURCE gs://$BUCKET/taxifare/taxi_preproc4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Modify TensorFlow code: change optimizable values to command-line arguments </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/trainer/task.py-      _, train_examples = taxifare.read_examples(\n",
      "taxifare/trainer/task.py-          training_data, HYPERPARAMS['batch_size'], shuffle=False)\n",
      "taxifare/trainer/task.py-\n",
      "taxifare/trainer/task.py:      # hyperparams\n",
      "taxifare/trainer/task.py:      hyperparams = {\n",
      "taxifare/trainer/task.py-        'hidden_layer1_size': args.hidden1,\n",
      "taxifare/trainer/task.py-        'hidden_layer2_size': args.hidden2,\n",
      "taxifare/trainer/task.py-        'hidden_layer3_size': args.hidden3,\n",
      "--\n",
      "taxifare/trainer/task.py-\n",
      "taxifare/trainer/task.py-      # Generate placeholders for the examples.\n",
      "taxifare/trainer/task.py-      placeholder, inputs, targets, _ = (\n",
      "taxifare/trainer/task.py:          taxifare.create_inputs(metadata, input_data=train_examples, hyperparams))\n",
      "taxifare/trainer/task.py-\n",
      "taxifare/trainer/task.py-      # Build a Graph that computes predictions from the inference model.\n",
      "taxifare/trainer/task.py:      output = taxifare.inference(inputs, metadata, hyperparams)\n",
      "taxifare/trainer/task.py-\n",
      "taxifare/trainer/task.py-      # Add to the Graph the Ops for loss calculation.\n",
      "taxifare/trainer/task.py-      loss = taxifare.loss(output, targets)\n",
      "--\n",
      "taxifare/trainer/task.py-  output_path = os.path.join(args.output_path, trial_id)\n",
      "taxifare/trainer/task.py-  with tf.Graph().as_default() as inference_graph:\n",
      "taxifare/trainer/task.py-    metadata = features.FeatureMetadata.get_metadata(args.metadata_path)\n",
      "taxifare/trainer/task.py:    hyperparams = {\n",
      "taxifare/trainer/task.py-      'hidden_layer1_size': args.hidden1,\n",
      "taxifare/trainer/task.py-      'hidden_layer2_size': args.hidden2,\n",
      "taxifare/trainer/task.py-      'hidden_layer3_size': args.hidden3,\n",
      "taxifare/trainer/task.py-      'number_buckets':     args.nbuckets\n",
      "taxifare/trainer/task.py-    }\n",
      "taxifare/trainer/task.py:    placeholder, inputs, _, keys = taxifare.create_inputs(metadata, None, hyperparams)\n",
      "taxifare/trainer/task.py:    output = taxifare.inference(inputs, metadata, hyperparams)\n",
      "taxifare/trainer/task.py-\n",
      "taxifare/trainer/task.py-    inference_saver = tf.train.Saver()\n",
      "taxifare/trainer/task.py-\n",
      "--\n",
      "taxifare/trainer/task.py-        eval_data, HYPERPARAMS['batch_size'],\n",
      "taxifare/trainer/task.py-        shuffle=False)\n",
      "taxifare/trainer/task.py-\n",
      "taxifare/trainer/task.py:    # hyperparams\n",
      "taxifare/trainer/task.py:    hyperparams = {\n",
      "taxifare/trainer/task.py-        'hidden_layer1_size': args.hidden1,\n",
      "taxifare/trainer/task.py-        'hidden_layer2_size': args.hidden2,\n",
      "taxifare/trainer/task.py-        'hidden_layer3_size': args.hidden3,\n",
      "--\n",
      "taxifare/trainer/task.py-\n",
      "taxifare/trainer/task.py-    # Generate placeholders for the examples.\n",
      "taxifare/trainer/task.py-    placeholder, inputs, targets, _ = (\n",
      "taxifare/trainer/task.py:        taxifare.create_inputs(metadata, input_data=examples, hyperparams))\n",
      "taxifare/trainer/task.py-\n",
      "taxifare/trainer/task.py-    # Build a Graph that computes predictions from the inference model.\n",
      "taxifare/trainer/task.py:    output = taxifare.inference(inputs, metadata, hyperparams)\n",
      "taxifare/trainer/task.py-\n",
      "taxifare/trainer/task.py-    # Add to the Graph the Ops for loss calculation.\n",
      "taxifare/trainer/task.py-    loss = taxifare.loss(output, targets)\n",
      "--\n",
      "taxifare/trainer/taxifare.py-   return tf.ones([batchsize], dtype=tf.float32)\n",
      "taxifare/trainer/taxifare.py-\n",
      "taxifare/trainer/taxifare.py-# TaxiFeatures is a dictionary; pull Tensors from the dictionary, and create features\n",
      "taxifare/trainer/taxifare.py:def create_inputs(metadata, input_data, hyperparams):\n",
      "taxifare/trainer/taxifare.py-  with tf.name_scope('inputs'):\n",
      "taxifare/trainer/taxifare.py-    if input_data is None:\n",
      "taxifare/trainer/taxifare.py-      input_data = tf.placeholder(tf.string, name='input', shape=(None,))\n",
      "--\n",
      "taxifare/trainer/taxifare.py-    hourofday = parsed['hourofday']\n",
      "taxifare/trainer/taxifare.py-\n",
      "taxifare/trainer/taxifare.py-    # pickup is [batchsize, nbuckets**2]\n",
      "taxifare/trainer/taxifare.py:    NUMBUCKETS = hyperparams['number_buckets']\n",
      "taxifare/trainer/taxifare.py-    pickup_index, pickup = feature_cross_latlon(plat, plon, 'pickup', NUMBUCKETS)\n",
      "taxifare/trainer/taxifare.py-    dropoff_index, dropoff = feature_cross_latlon(dlat, dlon, 'dropoff', NUMBUCKETS)\n",
      "taxifare/trainer/taxifare.py-    # pickupdropoff is [batchsize, nbuckets**4]\n",
      "--\n",
      "taxifare/trainer/taxifare.py-            _create_fakekey(input_data))  # no key ... tf.identity(parsed['key']))\n",
      "taxifare/trainer/taxifare.py-\n",
      "taxifare/trainer/taxifare.py-\n",
      "taxifare/trainer/taxifare.py:def inference(inputs, metadata, hyperparams):\n",
      "taxifare/trainer/taxifare.py:  NUMBUCKETS = hyperparams['number_buckets']\n",
      "taxifare/trainer/taxifare.py-  input_size = 6 + (NUMBUCKETS**2) * 2 + (NUMBUCKETS**4) + 4\n",
      "taxifare/trainer/taxifare.py-  output_size = metadata.features['fare_amount']['size'] # outputs\n",
      "taxifare/trainer/taxifare.py-\n",
      "taxifare/trainer/taxifare.py:  h = [hyperparams['hidden_layer1_size'],\n",
      "taxifare/trainer/taxifare.py:       hyperparams['hidden_layer2_size'],\n",
      "taxifare/trainer/taxifare.py:       hyperparams['hidden_layer3_size']]\n",
      "taxifare/trainer/taxifare.py-  hidden = tf.contrib.layers.stack(inputs,\n",
      "taxifare/trainer/taxifare.py-                                   tf.contrib.layers.fully_connected,\n",
      "taxifare/trainer/taxifare.py-                                   h, activation_fn=tf.nn.relu,\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "grep -3 hyperparam taxifare/trainer/*.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train once </h2>\n",
    "\n",
    "Here, we package up the code and train as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/\n",
      "taxifare/PKG-INFO\n",
      "taxifare/setup.cfg\n",
      "taxifare/setup.py\n",
      "taxifare/trainer/\n",
      "taxifare/trainer/__init__.py\n",
      "taxifare/trainer/task.py\n",
      "taxifare/trainer/taxifare.py\n",
      "taxifare/trainer.egg-info/\n",
      "taxifare/trainer.egg-info/dependency_links.txt\n",
      "taxifare/trainer.egg-info/PKG-INFO\n",
      "taxifare/trainer.egg-info/SOURCES.txt\n",
      "taxifare/trainer.egg-info/top_level.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://taxifare.tar.gz [Content-Type=application/x-tar]...\n",
      "Uploading   ...training-demos/taxifare/source4b/taxifare.tar.gz: 0 B/7.42 KiB    \r",
      "Uploading   ...training-demos/taxifare/source4b/taxifare.tar.gz: 7.42 KiB/7.42 KiB    \r\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare\n",
    "gsutil cp taxifare.tar.gz gs://cloud-training-demos/taxifare/source4b/taxifare.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos/taxifare/taxi_preproc4b/features_eval...\n",
      "Downloading ...ontent/CPB102/lab4b/taxi_preproc4b/features_eval: 0 B/293.47 KiB    \r",
      "Downloading ...ontent/CPB102/lab4b/taxi_preproc4b/features_eval: 72 KiB/293.47 KiB    \r",
      "Downloading ...ontent/CPB102/lab4b/taxi_preproc4b/features_eval: 144 KiB/293.47 KiB    \r",
      "Downloading ...ontent/CPB102/lab4b/taxi_preproc4b/features_eval: 216 KiB/293.47 KiB    \r",
      "Downloading ...ontent/CPB102/lab4b/taxi_preproc4b/features_eval: 288 KiB/293.47 KiB    \r",
      "Downloading ...ontent/CPB102/lab4b/taxi_preproc4b/features_eval: 293.47 KiB/293.47 KiB    \r\n",
      "Copying gs://cloud-training-demos/taxifare/taxi_preproc4b/features_train...\n",
      "Downloading ...ntent/CPB102/lab4b/taxi_preproc4b/features_train: 0 B/328.52 KiB    \r",
      "Downloading ...ntent/CPB102/lab4b/taxi_preproc4b/features_train: 72 KiB/328.52 KiB    \r",
      "Downloading ...ntent/CPB102/lab4b/taxi_preproc4b/features_train: 144 KiB/328.52 KiB    \r",
      "Downloading ...ntent/CPB102/lab4b/taxi_preproc4b/features_train: 216 KiB/328.52 KiB    \r",
      "Downloading ...ntent/CPB102/lab4b/taxi_preproc4b/features_train: 288 KiB/328.52 KiB    \r",
      "Downloading ...ntent/CPB102/lab4b/taxi_preproc4b/features_train: 328.52 KiB/328.52 KiB    \r\n",
      "Copying gs://cloud-training-demos/taxifare/taxi_preproc4b/info...\n",
      "Downloading file:///content/CPB102/lab4b/taxi_preproc4b/info:    0 B/461 B    \r",
      "Downloading file:///content/CPB102/lab4b/taxi_preproc4b/info:    461 B/461 B    \r\n",
      "Copying gs://cloud-training-demos/taxifare/taxi_preproc4b/metadata.yaml...\n",
      "Downloading ...ontent/CPB102/lab4b/taxi_preproc4b/metadata.yaml: 0 B/2.25 KiB    \r",
      "Downloading ...ontent/CPB102/lab4b/taxi_preproc4b/metadata.yaml: 2.25 KiB/2.25 KiB    \r\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil cp -R gs://cloud-training-demos/taxifare/taxi_preproc4b /content/CPB102/lab4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: Final error after 500 steps = 8.676<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:master/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:master/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: plat  =  [None, 1]<br/>master: latdist  =  [None, 1]<br/>master: pickup  = None<br/>master: pickupdropoff  = None<br/>master: inputs= [None, None]<br/>master: Done training.<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%ml train\n",
    "package_uris: /content/CPB102/lab4b/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - /content/CPB102/lab4b/taxi_preproc4b/features_train\n",
    "  eval_data_paths:\n",
    "    - /content/CPB102/lab4b/taxi_preproc4b/features_eval\n",
    "  metadata_path: /content/CPB102/lab4b/taxi_preproc4b/metadata.yaml\n",
    "  output_path: /content/CPB102/lab4b/taxi_trained\n",
    "  max_steps: 500\n",
    "  hidden1:  64\n",
    "  hidden2:   8\n",
    "  hidden3:   4\n",
    "  nbuckets:  5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hyperparameter training </h2>\n",
    "\n",
    "Now, we carry out the training, but this time on the cloud, and this time with some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job \"trainer_task_160918_235230\" was submitted successfully.<br/>Run \"%ml jobs --name trainer_task_160918_235230\" to view the status of the job.</p><p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=cloud-training-demos&resource=ml.googleapis.com%2Fjob_id%2Ftrainer_task_160918_235230\" target=\"_blank\">here</a> to view cloud log. <br/>Start TensorBoard by running \"%tensorboard start --logdir=&lt;YourLogDir&gt;\".</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ml train --cloud\n",
    "package_uris: gs://cloud-training-demos/taxifare/source4b/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - gs://cloud-training-demos/taxifare/taxi_preproc4b/features_train\n",
    "  eval_data_paths:\n",
    "    - gs://cloud-training-demos/taxifare/taxi_preproc4b/features_eval\n",
    "  metadata_path: gs://cloud-training-demos/taxifare/taxi_preproc4b/metadata.yaml\n",
    "  output_path: gs://cloud-training-demos/taxifare/taxi_trained4b\n",
    "  max_steps: 500\n",
    "  hidden1:  64\n",
    "  hidden2:   8\n",
    "  hidden3:   4\n",
    "  nbuckets:  5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>A training input template is created in next cell for you. See cell input instructions below.</p><table><tr><th>Parameters</th><th>Local Run Required</th><th>Cloud Run Required</th><th>Description</th></tr><tr><td>package_uris</td><td>True</td><td>True</td><td>A GCS or local (for local run only) path to your python training program package.</td></tr><tr><td>python_module</td><td>True</td><td>True</td><td>The module to run.</td></tr><tr><td>scale_tier</td><td>False</td><td>True</td><td>Type of resources requested for the job. On local run, BASIC means 1 master process only, and any other values mean 1 master 1 worker and 1 ps processes. But you can also override the values by setting worker_count and parameter_server_count. On cloud, see service definition for possible values.</td></tr><tr><td>region</td><td>False</td><td>True</td><td>Where the training job runs. For cloud run only.</td></tr><tr><td>args</td><td>False</td><td>False</td><td>Args that will be passed to your training program.</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ml train --cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%ml train [--cloud]\n",
    "package_uris: gs://your-bucket/my-training-package.tar.gz\n",
    "python_module: your_program.your_module\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  string_arg: value\n",
    "  int_arg: value\n",
    "  appendable_arg:\n",
    "    - value1\n",
    "    - value2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access /content/CPB102/lab2d/taxi_trained: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://cloud-training-demos/taxifare/taxi_trained4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 22285. Click <a href=\"/_proxy/34890/\" target=\"_blank\">here</a> to access it.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorboard start --logdir gs://cloud-training-demos/taxifare/taxi_trained4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%tensorboard stop --pid 3222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"c2bce566-56c6-48f5-923e-dce6a38cdc26\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"c2bce566-56c6-48f5-923e-dce6a38cdc26\", [{\"y\": [], \"x\": [], \"type\": \"scatter\", \"name\": \"loss-/content/CPB102/lab2d/taxi_trained/eval\"}, {\"y\": [5.379933834075928, 5.319031238555908, 5.307737827301025, 5.264418601989746, 5.236255168914795, 5.2314229011535645, 5.220809459686279], \"x\": [190, 397, 417, 585, 785, 982, 1000], \"type\": \"scatter\", \"name\": \"error-/content/CPB102/lab2d/taxi_trained/eval\"}, {\"y\": [4.62047815322876, 7.257615089416504, 4.671370506286621, 3.938429594039917, 4.98874044418335], \"x\": [200, 400, 600, 800, 1000], \"type\": \"scatter\", \"name\": \"loss-/content/CPB102/lab2d/taxi_trained/summaries\"}, {\"y\": [], \"x\": [], \"type\": \"scatter\", \"name\": \"error-/content/CPB102/lab2d/taxi_trained/summaries\"}], {\"title\": \"loss,error\", \"xaxis\": {\"title\": \"step\"}, \"yaxis\": {\"title\": \"loss,error\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%ml summary --dir gs://cloud-training-demos/taxifare/taxi_trained4b/summaries  gs://cloud-training-demos/taxifare/taxi_trained4b/eval  --name loss error --step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
