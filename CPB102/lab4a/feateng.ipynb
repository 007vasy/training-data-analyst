{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Feature Engineering </h1>\n",
    "\n",
    "This notebook is Lab4a of CPB 102, Google's course on Machine Learning using Cloud ML.\n",
    "\n",
    "This notebook demonstrates:\n",
    "<ol>\n",
    "<li> Reading data from BigQuery </li>\n",
    "<li> Carrying out preprocessing using the ML SDK </li>\n",
    "<li> Adding feature crosses in TensorFlow </li>\n",
    "</ol> \n",
    "\n",
    "By removing the BigQuery sampling, you can train on the whole dataset.  This will take quite a while, though.\n",
    "\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./cloudml-0.1.3.tar.gz\n",
      "Collecting oauth2client==2.2.0 (from cloudml==0.1.3)\n",
      "Collecting six>=1.10.0 (from cloudml==0.1.3)\n",
      "  Using cached six-1.10.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-dataflow>=0.4.0 (from cloudml==0.1.3)\n",
      "Collecting bs4>=0.0.1 (from cloudml==0.1.3)\n",
      "  Using cached bs4-0.0.1.tar.gz\n",
      "Collecting numpy>=1.10.4 (from cloudml==0.1.3)\n",
      "  Using cached numpy-1.11.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting pillow>=3.2.0 (from cloudml==0.1.3)\n",
      "  Using cached Pillow-3.3.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting dpkt>=1.8.7 (from cloudml==0.1.3)\n",
      "  Using cached dpkt-1.8.8-py2-none-any.whl\n",
      "Collecting nltk>=3.2.1 (from cloudml==0.1.3)\n",
      "  Using cached nltk-3.2.1.tar.gz\n",
      "Collecting httplib2>=0.9.1 (from oauth2client==2.2.0->cloudml==0.1.3)\n",
      "  Using cached httplib2-0.9.2.zip\n",
      "Collecting rsa>=3.1.4 (from oauth2client==2.2.0->cloudml==0.1.3)\n",
      "  Using cached rsa-3.4.2-py2.py3-none-any.whl\n",
      "Collecting pyasn1>=0.1.7 (from oauth2client==2.2.0->cloudml==0.1.3)\n",
      "  Using cached pyasn1-0.1.9-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.0.5 (from oauth2client==2.2.0->cloudml==0.1.3)\n",
      "  Using cached pyasn1_modules-0.0.8-py2.py3-none-any.whl\n",
      "Collecting google-apitools>=0.5.2 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "  Using cached google_apitools-0.5.4-py2-none-any.whl\n",
      "Collecting dill>=0.2.5 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "Collecting protorpc>=0.9.1 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "  Using cached protorpc-0.11.1-py2-none-any.whl\n",
      "Collecting mock>=1.0.1 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "  Using cached mock-2.0.0-py2.py3-none-any.whl\n",
      "Collecting avro>=1.7.7 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "Collecting python-gflags>=2.0 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "  Downloading python-gflags-3.0.7.tar.gz (50kB)\n",
      "Collecting pyyaml>=3.10 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "  Downloading PyYAML-3.12.tar.gz (253kB)\n",
      "Collecting beautifulsoup4 (from bs4>=0.0.1->cloudml==0.1.3)\n",
      "  Using cached beautifulsoup4-4.5.1-py2-none-any.whl\n",
      "Collecting setuptools>=18.5 (from google-apitools>=0.5.2->google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "  Downloading setuptools-27.2.0-py2.py3-none-any.whl (465kB)\n",
      "Collecting funcsigs>=1; python_version < \"3.3\" (from mock>=1.0.1->google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "  Using cached funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting pbr>=0.11 (from mock>=1.0.1->google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "  Using cached pbr-1.10.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: cloudml, bs4, nltk, httplib2, python-gflags, pyyaml\n",
      "  Running setup.py bdist_wheel for cloudml: started\n",
      "  Running setup.py bdist_wheel for cloudml: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/10/dc/70a342013a376e08cdff5cf54c9a04194c08a3cc3191b52ac9\n",
      "  Running setup.py bdist_wheel for bs4: started\n",
      "  Running setup.py bdist_wheel for bs4: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/84/67/d4/9e09d9d5adede2ee1c7b7e8775ba3fbb04d07c4f946f0e4f11\n",
      "  Running setup.py bdist_wheel for nltk: started\n",
      "  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/55/0b/ce/960dcdaec7c9af5b1f81d471a90c8dae88374386efe6e54a50\n",
      "  Running setup.py bdist_wheel for httplib2: started\n",
      "  Running setup.py bdist_wheel for httplib2: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/c7/67/60/e0be8ccfc1e08f8ff1f50d99ea5378e204580ea77b0169fb55\n",
      "  Running setup.py bdist_wheel for python-gflags: started\n",
      "  Running setup.py bdist_wheel for python-gflags: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/17/e1/820c9c1d289adbb466fc23ae3810045a31a566036db975b358\n",
      "  Running setup.py bdist_wheel for pyyaml: started\n",
      "  Running setup.py bdist_wheel for pyyaml: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/f7/79/13f3a12cd723892437c0cfbde1230ab4d82947ff7b3839a4fc\n",
      "Successfully built cloudml bs4 nltk httplib2 python-gflags pyyaml\n",
      "Installing collected packages: httplib2, pyasn1, rsa, pyasn1-modules, six, oauth2client, setuptools, google-apitools, dill, protorpc, funcsigs, pbr, mock, avro, python-gflags, pyyaml, google-cloud-dataflow, beautifulsoup4, bs4, numpy, pillow, dpkt, nltk, cloudml\n",
      "  Found existing installation: httplib2 0.9.2\n",
      "    Uninstalling httplib2-0.9.2:\n",
      "      Successfully uninstalled httplib2-0.9.2\n",
      "  Found existing installation: pyasn1 0.1.9\n",
      "    Uninstalling pyasn1-0.1.9:\n",
      "      Successfully uninstalled pyasn1-0.1.9\n",
      "  Found existing installation: rsa 3.4.2\n",
      "    Uninstalling rsa-3.4.2:\n",
      "      Successfully uninstalled rsa-3.4.2\n",
      "  Found existing installation: pyasn1-modules 0.0.8\n",
      "    Uninstalling pyasn1-modules-0.0.8:\n",
      "      Successfully uninstalled pyasn1-modules-0.0.8\n",
      "  Found existing installation: six 1.10.0\n",
      "    Uninstalling six-1.10.0:\n",
      "      Successfully uninstalled six-1.10.0\n",
      "  Found existing installation: oauth2client 2.0.2\n",
      "    Uninstalling oauth2client-2.0.2:\n",
      "      Successfully uninstalled oauth2client-2.0.2\n",
      "  Found existing installation: setuptools 25.2.0\n",
      "    Uninstalling setuptools-25.2.0:\n",
      "      Successfully uninstalled setuptools-25.2.0\n",
      "  Found existing installation: google-apitools 0.5.4\n",
      "    Uninstalling google-apitools-0.5.4:\n",
      "      Successfully uninstalled google-apitools-0.5.4\n",
      "  Found existing installation: dill 0.2.5\n",
      "    Uninstalling dill-0.2.5:\n",
      "      Successfully uninstalled dill-0.2.5\n",
      "  Found existing installation: protorpc 0.11.1\n",
      "    Uninstalling protorpc-0.11.1:\n",
      "      Successfully uninstalled protorpc-0.11.1\n",
      "  Found existing installation: funcsigs 1.0.2\n",
      "    Uninstalling funcsigs-1.0.2:\n",
      "      Successfully uninstalled funcsigs-1.0.2\n",
      "  Found existing installation: pbr 1.10.0\n",
      "    Uninstalling pbr-1.10.0:\n",
      "      Successfully uninstalled pbr-1.10.0\n",
      "  Found existing installation: mock 2.0.0\n",
      "    Uninstalling mock-2.0.0:\n",
      "      Successfully uninstalled mock-2.0.0\n",
      "  Found existing installation: avro 1.8.1\n",
      "    Uninstalling avro-1.8.1:\n",
      "      Successfully uninstalled avro-1.8.1\n",
      "  Found existing installation: python-gflags 3.0.6\n",
      "    Uninstalling python-gflags-3.0.6:\n",
      "      Successfully uninstalled python-gflags-3.0.6\n",
      "  Found existing installation: PyYAML 3.11\n",
      "    Uninstalling PyYAML-3.11:\n",
      "      Successfully uninstalled PyYAML-3.11\n",
      "  Found existing installation: google-cloud-dataflow 0.4.1\n",
      "    Uninstalling google-cloud-dataflow-0.4.1:\n",
      "      Successfully uninstalled google-cloud-dataflow-0.4.1\n",
      "  Found existing installation: beautifulsoup4 4.5.1\n",
      "    Uninstalling beautifulsoup4-4.5.1:\n",
      "      Successfully uninstalled beautifulsoup4-4.5.1\n",
      "  Found existing installation: bs4 0.0.1\n",
      "    Uninstalling bs4-0.0.1:\n",
      "      Successfully uninstalled bs4-0.0.1\n",
      "  Found existing installation: numpy 1.11.1\n",
      "    Uninstalling numpy-1.11.1:\n",
      "      Successfully uninstalled numpy-1.11.1\n",
      "  Found existing installation: Pillow 3.3.1\n",
      "    Uninstalling Pillow-3.3.1:\n",
      "      Successfully uninstalled Pillow-3.3.1\n",
      "  Found existing installation: dpkt 1.8.8\n",
      "    Uninstalling dpkt-1.8.8:\n",
      "      Successfully uninstalled dpkt-1.8.8\n",
      "  Found existing installation: nltk 3.2.1\n",
      "    Uninstalling nltk-3.2.1:\n",
      "      Successfully uninstalled nltk-3.2.1\n",
      "  Found existing installation: cloudml 0.1.2\n",
      "    Uninstalling cloudml-0.1.2:\n",
      "      Successfully uninstalled cloudml-0.1.2\n",
      "Successfully installed avro-1.8.1 beautifulsoup4-4.5.1 bs4-0.0.1 cloudml-0.1.3 dill-0.2.5 dpkt-1.8.8 funcsigs-1.0.2 google-apitools-0.5.4 google-cloud-dataflow-0.4.1 httplib2-0.9.2 mock-2.0.0 nltk-3.2.1 numpy-1.11.1 oauth2client-2.2.0 pbr-1.10.0 pillow-3.3.1 protorpc-0.11.1 pyasn1-0.1.9 pyasn1-modules-0.0.8 python-gflags-3.0.7 pyyaml-3.12 rsa-3.4.2 setuptools-27.2.0 six-1.10.0\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "# remember to \"Reset Session\" if you execute this cell -- this is needed to restart the Python kernel with updated package\n",
    "#gsutil cp gs://cloud-ml/sdk/cloudml-0.1.4.tar.gz .\n",
    "pip install --force-reinstall --upgrade cloudml-0.1.3.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Specifying query to pull the data </h1>\n",
    "\n",
    "The full dataset is 1 billion rows. For experimentation, let's sample it to create 10,000 samples.\n",
    "Towards the end of this notebook, we'll remove the limit and train on the full dataset.\n",
    "We're also using BigQuery sampling to divide up the data into train (50%), valid (25%) and test (25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT\n",
      "  DAYOFWEEK(pickup_datetime)*1.0 AS dayofweek,\n",
      "  HOUR(pickup_datetime)*1.0 AS hourofday,\n",
      "  pickup_longitude, pickup_latitude, \n",
      "  dropoff_longitude, dropoff_latitude,\n",
      "  passenger_count,\n",
      "  (tolls_amount + fare_amount) as fare_amount\n",
      "FROM\n",
      "  [nyc-tlc:yellow.trips]\n",
      "WHERE\n",
      "    trip_distance > 0\n",
      "    AND fare_amount >= 2.5\n",
      "    AND pickup_longitude > -78\n",
      "    AND pickup_longitude < -70\n",
      "    AND dropoff_longitude > -78\n",
      "    AND dropoff_longitude < -70\n",
      "    AND pickup_latitude > 37\n",
      "    AND pickup_latitude < 45\n",
      "    AND dropoff_latitude > 37\n",
      "    AND dropoff_latitude < 45\n",
      "    AND passenger_count > 0 \n",
      "   AND ABS(HASH(pickup_datetime)) % 100000 == 1 AND ABS(HASH(pickup_datetime)) % 4 < 2\n"
     ]
    }
   ],
   "source": [
    "def create_query(phase, EVERY_N):\n",
    "  \"\"\"\n",
    "  phase: 1=train 2=valid, 3=test\n",
    "  \"\"\"\n",
    "  base_query = \"\"\"\n",
    "SELECT\n",
    "  DAYOFWEEK(pickup_datetime)*1.0 AS dayofweek,\n",
    "  HOUR(pickup_datetime)*1.0 AS hourofday,\n",
    "  pickup_longitude, pickup_latitude, \n",
    "  dropoff_longitude, dropoff_latitude,\n",
    "  passenger_count,\n",
    "  (tolls_amount + fare_amount) as fare_amount\n",
    "FROM\n",
    "  [nyc-tlc:yellow.trips]\n",
    "WHERE\n",
    "    trip_distance > 0\n",
    "    AND fare_amount >= 2.5\n",
    "    AND pickup_longitude > -78\n",
    "    AND pickup_longitude < -70\n",
    "    AND dropoff_longitude > -78\n",
    "    AND dropoff_longitude < -70\n",
    "    AND pickup_latitude > 37\n",
    "    AND pickup_latitude < 45\n",
    "    AND dropoff_latitude > 37\n",
    "    AND dropoff_latitude < 45\n",
    "    AND passenger_count > 0 \n",
    "  \"\"\"\n",
    "\n",
    "  if EVERY_N == None:\n",
    "    query = base_query\n",
    "  else:\n",
    "    query = \"{0} AND ABS(HASH(pickup_datetime)) % {1} == 1\".format(base_query, EVERY_N)\n",
    "\n",
    "  if phase < 2:\n",
    "    # training\n",
    "    query = \"{0} AND ABS(HASH(pickup_datetime)) % 4 < 2\".format(query)\n",
    "  else:\n",
    "    query = \"{0} AND ABS(HASH(pickup_datetime)) % 4 == {1}\".format(query, phase)\n",
    "    \n",
    "  return query\n",
    "    \n",
    "print create_query(1, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the query above in https://bigquery.cloud.google.com/table/nyc-tlc:yellow.trips if you want to see what it does (ADD LIMIT 10 to the query!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing features using Cloud ML SDK </h2>\n",
    "\n",
    "We could discretize the lat-lon columns using the SDK, but we'll defer that to TensorFlow to enable it to be a hyper-parameter if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml/sdk/cloudml-0.1.4.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.ml.features as features\n",
    "\n",
    "import google.cloud.ml as ml\n",
    "print ml.sdk_location\n",
    "\n",
    "class TaxifareFeatures(object):\n",
    "  csv_columns = ('dayofweek', 'hourofday', 'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','fare_amount')\n",
    "  fare_amount = features.target('fare_amount').regression()\n",
    "  pcount = features.numeric('passenger_count') # default is to scale all values in [-1, 1]\n",
    "  plat = features.numeric('pickup_latitude') # .discretize(buckets=10), sparse=False),\n",
    "  dlat = features.numeric('dropoff_latitude')\n",
    "  plon = features.numeric('pickup_longitude')\n",
    "  dlon = features.numeric('dropoff_longitude')\n",
    "  dayofweek = features.numeric('dayofweek').identity()\n",
    "  hourofday = features.numeric('hourofday').identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing Dataflow job </h2>\n",
    "\n",
    "This code reads from BigQuery and runs the above preprocessing, saving the data on Google Cloud.  Make sure to change the BUCKET and PROJECt variable to be yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.direct_runner.DirectPipelineResult at 0x7f11905c4710>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.dataflow.io.tfrecordio as tfrecordio\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "\n",
    "# Change as needed\n",
    "BUCKET = 'cloud-training-demos'\n",
    "PROJECT = 'cloud-training-demos'\n",
    "EVERY_N = 100*1000 # 100 * 1000 # Change this to None to preprocess full dataset\n",
    "\n",
    "# Run full data preprocessing on Cloud, else locally\n",
    "RUNNER = 'DirectPipelineRunner'\n",
    "if EVERY_N == None:\n",
    "   RUNNER = 'BlockingDataflowPipelineRunner'\n",
    "\n",
    "# defines\n",
    "feature_set = TaxifareFeatures()\n",
    "pipeline = beam.Pipeline(argv=['--project', PROJECT,\n",
    "                               '--runner', RUNNER,\n",
    "                               '--job_name', 'lab3a',\n",
    "                               '--extra_package', ml.sdk_location,\n",
    "                               '--no_save_main_session', 'True',  # to prevent pickling and uploading Datalab itself!\n",
    "                               '--staging_location', 'gs://{0}/taxifare/staging'.format(BUCKET),\n",
    "                               '--temp_location', 'gs://{0}/taxifare/temp'.format(BUCKET)])\n",
    "\n",
    "\n",
    "\n",
    "# defines\n",
    "feature_set = TaxifareFeatures()\n",
    "OUTPUT_DIR = 'gs://{0}/taxifare/taxi_preproc4a'.format(BUCKET)\n",
    "pipeline = beam.Pipeline(argv=['--project', PROJECT])\n",
    "train_query = create_query(1, EVERY_N)\n",
    "valid_query = create_query(2, EVERY_N)\n",
    "\n",
    "train = pipeline | 'read_train' >> beam.Read(beam.io.BigQuerySource(query=train_query))\n",
    "eval = pipeline | 'read_valid' >> beam.Read(beam.io.BigQuerySource(query=valid_query))\n",
    "\n",
    "(metadata, train_features, eval_features) = ((train, eval) |\n",
    "    ml.Preprocess('Preprocess', feature_set))\n",
    "\n",
    "train_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix=os.path.join(OUTPUT_DIR, 'features_train'),\n",
    "    file_name_suffix='',\n",
    "    shard_file=True,\n",
    "    compress_file=True)\n",
    "eval_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix=os.path.join(OUTPUT_DIR, 'features_eval'),\n",
    "    file_name_suffix='',\n",
    "    shard_file=True,\n",
    "    compress_file=True)\n",
    "(metadata, train_features, eval_features) | (\n",
    "    io.SavePreprocessed('SavingData', OUTPUT_DIR,\n",
    "                        file_parameters_list=[\n",
    "                            os.path.join(OUTPUT_DIR, 'metadata.yaml'),\n",
    "                            train_parameters, eval_parameters]))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "gs://cloud-training-demos/taxifare/taxi_preproc4a/features_eval-00000-of-00001\n",
      "gs://cloud-training-demos/taxifare/taxi_preproc4a/features_train-00000-of-00001\n",
      "gs://cloud-training-demos/taxifare/taxi_preproc4a/info\n",
      "gs://cloud-training-demos/taxifare/taxi_preproc4a/metadata.yaml\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://cloud-training-demos/taxifare/taxi_preproc4a/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Training </h2>\n",
    "\n",
    "Training requires you to package up your TensorFlow model into a Python package. We've done this in the directory 'taxifare'\n",
    "\n",
    "In that code, the hourofday and dayofweek are one-hot encoded.  The latitude and longitude are discretized, and feature-crossed. The whole model is then trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/\n",
      "taxifare/PKG-INFO\n",
      "taxifare/setup.cfg\n",
      "taxifare/setup.py\n",
      "taxifare/trainer/\n",
      "taxifare/trainer/__init__.py\n",
      "taxifare/trainer/task.py\n",
      "taxifare/trainer/taxifare.py\n",
      "taxifare/trainer.egg-info/\n",
      "taxifare/trainer.egg-info/dependency_links.txt\n",
      "taxifare/trainer.egg-info/PKG-INFO\n",
      "taxifare/trainer.egg-info/SOURCES.txt\n",
      "taxifare/trainer.egg-info/top_level.txt\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare\n",
    "#gsutil cp taxifare.tar.gz gs://cloud-training-demos/taxifare/source/taxifare.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos/taxifare/taxi_preproc4a/features_eval-00000-of-00001...\n",
      "Downloading ...ab4a/taxi_preproc4a/features_eval-00000-of-00001: 0 B/3 B    \r",
      "Downloading ...ab4a/taxi_preproc4a/features_eval-00000-of-00001: 3 B/3 B    \r\n",
      "Copying gs://cloud-training-demos/taxifare/taxi_preproc4a/features_train-00000-of-00001...\n",
      "Downloading ...b4a/taxi_preproc4a/features_train-00000-of-00001: 0 B/328.58 KiB    \r",
      "Downloading ...b4a/taxi_preproc4a/features_train-00000-of-00001: 72 KiB/328.58 KiB    \r",
      "Downloading ...b4a/taxi_preproc4a/features_train-00000-of-00001: 144 KiB/328.58 KiB    \r",
      "Downloading ...b4a/taxi_preproc4a/features_train-00000-of-00001: 216 KiB/328.58 KiB    \r",
      "Downloading ...b4a/taxi_preproc4a/features_train-00000-of-00001: 288 KiB/328.58 KiB    \r",
      "Downloading ...b4a/taxi_preproc4a/features_train-00000-of-00001: 328.58 KiB/328.58 KiB    \r\n",
      "Copying gs://cloud-training-demos/taxifare/taxi_preproc4a/info...\n",
      "Downloading file:///content/CPB102/lab4a/taxi_preproc4a/info:    0 B/441 B    \r",
      "Downloading file:///content/CPB102/lab4a/taxi_preproc4a/info:    441 B/441 B    \r\n",
      "Copying gs://cloud-training-demos/taxifare/taxi_preproc4a/metadata.yaml...\n",
      "Downloading ...ontent/CPB102/lab4a/taxi_preproc4a/metadata.yaml: 0 B/2.25 KiB    \r",
      "Downloading ...ontent/CPB102/lab4a/taxi_preproc4a/metadata.yaml: 2.25 KiB/2.25 KiB    \r\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil cp -R gs://cloud-training-demos/taxifare/taxi_preproc4a /content/CPB102/lab4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>A training input template is created in next cell for you. See cell input instructions below.</p><table><tr><th>Parameters</th><th>Local Run Required</th><th>Cloud Run Required</th><th>Description</th></tr><tr><td>package_uris</td><td>True</td><td>True</td><td>A GCS or local (for local run only) path to your python training program package.</td></tr><tr><td>python_module</td><td>True</td><td>True</td><td>The module to run.</td></tr><tr><td>scale_tier</td><td>False</td><td>True</td><td>Type of resources requested for the job. On local run, BASIC means 1 master process only, and any other values mean 1 master 1 worker and 1 ps processes. But you can also override the values by setting worker_count and parameter_server_count. On cloud, see service definition for possible values.</td></tr><tr><td>region</td><td>False</td><td>True</td><td>Where the training job runs. For cloud run only.</td></tr><tr><td>args</td><td>False</td><td>False</td><td>Args that will be passed to your training program.</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ml train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job master -> {localhost:33495}<br/>master: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:33495<br/>master: plat  =  [64, 1]<br/>master: latdist  =  [64, 1]<br/>master: pickup  =  [64, 25]<br/>master: pickupdropoff  =  [64, 625]<br/>master: inputs= [64, 685]<br/>master: Starting the loop.<br/>master: <br/>master: Step 200: loss = 6.88 (0.042 sec)<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%ml train\n",
    "package_uris: /content/CPB102/lab4a/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - /content/CPB102/lab4a/taxi_preproc4a/features_train-00000-of-00001\n",
    "  eval_data_paths:\n",
    "    - /content/CPB102/lab4a/taxi_preproc4a/features_eval-00000-of-00001\n",
    "  metadata_path: /content/CPB102/lab4a/taxi_preproc4a/metadata.yaml\n",
    "  output_path: /content/CPB102/lab4a/taxi_trained\n",
    "  max_steps: 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type %ml train into an empty cell, run it, fill in some params and execute it again. (create a new code cell and try it out!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job \"trainer_task_160917_222322\" was submitted successfully.<br/>Run \"%ml jobs --name trainer_task_160917_222322\" to view the status of the job.</p><p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=cloud-training-demos&resource=ml.googleapis.com%2Fjob_id%2Ftrainer_task_160917_222322\" target=\"_blank\">here</a> to view cloud log. <br/>Start TensorBoard by running \"%tensorboard start --logdir=&lt;YourLogDir&gt;\".</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ml train --cloud\n",
    "package_uris: gs://cloud-training-demos/taxifare/source/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - gs://cloud-training-demos/taxifare/taxi_preproc4a/features_train-00000-of-00001\n",
    "  eval_data_paths:\n",
    "    - gs://cloud-training-demos/taxifare/taxi_preproc4a/features_eval-00000-of-00001\n",
    "  metadata_path: gs://cloud-training-demos/taxifare/taxi_preproc4a/metadata.yaml\n",
    "  output_path: gs://cloud-training-demos/taxifare/taxi_trained4a\n",
    "  max_steps: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access /content/CPB102/lab2d/taxi_trained: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://cloud-training-demos/taxifare/taxi_trained4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 22285. Click <a href=\"/_proxy/34890/\" target=\"_blank\">here</a> to access it.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorboard start --logdir gs://cloud-training-demos/taxifare/taxi_trained4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%tensorboard stop --pid 3222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"c2bce566-56c6-48f5-923e-dce6a38cdc26\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"c2bce566-56c6-48f5-923e-dce6a38cdc26\", [{\"y\": [], \"x\": [], \"type\": \"scatter\", \"name\": \"loss-/content/CPB102/lab2d/taxi_trained/eval\"}, {\"y\": [5.379933834075928, 5.319031238555908, 5.307737827301025, 5.264418601989746, 5.236255168914795, 5.2314229011535645, 5.220809459686279], \"x\": [190, 397, 417, 585, 785, 982, 1000], \"type\": \"scatter\", \"name\": \"error-/content/CPB102/lab2d/taxi_trained/eval\"}, {\"y\": [4.62047815322876, 7.257615089416504, 4.671370506286621, 3.938429594039917, 4.98874044418335], \"x\": [200, 400, 600, 800, 1000], \"type\": \"scatter\", \"name\": \"loss-/content/CPB102/lab2d/taxi_trained/summaries\"}, {\"y\": [], \"x\": [], \"type\": \"scatter\", \"name\": \"error-/content/CPB102/lab2d/taxi_trained/summaries\"}], {\"title\": \"loss,error\", \"xaxis\": {\"title\": \"step\"}, \"yaxis\": {\"title\": \"loss,error\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%ml summary --dir gs://cloud-training-demos/taxifare/taxi_trained4a/summaries  gs://cloud-training-demos/taxifare/taxi_trained4a/eval  --name loss error --step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
