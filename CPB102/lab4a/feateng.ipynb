{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Feature Engineering </h1>\n",
    "\n",
    "In this notebook, you will learn how to incorporate feature engineering into your pipeline.\n",
    "<ol>\n",
    "<li> Adding feature crosses in TensorFlow </li>\n",
    "<li> Reading data from BigQuery </li>\n",
    "<li> Carrying out preprocessing using tf.transform </li>\n",
    "</ol>\n",
    "\n",
    "Table of Contents:\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "pip install --upgrade tensorflow_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0-rc1\n",
      "gs://cloud-ml/sdk/cloudml-0.1.9-alpha.dataflow.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.ml as ml\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import shutil\n",
    "print tf.__version__\n",
    "print ml.sdk_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Environment variables for project and bucket </h2>\n",
    "\n",
    "Change the cell below to reflect your Project ID and bucket name. See Lab 3a for setup instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos'    # CHANGE THIS\n",
    "BUCKET = 'cloud-training-demos-ml'  # CHANGE THIS\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT # for bash\n",
    "os.environ['BUCKET'] = BUCKET # for bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Specifying query to pull the data </h2>\n",
    "\n",
    "The full dataset is 1 billion rows. For experimentation, let's sample it to create 10,000 samples.\n",
    "Later, we'll remove the limit and train on the full dataset.\n",
    "We're also using BigQuery sampling to pull out independent training and validation samples.\n",
    "\n",
    "Note that because the test dataset is now different, we can not really compare test statistics between this and the previous .csv methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT\n",
      "  (tolls_amount + fare_amount) AS fare_amount,\n",
      "  CONCAT(STRING(pickup_datetime), STRING(pickup_longitude), STRING(pickup_latitude), STRING(dropoff_latitude), STRING(dropoff_longitude)) AS key,\n",
      "  DAYOFWEEK(pickup_datetime)*1.0 AS dayofweek,\n",
      "  HOUR(pickup_datetime)*1.0 AS hourofday,\n",
      "  pickup_longitude AS pickuplon,\n",
      "  pickup_latitude AS pickuplat,\n",
      "  dropoff_longitude AS dropofflon,\n",
      "  dropoff_latitude AS dropofflat,\n",
      "  passenger_count*1.0 AS passengers,\n",
      "FROM\n",
      "  [nyc-tlc:yellow.trips]\n",
      "WHERE\n",
      "  trip_distance > 0\n",
      "  AND fare_amount >= 2.5\n",
      "  AND pickup_longitude > -78\n",
      "  AND pickup_longitude < -70\n",
      "  AND dropoff_longitude > -78\n",
      "  AND dropoff_longitude < -70\n",
      "  AND pickup_latitude > 37\n",
      "  AND pickup_latitude < 45\n",
      "  AND dropoff_latitude > 37\n",
      "  AND dropoff_latitude < 45\n",
      "  AND passenger_count > 0\n",
      "   AND ABS(HASH(pickup_datetime)) % 100000 == 2\n"
     ]
    }
   ],
   "source": [
    "def create_query(phase, EVERY_N):\n",
    "  \"\"\"\n",
    "  phase: 1=train 2=valid\n",
    "  \"\"\"\n",
    "  base_query = \"\"\"\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount,\n",
    "  CONCAT(STRING(pickup_datetime), STRING(pickup_longitude), STRING(pickup_latitude), STRING(dropoff_latitude), STRING(dropoff_longitude)) AS key,\n",
    "  DAYOFWEEK(pickup_datetime)*1.0 AS dayofweek,\n",
    "  HOUR(pickup_datetime)*1.0 AS hourofday,\n",
    "  pickup_longitude AS pickuplon,\n",
    "  pickup_latitude AS pickuplat,\n",
    "  dropoff_longitude AS dropofflon,\n",
    "  dropoff_latitude AS dropofflat,\n",
    "  passenger_count*1.0 AS passengers,\n",
    "FROM\n",
    "  [nyc-tlc:yellow.trips]\n",
    "WHERE\n",
    "  trip_distance > 0\n",
    "  AND fare_amount >= 2.5\n",
    "  AND pickup_longitude > -78\n",
    "  AND pickup_longitude < -70\n",
    "  AND dropoff_longitude > -78\n",
    "  AND dropoff_longitude < -70\n",
    "  AND pickup_latitude > 37\n",
    "  AND pickup_latitude < 45\n",
    "  AND dropoff_latitude > 37\n",
    "  AND dropoff_latitude < 45\n",
    "  AND passenger_count > 0\n",
    "  \"\"\"\n",
    "\n",
    "  if EVERY_N == None:\n",
    "    if phase < 2:\n",
    "      # training\n",
    "      query = \"{0} AND ABS(HASH(pickup_datetime)) % 4 < 2\".format(base_query)\n",
    "    else:\n",
    "      query = \"{0} AND ABS(HASH(pickup_datetime)) % 4 == {1}\".format(base_query, phase)\n",
    "  else:\n",
    "      query = \"{0} AND ABS(HASH(pickup_datetime)) % {1} == {2}\".format(base_query, EVERY_N, phase)\n",
    "\n",
    "  \n",
    "    \n",
    "  return query\n",
    "    \n",
    "print create_query(2, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the query above in https://bigquery.cloud.google.com/table/nyc-tlc:yellow.trips if you want to see what it does (ADD LIMIT 10 to the query!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing Dataflow job from BigQuery </h2>\n",
    "\n",
    "This code reads from BigQuery and saving the data on Google Cloud.  We can do preprocessing inside Dataflow, but then we'll have to remember to repeat the prepreprocessing during inference. It is better to use tf.transform which will do this book-keeping for you, or to do preprocessing within your TensorFlow.\n",
    "\n",
    "So, our \"preprocessing\" is simply a data transformation.  While we could read from BQ directly from TensorFlow, it is less expensive to export to CSV and do the training off CSV.\n",
    "\n",
    "If you are running on the Cloud, you should go to the GCP Console (https://console.cloud.google.com/dataflow) to look at the status of the job. If you are running locally, you'll get a Running bar and it will take up to 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://cloud-training-demos-ml/taxifare/taxi_preproc/features_eval-00000-of-00001.tfrecord.gz#1484431738311206...\n",
      "Removing gs://cloud-training-demos-ml/taxifare/taxi_preproc/features_train-00000-of-00001.tfrecord.gz#1484431731394942...\n",
      "Removing gs://cloud-training-demos-ml/taxifare/taxi_preproc/metadata.yaml#1484431721525262...\n",
      "/ [3/3 objects] 100% Done                                                       \n",
      "Operation completed over 3 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m rm -rf gs://$BUCKET/taxifare/taxi_preproc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change as needed\n",
    "EVERY_N = 50 * 1000 # Change this to None to preprocess full dataset\n",
    "\n",
    "# Direct runs locally; Dataflow runs on the Cloud.\n",
    "RUNNER = 'DirectPipelineRunner'\n",
    "#RUNNER = 'DataflowPipelineRunner'\n",
    "\n",
    "OUTPUT_DIR = 'gs://{0}/taxifare/taxi_preproc/'.format(BUCKET)\n",
    "options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "    'project': PROJECT,\n",
    "    'extra_packages': ['tensorflow_transform'],\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'no_save_main_session': True\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "pipeline = beam.Pipeline(RUNNER, options=opts)\n",
    "p = beam.Pipeline()\n",
    "transform_fn, train_dataset, eval_dataset = preprocess_all(\n",
    "      pipeline, create_query(1, 100000), create_query(2, 100000), None, OUTPUT_DIR)\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gsutil ls gs://$BUCKET/taxifare/taxi_preproc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil cp -R gs://$BUCKET/taxifare/taxi_preproc /content/training-data-analyst/CPB102/lab4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Training </h2>\n",
    "\n",
    "Training requires you to package up your TensorFlow model into a Python package. We've done this in the directory 'taxifare'\n",
    "\n",
    "In that code, the latitude and longitude are discretized, and feature-crossed. The hourofday and dayofweek are divided into buckets that reflect typical traffic patterns.  The whole model is then trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "grep -A 40 create_inputs taxifare/trainer/taxifare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mlalpha train\n",
    "package_uris: /content/training-data-analyst/CPB102/lab4a/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths: /content/training-data-analyst/CPB102/lab4a/taxi_preproc4a/features_train*\n",
    "  eval_data_paths: /content/training-data-analyst/CPB102/lab4a/taxi_preproc4a/features_eval*\n",
    "  metadata_path: /content/training-data-analyst/CPB102/lab4a/taxi_preproc4a/metadata.yaml\n",
    "  output_path: /content/training-data-analyst/CPB102/lab4a/taxi_trained\n",
    "  max_steps: 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
