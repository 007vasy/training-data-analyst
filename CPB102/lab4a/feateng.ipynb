{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Feature Engineering </h1>\n",
    "\n",
    "In this notebook, you will learn how to incorporate feature engineering into your pipeline.\n",
    "<ol>\n",
    "<li> Reading data from BigQuery </li>\n",
    "<li> Carrying out preprocessing using the ML SDK </li>\n",
    "<li> Adding feature crosses in TensorFlow </li>\n",
    "</ol>\n",
    "\n",
    "Table of Contents:\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import google.cloud.ml as ml\n",
    "import tensorflow as tf\n",
    "print tf.__version__\n",
    "print ml.sdk_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Environment variables for project and bucket </h2>\n",
    "\n",
    "Change the cell below to reflect your Project ID and bucket name. See Lab 3a for setup instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos'    # CHANGE THIS\n",
    "BUCKET = 'cloud-training-demos-ml'  # CHANGE THIS\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT # for bash\n",
    "os.environ['BUCKET'] = BUCKET # for bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Specifying query to pull the data </h2>\n",
    "\n",
    "The full dataset is 1 billion rows. For experimentation, let's sample it to create 10,000 samples.\n",
    "Later, we'll remove the limit and train on the full dataset.\n",
    "We're also using BigQuery sampling to pull out independent training and validation samples.\n",
    "\n",
    "Note that because the test dataset is now different, we can not really compare test statistics between this and the previous .csv methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_query(phase, EVERY_N):\n",
    "  \"\"\"\n",
    "  phase: 1=train 2=valid\n",
    "  \"\"\"\n",
    "  base_query = \"\"\"\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount,\n",
    "  CONCAT(STRING(pickup_datetime), STRING(pickup_longitude), STRING(pickup_latitude), STRING(dropoff_latitude), STRING(dropoff_longitude)) AS key,\n",
    "  DAYOFWEEK(pickup_datetime)*1.0 AS dayofweek,\n",
    "  HOUR(pickup_datetime)*1.0 AS hourofday,\n",
    "  pickup_longitude AS pickuplon,\n",
    "  pickup_latitude AS pickuplat,\n",
    "  dropoff_longitude AS dropofflon,\n",
    "  dropoff_latitude AS dropofflat,\n",
    "  passenger_count*1.0 AS passengers,\n",
    "FROM\n",
    "  [nyc-tlc:yellow.trips]\n",
    "WHERE\n",
    "  trip_distance > 0\n",
    "  AND fare_amount >= 2.5\n",
    "  AND pickup_longitude > -78\n",
    "  AND pickup_longitude < -70\n",
    "  AND dropoff_longitude > -78\n",
    "  AND dropoff_longitude < -70\n",
    "  AND pickup_latitude > 37\n",
    "  AND pickup_latitude < 45\n",
    "  AND dropoff_latitude > 37\n",
    "  AND dropoff_latitude < 45\n",
    "  AND passenger_count > 0\n",
    "  \"\"\"\n",
    "\n",
    "  if EVERY_N == None:\n",
    "    if phase < 2:\n",
    "      # training\n",
    "      query = \"{0} AND ABS(HASH(pickup_datetime)) % 4 < 2\".format(base_query)\n",
    "    else:\n",
    "      query = \"{0} AND ABS(HASH(pickup_datetime)) % 4 == {1}\".format(base_query, phase)\n",
    "  else:\n",
    "      query = \"{0} AND ABS(HASH(pickup_datetime)) % {1} == {2}\".format(base_query, EVERY_N, phase)\n",
    "\n",
    "  \n",
    "    \n",
    "  return query\n",
    "    \n",
    "print create_query(2, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the query above in https://bigquery.cloud.google.com/table/nyc-tlc:yellow.trips if you want to see what it does (ADD LIMIT 10 to the query!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing features using Cloud ML SDK </h2>\n",
    "\n",
    "We could discretize the lat-lon columns using the SDK, but we'll defer that to TensorFlow to enable it to be a hyper-parameter if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import google.cloud.ml.features as features\n",
    "\n",
    "import google.cloud.ml as ml\n",
    "print ml.sdk_location\n",
    "\n",
    "class TaxifareFeatures(object):\n",
    "  csv_columns = ('dayofweek', 'hourofday', 'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','fare_amount')\n",
    "  fare_amount = features.target('fare_amount').continuous()\n",
    "  pcount = features.numeric('passenger_count').scale()\n",
    "  plat = features.numeric('pickup_latitude').scale()\n",
    "  dlat = features.numeric('dropoff_latitude').scale()\n",
    "  plon = features.numeric('pickup_longitude').scale()\n",
    "  dlon = features.numeric('dropoff_longitude').scale()\n",
    "  dayofweek = features.numeric('dayofweek').identity()\n",
    "  hourofday = features.numeric('hourofday').identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing Dataflow job from BigQuery </h2>\n",
    "\n",
    "This code reads from BigQuery and runs the above preprocessing, saving the data on Google Cloud.\n",
    "\n",
    "If you are running on the Cloud, you should go to the GCP Console (https://console.cloud.google.com/dataflow) to look at the status of the job. If you are running locally, you'll get a Running bar and it will take up to 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil -m rm -r -f gs://$BUCKET/taxifare/taxi_preproc4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.dataflow.io.tfrecordio as tfrecordio\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Change as needed\n",
    "EVERY_N = 50 * 1000 # Change this to None to preprocess full dataset\n",
    "\n",
    "# Direct runs locally; Dataflow runs on the Cloud.\n",
    "RUNNER = 'DirectPipelineRunner'\n",
    "#RUNNER = 'DataflowPipelineRunner'\n",
    "\n",
    "OUTPUT_DIR = 'gs://{0}/taxifare/taxi_preproc4a/'.format(BUCKET)\n",
    "options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "    'project': PROJECT,\n",
    "    'extra_packages': [ml.sdk_location],\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'no_save_main_session': True\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "pipeline = beam.Pipeline(RUNNER, options=opts)\n",
    "\n",
    "# defines\n",
    "feature_set = TaxifareFeatures()\n",
    "train_query = create_query(1, EVERY_N)\n",
    "valid_query = create_query(2, EVERY_N)\n",
    "train = pipeline | 'read_train' >> beam.Read(beam.io.BigQuerySource(query=train_query))\n",
    "eval = pipeline | 'read_valid' >> beam.Read(beam.io.BigQuerySource(query=valid_query))\n",
    "\n",
    "(metadata, train_features, eval_features) = ((train, eval) |\n",
    "   'Preprocess' >> ml.Preprocess(feature_set))\n",
    "\n",
    "(metadata\n",
    "   | 'SaveMetadata'\n",
    "   >> io.SaveMetadata(os.path.join(OUTPUT_DIR, 'metadata.yaml')))\n",
    "(train_features\n",
    "   | 'WriteTraining'\n",
    "   >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_train')))\n",
    "(eval_features\n",
    "   | 'WriteEval'\n",
    "   >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_eval')))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gsutil ls gs://$BUCKET/taxifare/taxi_preproc4a/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil cp -R gs://$BUCKET/taxifare/taxi_preproc4a /content/training-data-analyst/CPB102/lab4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Training </h2>\n",
    "\n",
    "Training requires you to package up your TensorFlow model into a Python package. We've done this in the directory 'taxifare'\n",
    "\n",
    "In that code, the latitude and longitude are discretized, and feature-crossed. The hourofday and dayofweek are divided into buckets that reflect typical traffic patterns.  The whole model is then trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "grep -A 40 create_inputs taxifare/trainer/taxifare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mlalpha train\n",
    "package_uris: /content/training-data-analyst/CPB102/lab4a/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths: /content/training-data-analyst/CPB102/lab4a/taxi_preproc4a/features_train*\n",
    "  eval_data_paths: /content/training-data-analyst/CPB102/lab4a/taxi_preproc4a/features_eval*\n",
    "  metadata_path: /content/training-data-analyst/CPB102/lab4a/taxi_preproc4a/metadata.yaml\n",
    "  output_path: /content/training-data-analyst/CPB102/lab4a/taxi_trained\n",
    "  max_steps: 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
