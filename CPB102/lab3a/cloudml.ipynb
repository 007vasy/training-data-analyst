{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Scaling up ML using Cloud ML </h1>\n",
    "\n",
    "This notebook is Lab3a of CPB 102, Google's course on Machine Learning using Cloud ML.\n",
    "\n",
    "In this notebook, we take a previously developed TensorFlow model to predict taxifare rides and package it up so that it can be run in Cloud ML. For now, we'll run this on a small dataset. The model that was developed is rather simplistic, and therefore, the accuracy of the model is not great either.  However, this notebook illustrates *how* to package up a TensorFlow model to run it within Cloud ML. \n",
    "\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "Later in the course, we will look at ways to make a more effective machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "pip install --upgrade tensorflow_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import google.cloud.ml as ml\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "print tf.__version__\n",
    "print ml.sdk_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Environment variables for project and bucket </h2>\n",
    "\n",
    "Change the cell below to reflect your Project ID and bucket name. Note that:\n",
    "<ol>\n",
    "<li> Your project id is the *unique* string that identifies your project (not the project name). You can find this from the GCP Console dashboard's Home page.  My dashboard reads:  <b>Project ID:</b> cloud-training-demos </li>\n",
    "<li> Cloud training often involves saving and restoring model files. Therefore, we should <b>create a single-region bucket</b>. If you don't have a bucket already, I suggest that you create one from the GCP console (because it will dynamically check whether the bucket name you want is available) </li>\n",
    "</ol>\n",
    "\n",
    "The next cell ensures that your bucket is writeable by Cloud ML. You need to do this only once (not once per notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos'    # CHANGE THIS\n",
    "BUCKET = 'cloud-training-demos-ml'  # CHANGE THIS\n",
    "REGION = 'us-central1' # CHANGE THIS\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT # for bash\n",
    "os.environ['BUCKET'] = BUCKET # for bash\n",
    "os.environ['REGION'] = REGION # for bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gcloud beta ml init-project -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the project and bucket settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "echo \"project=$PROJECT\"\n",
    "echo \"bucket=$BUCKET\"\n",
    "echo \"region=$REGION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Packaging up the code </h2>\n",
    "\n",
    "Take your code and put into a standard Python package structure.  model.py and task.py contain the Tensorflow code from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!find taxifare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat taxifare/trainer/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Find absolute paths to your data </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the absolute paths below. /content is mapped in Datalab to where the home icon takes you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf /content/training-data-analyst/CPB102/lab3a/taxi_trained\n",
    "head -1 /content/training-data-analyst/CPB102/lab1a/taxi-train.csv\n",
    "head -1 /content/training-data-analyst/CPB102/lab1a/taxi-valid.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running the Python module from the command-line </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:/content/training-data-analyst/CPB102/lab3a/taxifare\n",
    "python -m trainer.task \\\n",
    "   --train_data_paths=\"/content/training-data-analyst/CPB102/lab1a/taxi-train*\" \\\n",
    "   --eval_data_paths=/content/training-data-analyst/CPB102/lab1a/taxi-valid.csv  \\\n",
    "   --output_dir=/content/training-data-analyst/CPB102/lab3a/taxi_trained \\\n",
    "   --num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls /content/training-data-analyst/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running locally using gcloud </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gcloud components update --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "gcloud beta ml local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=/content/training-data-analyst/CPB102/lab3a/taxifare/trainer \\\n",
    "   -- \\\n",
    "   --train_data_paths=/content/training-data-analyst/CPB102/lab1a/taxi-train.csv \\\n",
    "   --eval_data_paths=/content/training-data-analyst/CPB102/lab1a/taxi-valid.csv  \\\n",
    "   --num_epochs=10 \\\n",
    "   --output_dir=/content/training-data-analyst/CPB102/lab3a/taxi_trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datalab.mlalpha import TensorBoard\n",
    "TensorBoard().start('/content/training-data-analyst/CPB102/lab3a/taxi_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TensorBoard().stop(7279)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls /content/training-data-analyst/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Submit training job using gcloud </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "echo $BUCKET\n",
    "gsutil rm -rf gs://${BUCKET}/taxifare/smallinput/\n",
    "gsutil cp /content/training-data-analyst/CPB102/lab1a/*.csv gs://${BUCKET}/taxifare/smallinput/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/taxifare/smallinput/taxi_trained\n",
    "JOBNAME=lab3a_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil rm -rf $OUTDIR\n",
    "gcloud beta ml jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=/content/training-data-analyst/CPB102/lab3a/taxifare/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC \\\n",
    "   --runtime-version=1.0 \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/taxifare/smallinput/taxi-train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/taxifare/smallinput/taxi-valid*\"  \\\n",
    "   --output_dir=$OUTDIR \\\n",
    "   --num_epochs=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/training-data-analyst/CPB102/lab3a/taxi_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_transform import coders\n",
    "from tensorflow_transform.beam import impl as tft\n",
    "from tensorflow_transform.beam import io\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "\n",
    "from tensorflow_transform import api\n",
    "from tensorflow_transform import mappers\n",
    "\n",
    "INPUT_COLUMNS = ['pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "\n",
    "class PathConstants:\n",
    "  def __init__(self):\n",
    "    self.TEMP_DIR = 'tmp'\n",
    "    self.TRANSFORM_FN_DIR = 'transform_fn'\n",
    "    self.RAW_METADATA_DIR = 'raw_metadata'\n",
    "    self.TRANSFORMED_METADATA_DIR = 'transformed_metadata'\n",
    "    self.TRANSFORMED_TRAIN_DATA_FILE_PREFIX = 'features_train'\n",
    "    self.TRANSFORMED_EVAL_DATA_FILE_PREFIX = 'features_eval'\n",
    "    self.TRANSFORMED_PREDICT_DATA_FILE_PREFIX = 'features_predict'\n",
    "    self.TRAIN_RESULTS_FILE = 'train_results'\n",
    "    self.DEPLOY_SAVED_MODEL_DIR = 'saved_model'\n",
    "    self.MODEL_EVALUATIONS_FILE = 'model_evaluations'\n",
    "    self.BATCH_PREDICTION_RESULTS_FILE = 'batch_prediction_results'\n",
    "    \n",
    "def make_preprocessing_fn():\n",
    "  # stop-gap ...\n",
    "  def _scalar_to_vector(scalar):\n",
    "    # FeatureColumns expect shape (batch_size, 1), not just (batch_size)\n",
    "    return api.map(lambda x: tf.expand_dims(x, -1), scalar)\n",
    "  \n",
    "  def preprocessing_fn(inputs):\n",
    "    result = {LABEL_COLUMN: _scalar_to_vector(inputs[LABEL_COLUMN])}\n",
    "    for name in INPUT_COLUMNS:\n",
    "      result[name] = _scalar_to_vector(mappers.scale_to_0_1(inputs[name]))\n",
    "\n",
    "    # use tft.map() to create new columns\n",
    "    # tft.scale_to_0_1\n",
    "    # tft.map(tf.sparse_column_with_keys, inputs['gender'], Statistic({'M', 'F'})\n",
    "    # tft.string_to_int(inputs[name], frequency_threshold=frequency_threshold)\n",
    "    return result\n",
    "\n",
    "  return preprocessing_fn\n",
    "\n",
    "def make_input_schema(mode):\n",
    "  input_schema = ({} if mode == tf.contrib.learn.ModeKeys.INFER\n",
    "            else {LABEL_COLUMN: tf.FixedLenFeature(shape=[], dtype=tf.float64)})\n",
    "  for name in INPUT_COLUMNS:\n",
    "    input_schema[name] = tf.FixedLenFeature(\n",
    "        shape=[], dtype=tf.float64, default_value=0)\n",
    "  input_schema = dataset_schema.from_feature_spec(input_schema)\n",
    "  return input_schema\n",
    "\n",
    "def make_coder(schema, mode):\n",
    "  column_names = [] if mode == tf.contrib.learn.ModeKeys.INFER else [LABEL_COLUMN]\n",
    "  column_names.extend(INPUT_COLUMNS)\n",
    "  coder = coders.CsvCoder(column_names, schema)\n",
    "  return coder\n",
    "\n",
    "def preprocess(pipeline, training_data, eval_data, predict_data, output_dir, mode=tf.contrib.learn.ModeKeys.TRAIN):\n",
    "  path_constants = PathConstants()\n",
    "  work_dir = os.path.join(output_dir, path_constants.TEMP_DIR)\n",
    "  \n",
    "  # create schema\n",
    "  input_schema = make_input_schema(mode)\n",
    "\n",
    "  # coder\n",
    "  coder = make_coder(input_schema, mode)\n",
    "\n",
    "  # 3) Read from text using the coder.\n",
    "  train_data = (\n",
    "      pipeline\n",
    "      | 'ReadTrainingData' >> beam.io.ReadFromText(training_data)\n",
    "      | 'ParseTrainingCsv' >> beam.Map(coder.decode))\n",
    "\n",
    "  evaluate_data = (\n",
    "      pipeline\n",
    "      | 'ReadEvalData' >> beam.io.ReadFromText(eval_data)\n",
    "      | 'ParseEvalCsv' >> beam.Map(coder.decode))\n",
    "\n",
    "  # metadata\n",
    "  input_metadata = dataset_metadata.DatasetMetadata(schema=input_schema)\n",
    "\n",
    "  _ = (input_metadata\n",
    "       | 'WriteInputMetadata' >> io.WriteMetadata(\n",
    "           os.path.join(output_dir, path_constants.RAW_METADATA_DIR),\n",
    "           pipeline=pipeline))\n",
    "\n",
    "  preprocessing_fn = make_preprocessing_fn()\n",
    "  (train_dataset, train_metadata), transform_fn = (\n",
    "      (train_data, input_metadata)\n",
    "      | 'AnalyzeAndTransform' >> tft.AnalyzeAndTransformDataset(\n",
    "          preprocessing_fn, work_dir))\n",
    "\n",
    "  # WriteTransformFn writes transform_fn and metadata to fixed subdirectories\n",
    "  # of output_dir, which are given by path_constants.TRANSFORM_FN_DIR and\n",
    "  # path_constants.TRANSFORMED_METADATA_DIR.\n",
    "  transform_fn_is_written = (transform_fn | io.WriteTransformFn(output_dir))\n",
    "\n",
    "  # TODO(b/34231369) Remember to eventually also save the statistics.\n",
    "\n",
    "  (evaluate_dataset, evaluate_metadata) = (\n",
    "      ((evaluate_data, input_metadata), transform_fn)\n",
    "      | 'TransformEval' >> tft.TransformDataset())\n",
    "\n",
    "  train_coder = coders.ExampleProtoCoder(train_metadata.schema)\n",
    "  _ = (train_dataset\n",
    "       | 'SerializeTrainExamples' >> beam.Map(train_coder.encode)\n",
    "       | 'WriteTraining'\n",
    "       >> beam.io.WriteToTFRecord(\n",
    "           os.path.join(output_dir,\n",
    "                        path_constants.TRANSFORMED_TRAIN_DATA_FILE_PREFIX),\n",
    "           file_name_suffix='.tfrecord.gz'))\n",
    "\n",
    "  evaluate_coder = coders.ExampleProtoCoder(evaluate_metadata.schema)\n",
    "  _ = (evaluate_dataset\n",
    "       | 'SerializeEvalExamples' >> beam.Map(evaluate_coder.encode)\n",
    "       | 'WriteEval'\n",
    "       >> beam.io.WriteToTFRecord(\n",
    "           os.path.join(output_dir,\n",
    "                        path_constants.TRANSFORMED_EVAL_DATA_FILE_PREFIX),\n",
    "           file_name_suffix='.tfrecord.gz'))\n",
    "\n",
    "  if predict_data:\n",
    "    predict_mode = tf.contrib.learn.ModeKeys.INFER\n",
    "    predict_schema = make_input_schema(mode=predict_mode)\n",
    "    tsv_coder = make_coder(predict_schema, mode=predict_mode)\n",
    "    predict_coder = coders.ExampleProtoCoder(predict_schema)\n",
    "    _ = (pipeline\n",
    "         | 'ReadPredictData' >> beam.io.ReadFromText(predict_data,\n",
    "                                                     coder=tsv_coder)\n",
    "         # TODO(b/35194257) Obviate the need for this explicit serialization.\n",
    "         | 'EncodePredictData' >> beam.Map(predict_coder.encode)\n",
    "         | 'WritePredictData' >> beam.io.WriteToTFRecord(\n",
    "             os.path.join(output_dir,\n",
    "                          path_constants.TRANSFORMED_PREDICT_DATA_FILE_PREFIX),\n",
    "             file_name_suffix='.tfrecord.gz'))\n",
    "\n",
    "  # Workaround b/35366670, to ensure that training and eval don't start before\n",
    "  # the transform_fn is written.\n",
    "  train_dataset |= beam.Map(\n",
    "      lambda x, y: x, y=beam.pvalue.AsSingleton(transform_fn_is_written))\n",
    "  evaluate_dataset |= beam.Map(\n",
    "      lambda x, y: x, y=beam.pvalue.AsSingleton(transform_fn_is_written))\n",
    "\n",
    "  return transform_fn, train_dataset, evaluate_dataset\n",
    "\n",
    "\n",
    "train_data_paths='/content/training-data-analyst/CPB102/lab1a/taxi-train.csv' \n",
    "eval_data_paths='/content/training-data-analyst/CPB102/lab1a/taxi-valid.csv'  \n",
    "output_dir='/content/training-data-analyst/CPB102/lab3a/taxi_preproc' \n",
    "predict_data_paths=None\n",
    "p = beam.Pipeline()\n",
    "transform_fn, train_dataset, eval_dataset = preprocess(\n",
    "      p, train_data_paths, eval_data_paths, predict_data_paths, output_dir)\n",
    "\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prediction </h2>\n",
    "\n",
    "Make sure that the training job has completed before proceeding to this step (check the log above)\n",
    "\n",
    "To predict the taxifare for new inputs, you first have to deploy the trained model (deleting a previous one if necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "# Work around https://buganizer.corp.google.com/issues/31730085\n",
    "gsutil cp gs://$BUCKET/taxifare/taxi_preproc/metadata.yaml gs://$BUCKET/taxifare/taxi_trained/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mlalpha delete --name taxifare.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mlalpha deploy --name taxifare.v1 --path gs://$BUCKET/taxifare/taxi_trained/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "import google.cloud.ml.features as features\n",
    "from google.cloud.ml import session_bundle\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1beta1', credentials=credentials,\n",
    "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1beta1_discovery.json')\n",
    "\n",
    "request_data = {'instances':\n",
    "  [\n",
    "    {'examples':\n",
    "      {\n",
    "        'pickup_longitude': -73.885262,\n",
    "        'pickup_latitude': 40.773008,\n",
    "        'dropoff_longitude': -73.987232,\n",
    "        'dropoff_latitude': 40.732403,\n",
    "        'passenger_count': 2,\n",
    "        'fare_amount': -999\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'taxifare', 'v1')\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "print \"response={0}\".format(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
