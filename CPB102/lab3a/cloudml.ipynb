{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Scaling up ML using Cloud ML </h1>\n",
    "\n",
    "This notebook is Lab3a of CPB 102, Google's course on Machine Learning using Cloud ML.\n",
    "\n",
    "In this notebook, we take a previously developed TensorFlow model to predict taxifare rides and package it up so that it can be run in Cloud ML. For now, we'll run this on a small dataset. The model that was developed is rather simplistic, and therefore, the accuracy of the model is not great either.  However, this notebook illustrates *how* to package up a TensorFlow model to run it within Cloud ML. \n",
    "\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "Later in the course, we will look at ways to make a more effective machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import google.cloud.ml as ml\n",
    "import tensorflow as tf\n",
    "print tf.__version__\n",
    "print ml.sdk_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Environment variables for project and bucket </h2>\n",
    "\n",
    "Change the cell below to reflect your Project ID and bucket name. Note that:\n",
    "<ol>\n",
    "<li> Your project id is the *unique* string that identifies your project (not the project name). You can find this from the GCP Console dashboard's Home page.  My dashboard reads:  <b>Project ID:</b> cloud-training-demos </li>\n",
    "<li> Cloud training often involves saving and restoring model files. Therefore, we should <b>create a single-region bucket</b>. If you don't have a bucket already, I suggest that you create one from the GCP console (because it will dynamically check whether the bucket name you want is available) </li>\n",
    "</ol>\n",
    "\n",
    "The next cell ensures that your bucket is writeable by Cloud ML. You need to do this only once (not once per notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos'    # CHANGE THIS\n",
    "BUCKET = 'cloud-training-demos-ml'  # CHANGE THIS\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT # for bash\n",
    "os.environ['BUCKET'] = BUCKET # for bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "PROJECT_ID=$PROJECT\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AUTH_TOKEN\" https://ml.googleapis.com/v1beta1/projects/$PROJECT_ID:getConfig | python -c \"import json; import sys; response = json.load(sys.stdin); print response['serviceAccount']\")\n",
    "echo \"Authorizing Cloud ML service account $SVC_ACCOUNT to write to $BUCKET\"\n",
    "gsutil acl ch -u $SVC_ACCOUNT:WRITE gs://$BUCKET/\n",
    "gsutil defacl ch -u $SVC_ACCOUNT:O gs://$BUCKET/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the project and bucket settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "echo \"project=$PROJECT\"\n",
    "echo \"bucket=$BUCKET\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Package up TensorFlow model </h2>\n",
    "\n",
    "The TensorFlow model needs to be packaged up into a Python module.  This has a very specific folder structure (you'd typically maintain this exact structure in your source repository). Then, you create an archive of it using the 'tar' command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task.py and model.py contain the code that you wrote earlier in Datalab. We moved it into a Python module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running training locally </h2>\n",
    "\n",
    "Once you have a packaged TensorFlow model, you can run training by passing in the paths to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the absolute paths below. /content is mapped in Datalab to where the home icon takes you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf /content/training-data-analyst/CPB102/lab3a/taxi_trained\n",
    "head -1 /content/training-data-analyst/CPB102/lab1a/taxi-train.csv\n",
    "head -1 /content/training-data-analyst/CPB102/lab1a/taxi-valid.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gcloud --quiet components update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:/content/training-data-analyst/CPB102/lab3a/taxifare\n",
    "python -m trainer.task \\\n",
    "   --train_data_paths=/content/training-data-analyst/CPB102/lab1a/taxi-train.csv \\\n",
    "   --eval_data_paths=/content/training-data-analyst/CPB102/lab1a/taxi-valid.csv  \\\n",
    "   --output_dir=/content/training-data-analyst/CPB102/lab3a/taxi_trained \\\n",
    "   --num_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud beta ml local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=/content/training-data-analyst/CPB102/lab3a/taxifare.tar.gz \\\n",
    "   -- \\\n",
    "   --train_data_paths=/content/training-data-analyst/CPB102/lab1a/taxi-train.csv \\\n",
    "   --eval_data_paths=/content/training-data-analyst/CPB102/lab1a/taxi-valid.csv  \\\n",
    "   --output_dir=/content/training-data-analyst/CPB102/lab3a/taxi_trained \\\n",
    "   --num_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%mlalpha train\n",
    "package_uris: /content/training-data-analyst/CPB102/lab3a/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "runtime_version: 1.0.0-rc2\n",
    "args:\n",
    "  train_data_paths: /content/training-data-analyst/CPB102/lab1a/taxi-train.csv\n",
    "  eval_data_paths: /content/training-data-analyst/CPB102/lab1a/taxi-valid.csv\n",
    "  output_dir: /content/training-data-analyst/CPB102/lab3a/taxi_trained\n",
    "  num_epochs: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls /content/training-data-analyst/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mlalpha summary --dir /content/training-data-analyst/CPB102/lab3a/taxi_trained/summaries  /content/training-data-analyst/CPB102/lab3a/taxi_trained/eval  --name loss accuracy --step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is the RMSE on the training dataset; the accuracy is the RMSE on the validation dataset.  The loss is reported frequently since it is computed anyway, but we compute the accuracy only once every 30s of training, so there won't be as many points associated with the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%tensorboard start --logdir /content/training-data-analyst/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%tensorboard stop --pid 6326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training on cloud </h2>\n",
    "\n",
    "In order to train on the cloud, we have to copy the model and data to our bucket on Google Cloud Storage (GCS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare\n",
    "gsutil cp taxifare.tar.gz gs://$BUCKET/taxifare/source/taxifare.tar.gz\n",
    "gsutil cp ../lab1a/*.csv  gs://$BUCKET/taxifare/input/\n",
    "gsutil -m rm -r -f gs://$BUCKET/taxifare/taxi_preproc\n",
    "gsutil -m rm -r -f gs://$BUCKET/taxifare/taxi_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run your preprocessor, you have to change the input and output to be on GCS.  \n",
    "\n",
    "Using DirectPipelineRunner runs Dataflow locally, but the inputs & outputs are on the cloud. Using BlockingDataflowPipelineRunner will use Cloud Dataflow (and take much longer because of the overhead involved for such a small dataset). To see the status of your BlockingDataflowPipelineRunner job, visit https://console.cloud.google.com/dataflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.dataflow.io.tfrecordio as tfrecordio\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "\n",
    "# Change as needed\n",
    "RUNNER = 'DirectPipelineRunner'  # \n",
    "#RUNNER = 'BlockingDataflowPipelineRunner'\n",
    "\n",
    "# defines\n",
    "feature_set = TaxifareFeatures()\n",
    "OUTPUT_DIR = 'gs://{0}/taxifare/taxi_preproc'.format(BUCKET)\n",
    "\n",
    "pipeline = beam.Pipeline(argv=['--project', PROJECT,\n",
    "                               '--runner', RUNNER,\n",
    "                               '--job_name', 'lab3a',\n",
    "                               '--extra_package', ml.sdk_location,\n",
    "                               '--no_save_main_session', 'True',  # to prevent pickling and uploading Datalab itself!\n",
    "                               '--staging_location', 'gs://{0}/taxifare/staging'.format(BUCKET),\n",
    "                               '--temp_location', 'gs://{0}/taxifare/temp'.format(BUCKET)])\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "training_data = beam.io.TextFileSource(\n",
    "    'gs://{0}/taxifare/input/taxi-train.csv'.format(BUCKET),\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "train = pipeline | beam.Read('ReadTrainingData', training_data)\n",
    "eval_data = beam.io.TextFileSource(\n",
    "    'gs://{0}/taxifare/input/taxi-valid.csv'.format(BUCKET),\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "eval = pipeline | beam.Read('ReadEvalData', eval_data)\n",
    "\n",
    "\n",
    "(metadata, train_features, eval_features) = ((train, eval) |\n",
    "   'Preprocess' >> ml.Preprocess(feature_set))\n",
    "\n",
    "(metadata\n",
    "   | 'SaveMetadata'\n",
    "   >> io.SaveMetadata(os.path.join(OUTPUT_DIR, 'metadata.yaml')))\n",
    "(train_features\n",
    "   | 'WriteTraining'\n",
    "   >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_train')))\n",
    "(eval_features\n",
    "   | 'WriteEval'\n",
    "   >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_eval')))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls gs://$BUCKET/taxifare/taxi_preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, submit the training job to the cloud.  Cloud ML jobs usually take hours and are, therefore, queued. It may be a couple of minutes before your job starts being executed. This being a small job, though, the task should complete a few seconds later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up parameters for mlapha command.\n",
    "package_uris = 'gs://' + BUCKET + '/taxifare/source/taxifare.tar.gz'\n",
    "train_data_paths = 'gs://' + BUCKET + '/taxifare/taxi_preproc/features_train*'\n",
    "eval_data_paths = 'gs://' + BUCKET + '/taxifare/taxi_preproc/features_eval*'\n",
    "metadata_path = 'gs://' + BUCKET + '/taxifare/taxi_preproc/metadata.yaml'\n",
    "output_path = 'gs://' + BUCKET + '/taxifare/taxi_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mlalpha train --cloud\n",
    "package_uris: $package_uris\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths: $train_data_paths\n",
    "  eval_data_paths: $eval_data_paths\n",
    "  metadata_path: $metadata_path\n",
    "  output_path: $output_path\n",
    "  max_steps: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mlalpha jobs --name trainer_task_170114_222854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prediction </h2>\n",
    "\n",
    "Make sure that the training job has completed before proceeding to this step (check the log above)\n",
    "\n",
    "To predict the taxifare for new inputs, you first have to deploy the trained model (deleting a previous one if necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "# Work around https://buganizer.corp.google.com/issues/31730085\n",
    "gsutil cp gs://$BUCKET/taxifare/taxi_preproc/metadata.yaml gs://$BUCKET/taxifare/taxi_trained/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mlalpha delete --name taxifare.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mlalpha deploy --name taxifare.v1 --path gs://$BUCKET/taxifare/taxi_trained/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "import google.cloud.ml.features as features\n",
    "from google.cloud.ml import session_bundle\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1beta1', credentials=credentials,\n",
    "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1beta1_discovery.json')\n",
    "\n",
    "request_data = {'instances':\n",
    "  [\n",
    "    {'examples':\n",
    "      {\n",
    "        'pickup_longitude': -73.885262,\n",
    "        'pickup_latitude': 40.773008,\n",
    "        'dropoff_longitude': -73.987232,\n",
    "        'dropoff_latitude': 40.732403,\n",
    "        'passenger_count': 2,\n",
    "        'fare_amount': -999\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'taxifare', 'v1')\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "print \"response={0}\".format(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
