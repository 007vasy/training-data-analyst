{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Scaling up ML using Cloud ML </h1>\n",
    "\n",
    "This notebook is Lab3a of CPB 102, Google's course on Machine Learning using Cloud ML.\n",
    "\n",
    "In this notebook, we take a previously developed TensorFlow model to predict taxifare rides and package it up so that it can be run in Cloud ML. For now, we'll run this on a small dataset. The model that was developed is rather simplistic, and therefore, the accuracy of the model is not great either.  However, this notebook illustrates *how* to package up a TensorFlow model to run it within Cloud ML. \n",
    "\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./cloudml-0.1.3.latest.tar.gz\n",
      "Collecting oauth2client==2.2.0 (from cloudml==0.1.3)\n",
      "Requirement already up-to-date: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.3)\n",
      "Requirement already up-to-date: google-cloud-dataflow>=0.4.0 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.3)\n",
      "Requirement already up-to-date: bs4>=0.0.1 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.3)\n",
      "Requirement already up-to-date: numpy>=1.10.4 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.3)\n",
      "Requirement already up-to-date: pillow>=3.2.0 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.3)\n",
      "Requirement already up-to-date: dpkt>=1.8.7 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.3)\n",
      "Requirement already up-to-date: nltk>=3.2.1 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.3)\n",
      "Requirement already up-to-date: httplib2>=0.9.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client==2.2.0->cloudml==0.1.3)\n",
      "Requirement already up-to-date: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from oauth2client==2.2.0->cloudml==0.1.3)\n",
      "Requirement already up-to-date: pyasn1>=0.1.7 in /usr/local/lib/python2.7/dist-packages (from oauth2client==2.2.0->cloudml==0.1.3)\n",
      "Requirement already up-to-date: pyasn1-modules>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from oauth2client==2.2.0->cloudml==0.1.3)\n",
      "Requirement already up-to-date: google-apitools>=0.5.2 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "Requirement already up-to-date: dill>=0.2.5 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "Requirement already up-to-date: protorpc>=0.9.1 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "Requirement already up-to-date: mock>=1.0.1 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "Requirement already up-to-date: avro>=1.7.7 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "Collecting python-gflags>=2.0 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "  Downloading python-gflags-3.0.7.tar.gz (50kB)\n",
      "Collecting pyyaml>=3.10 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "  Downloading PyYAML-3.12.tar.gz (253kB)\n",
      "Requirement already up-to-date: beautifulsoup4 in /usr/local/lib/python2.7/dist-packages (from bs4>=0.0.1->cloudml==0.1.3)\n",
      "Collecting setuptools>=18.5 (from google-apitools>=0.5.2->google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "  Downloading setuptools-27.1.2-py2.py3-none-any.whl (464kB)\n",
      "Requirement already up-to-date: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=1.0.1->google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "Requirement already up-to-date: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=1.0.1->google-cloud-dataflow>=0.4.0->cloudml==0.1.3)\n",
      "Building wheels for collected packages: cloudml, python-gflags, pyyaml\n",
      "  Running setup.py bdist_wheel for cloudml: started\n",
      "  Running setup.py bdist_wheel for cloudml: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/f8/ce/b0/1c64075e3a786baad108498b59900c0b9e9ef44a645d75c67c\n",
      "  Running setup.py bdist_wheel for python-gflags: started\n",
      "  Running setup.py bdist_wheel for python-gflags: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/17/e1/820c9c1d289adbb466fc23ae3810045a31a566036db975b358\n",
      "  Running setup.py bdist_wheel for pyyaml: started\n",
      "  Running setup.py bdist_wheel for pyyaml: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/f7/79/13f3a12cd723892437c0cfbde1230ab4d82947ff7b3839a4fc\n",
      "Successfully built cloudml python-gflags pyyaml\n",
      "Installing collected packages: oauth2client, cloudml, python-gflags, pyyaml, setuptools\n",
      "  Found existing installation: oauth2client 2.0.2\n",
      "    Uninstalling oauth2client-2.0.2:\n",
      "      Successfully uninstalled oauth2client-2.0.2\n",
      "  Found existing installation: cloudml 0.1.2\n",
      "    Uninstalling cloudml-0.1.2:\n",
      "      Successfully uninstalled cloudml-0.1.2\n",
      "  Found existing installation: python-gflags 3.0.6\n",
      "    Uninstalling python-gflags-3.0.6:\n",
      "      Successfully uninstalled python-gflags-3.0.6\n",
      "  Found existing installation: PyYAML 3.11\n",
      "    Uninstalling PyYAML-3.11:\n",
      "      Successfully uninstalled PyYAML-3.11\n",
      "  Found existing installation: setuptools 25.2.0\n",
      "    Uninstalling setuptools-25.2.0:\n",
      "      Successfully uninstalled setuptools-25.2.0\n",
      "Successfully installed cloudml-0.1.3 oauth2client-2.2.0 python-gflags-3.0.7 pyyaml-3.12 setuptools-27.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-ml/sdk/cloudml-0.1.3.latest.tar.gz...\n",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                0 B/548.29 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                72 KiB/548.29 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                144 KiB/548.29 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                216 KiB/548.29 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                288 KiB/548.29 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                360 KiB/548.29 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                432 KiB/548.29 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                504 KiB/548.29 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                548.29 KiB/548.29 KiB    \r\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil cp gs://cloud-ml/sdk/cloudml-0.1.3.latest.tar.gz .\n",
    "pip install --upgrade cloudml-0.1.3.latest.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Write code for preprocessing and feature engineering </h2>\n",
    "\n",
    "Realistic ML models involve a fair bit of preprocessing and feature engineering. The standard Cloud ML pipeline expects this. We haven't covered this in class yet, so we'll just pull out the input variables and pass them through untransformed.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Datalab can generate the following template code for you.  Just type <b>%ml features</b> into an empty cell, and then fill out the path, headers, target, id.  Running that cell in turn will create Python code to define features. You can then edit it. (try it out by creating a new code block, and starting with %ml features in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import google.cloud.ml.features as features\n",
    "\n",
    "class TaxifareFeatures(object):\n",
    "  \"\"\"This class is generated from command line:\n",
    "        %ml features\n",
    "        path: ../lab1a/taxi-train.csv\n",
    "        headers: pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,fare_amount\n",
    "        target: fare_amount\n",
    "        Please modify it as appropriate!!!\n",
    "  \"\"\"\n",
    "  csv_columns = ('pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','fare_amount')\n",
    "  fare_amount = features.target('fare_amount').regression()\n",
    "  attrs = [\n",
    "      features.numeric('pickup_longitude').identity(),\n",
    "      features.numeric('dropoff_longitude').identity(),\n",
    "      features.numeric('passenger_count').identity(),\n",
    "      features.numeric('pickup_latitude').identity(),\n",
    "      features.numeric('dropoff_latitude').identity(),\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Dataflow pipeline for preprocessing </h2>\n",
    "\n",
    "Dataflow pipeline code can also be created using code generation in Datalab.  Type <b>%ml preprocess</b> into an empty cell, run it, fill in some params and execute it again. (create a new code cell and try it out!)\n",
    "\n",
    "Note that this code references the features class above (TaxifareFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.direct_runner.DirectPipelineResult at 0x7f94c82d42d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# header\n",
    "\"\"\"\n",
    "Following code is generated from command line:\n",
    "%%ml preprocess\n",
    "train_data_path: ../lab1a/taxi-train.csv\n",
    "eval_data_path: ../lab1a/taxi-valid.csv\n",
    "data_format: CSV\n",
    "output_dir: ./taxi_preproc\n",
    "feature_set_class_name: TaxifareFeatures\n",
    "\n",
    "Please modify as appropriate!!!\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.dataflow.io.tfrecordio as tfrecordio\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "\n",
    "# defines\n",
    "feature_set = TaxifareFeatures()\n",
    "OUTPUT_DIR = './taxi_preproc'\n",
    "pipeline = beam.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "training_data = beam.io.TextFileSource(\n",
    "    '../lab1a/taxi-train.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "train = pipeline | beam.Read('ReadTrainingData', training_data)\n",
    "eval_data = beam.io.TextFileSource(\n",
    "    '../lab1a/taxi-valid.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "eval = pipeline | beam.Read('ReadEvalData', eval_data)\n",
    "(metadata, train_features, eval_features) = ((train, eval) |\n",
    "    ml.Preprocess('Preprocess', feature_set))\n",
    "train_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix=os.path.join(OUTPUT_DIR, 'features_train'),\n",
    "    file_name_suffix='',\n",
    "    shard_file=False,\n",
    "    compress_file=True)\n",
    "eval_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix=os.path.join(OUTPUT_DIR, 'features_eval'),\n",
    "    file_name_suffix='',\n",
    "    shard_file=False,\n",
    "    compress_file=True)\n",
    "(metadata, train_features, eval_features) | (\n",
    "    io.SavePreprocessed('SavingData', OUTPUT_DIR,\n",
    "                        file_parameters_list=[\n",
    "                            os.path.join(OUTPUT_DIR, 'metadata.yaml'),\n",
    "                            train_parameters, eval_parameters]))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above preprocessing code creates TFRecords, an efficient compressed format that is suitable for repeated training and hyperparameter tuning. This is what our TensorFlow model receives. In addition, the preprocessing pipeline creates metadata.yaml, a set of statistics computed from the input data that is necessary for many of the input transformations covered in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_eval  features_train  info  metadata.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!ls taxi_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns:\r\n",
      "  dropoff_latitude:\r\n",
      "    max: 41.366138\r\n",
      "    mean: 40.751464661690754\r\n",
      "    min: 40.514429\r\n",
      "    name: dropoff_latitude\r\n",
      "    type: numeric\r\n",
      "  dropoff_longitude:\r\n",
      "    max: -73.137393\r\n",
      "    mean: -73.97474299191431\r\n",
      "    min: -74.417107\r\n",
      "    name: dropoff_longitude\r\n",
      "    type: numeric\r\n",
      "  fare_amount:\r\n",
      "    max: 88.0\r\n",
      "    mean: 11.195111969111972\r\n",
      "    min: 2.5\r\n",
      "    name: fare_amount\r\n",
      "    scenario: continuous\r\n",
      "    type: target\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 taxi_preproc/metadata.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Package up TensorFlow model </h2>\n",
    "\n",
    "The TensorFlow model needs to be packaged up into a Python module.  This has a very specific folder structure (you'd typically do this using Linux commands) or by dragging-and-dropping files into the appropriate folder structure. Then, you create an archive of it using the 'tar' command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/\n",
      "taxifare/PKG-INFO\n",
      "taxifare/setup.cfg\n",
      "taxifare/setup.py\n",
      "taxifare/trainer/\n",
      "taxifare/trainer/.taxifare.py.swp\n",
      "taxifare/trainer/__init__.py\n",
      "taxifare/trainer/task.py\n",
      "taxifare/trainer/taxifare.py\n",
      "taxifare/trainer.egg-info/\n",
      "taxifare/trainer.egg-info/dependency_links.txt\n",
      "taxifare/trainer.egg-info/PKG-INFO\n",
      "taxifare/trainer.egg-info/SOURCES.txt\n",
      "taxifare/trainer.egg-info/top_level.txt\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only three of those files ones that you would actually edit.\n",
    "\n",
    "The first is setup.py.  You would change it to reflect your module name, author, author_email and description. You might add Python packages that you depend upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "from setuptools import find_packages\r\n",
      "from setuptools import setup\r\n",
      "\r\n",
      "REQUIRED_PACKAGES = [\r\n",
      "]\r\n",
      "\r\n",
      "setup(\r\n",
      "    name='taxifare',\r\n",
      "    version='0.1',\r\n",
      "    author = 'Google',\r\n",
      "    author_email = 'training-feedback@cloud.google.com',\r\n",
      "    install_requires=REQUIRED_PACKAGES,\r\n",
      "    packages=find_packages(),\r\n",
      "    include_package_data=True,\r\n",
      "    description='CPB102 taxifare in Cloud ML',\r\n",
      "    requires=[]\r\n",
      ")\r\n"
     ]
    }
   ],
   "source": [
    "!grep -v \"^#\" taxifare/setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, the second is task.py. This file is canonical code to run your TensorFlow model by reading data in batches, setting up summary statistics, etc.  Most of this code will, in the future, move away from your Python module. For now, simply do a string-replace of 'taxifare' with the name of your Python module and add any hyperparameters you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import taxifare\r\n",
      "  parser.add_argument(\"--train_data_paths\", type=str, action='append')\r\n",
      "  parser.add_argument(\"--eval_data_paths\", type=str, action='append')\r\n",
      "  parser.add_argument(\"--metadata_path\", type=str)\r\n",
      "  parser.add_argument(\"--output_path\", type=str)\r\n",
      "  parser.add_argument(\"--max_steps\", type=int, default=2000)\r\n",
      "  parser.add_argument(\"--hidden1\", type=int, default=300)\r\n",
      "  parser.add_argument(\"--hidden2\", type=int, default=200)\r\n",
      "  parser.add_argument(\"--hidden3\", type=int, default=100)\r\n",
      "  \"\"\"Train taxifare for a number of steps.\"\"\"\r\n",
      "  # test on taxifare.\r\n",
      "      _, train_examples = taxifare.read_examples(\r\n",
      "          taxifare.create_inputs(metadata, input_data=train_examples))\r\n",
      "      output = taxifare.inference(inputs, metadata, layer_sizes)\r\n",
      "      loss = taxifare.loss(output, targets)\r\n",
      "      train_op, global_step = taxifare.training(loss,\r\n",
      "    placeholder, inputs, _, keys = taxifare.create_inputs(metadata)\r\n",
      "    output = taxifare.inference(inputs, metadata, layer_sizes)\r\n",
      "    _, examples = taxifare.read_examples(\r\n",
      "        taxifare.create_inputs(metadata, input_data=examples))\r\n",
      "    output = taxifare.inference(inputs, metadata, layer_sizes)\r\n",
      "    loss = taxifare.loss(output, targets)\r\n"
     ]
    }
   ],
   "source": [
    "!grep -E \"add_argument|taxifare\" taxifare/trainer/task.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third one is taxifare.py -- this is the real TensorFlow model and the only one for which you have work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Implementing TensorFlow model </h2>\n",
    "\n",
    "Here are the methods in taxifare.py that get called from task.py. It's your job to implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def read_examples(input_files, batch_size, shuffle, num_epochs=None):\r\n",
      "def create_inputs(metadata, input_data=None):\r\n",
      "def inference(inputs, metadata, hyperparams):\r\n",
      "def loss(output, targets):\r\n",
      "def training(loss, learning_rate):\r\n"
     ]
    }
   ],
   "source": [
    "!grep def taxifare/trainer/taxifare.py | grep -v \"def _\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the loss function for example.  This should feel familiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def loss(output, targets):\r\n",
      "  \"\"\"Calculates the loss from the output and the labels.\r\n",
      "  Args:\r\n",
      "    output: output layer tensor, float - [batch_size].\r\n",
      "    targets: Target value tensor, float - [batch_size].\r\n",
      "  Returns:\r\n",
      "    loss: Loss tensor of type float.\r\n",
      "  \"\"\"\r\n",
      "  loss = tf.sqrt(tf.reduce_mean(tf.square(output - targets)), name = 'loss') # RMSE\r\n",
      "  return loss\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep -A 10 \"def loss\" taxifare/trainer/taxifare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the cell above to look at the other functions.  Essentially, you'll take your TensorFlow model and separate out into modules and put the pieces in the right spots:\n",
    "<ol>\n",
    "<li> create_inputs will take the input data and do any input transformations that you want to do. </li>\n",
    "<li> inference will create the TensorFlow ML model i.e. the computational graph. </li>\n",
    "<li> loss will specify what you want to optimize </li>\n",
    "<li> training will implement the training loop. You typically don't have to change this </li>\n",
    "<li> read_examples as-is unless you want to change the way batching happens (you probably don't).\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running training locally </h2>\n",
    "\n",
    "Once you have a packaged TensorFlow model, you can run training by passing in the paths to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type %ml train into an empty cell, run it, fill in some params and execute it again. (create a new code cell and try it out!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>A training input template is created in next cell for you. See cell input instructions below.</p><table><tr><th>Parameters</th><th>Local Run Required</th><th>Cloud Run Required</th><th>Description</th></tr><tr><td>package_uris</td><td>True</td><td>True</td><td>A GCS or local (for local run only) path to your python training program package.</td></tr><tr><td>python_module</td><td>True</td><td>True</td><td>The module to run.</td></tr><tr><td>scale_tier</td><td>False</td><td>True</td><td>Type of resources requested for the job. On local run, BASIC means 1 master process only, and any other values mean 1 master 1 worker and 1 ps processes. But you can also override the values by setting worker_count and parameter_server_count. On cloud, see service definition for possible values.</td></tr><tr><td>region</td><td>False</td><td>True</td><td>Where the training job runs. For cloud run only.</td></tr><tr><td>args</td><td>False</td><td>False</td><td>Args that will be passed to your training program.</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ml train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%ml train [--cloud]\n",
    "package_uris: gs://your-bucket/my-training-package.tar.gz\n",
    "python_module: your_program.your_module\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  string_arg: value\n",
    "  int_arg: value\n",
    "  appendable_arg:\n",
    "    - value1\n",
    "    - value2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/\n",
      "taxifare/PKG-INFO\n",
      "taxifare/setup.cfg\n",
      "taxifare/setup.py\n",
      "taxifare/trainer/\n",
      "taxifare/trainer/.taxifare.py.swp\n",
      "taxifare/trainer/__init__.py\n",
      "taxifare/trainer/task.py\n",
      "taxifare/trainer/taxifare.py\n",
      "taxifare/trainer.egg-info/\n",
      "taxifare/trainer.egg-info/dependency_links.txt\n",
      "taxifare/trainer.egg-info/PKG-INFO\n",
      "taxifare/trainer.egg-info/SOURCES.txt\n",
      "taxifare/trainer.egg-info/top_level.txt\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master: Step 150: loss = 9.52 (0.044 sec)<br/>master: <br/>master: Step 200: loss = 7.96 (0.043 sec)<br/>master: <br/>master: Step 250: loss = 7.77 (0.040 sec)<br/>master: Step 300: loss = 8.59 (0.043 sec)<br/>master: <br/>master: inputs  =  [64, 5]<br/>master: Final error after 300 steps = 5.928<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:master/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:master/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: inputs  =  [None, 5]<br/>master: Done training.<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%ml train\n",
    "package_uris: /content/CPB102/lab3a/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - /content/CPB102/lab3a/taxi_preproc/features_train\n",
    "  eval_data_paths:\n",
    "    - /content/CPB102/lab3a/taxi_preproc/features_eval\n",
    "  metadata_path: /content/CPB102/lab3a/taxi_preproc/metadata.yaml\n",
    "  output_path: /content/CPB102/lab3a/taxi_trained\n",
    "  max_steps: 300\n",
    "  hidden1: 64\n",
    "  hidden2:  8\n",
    "  hidden3:  4\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval  logdir  model  summaries\r\n"
     ]
    }
   ],
   "source": [
    "!ls /content/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"62569b42-b9a8-4147-a1db-583f816db1cc\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"62569b42-b9a8-4147-a1db-583f816db1cc\", [{\"y\": [], \"x\": [], \"type\": \"scatter\", \"name\": \"loss-/content/CPB102/lab3a/taxi_trained/eval\"}, {\"y\": [5.909721851348877], \"x\": [300], \"type\": \"scatter\", \"name\": \"error-/content/CPB102/lab3a/taxi_trained/eval\"}, {\"y\": [10.311089515686035, 9.275343894958496, 7.5598320960998535, 9.407431602478027, 5.314665794372559, 9.781115531921387], \"x\": [50, 100, 150, 200, 250, 300], \"type\": \"scatter\", \"name\": \"loss-/content/CPB102/lab3a/taxi_trained/summaries\"}, {\"y\": [], \"x\": [], \"type\": \"scatter\", \"name\": \"error-/content/CPB102/lab3a/taxi_trained/summaries\"}], {\"title\": \"loss,error\", \"xaxis\": {\"title\": \"step\"}, \"yaxis\": {\"title\": \"loss,error\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%ml summary --dir /content/CPB102/lab3a/taxi_trained/summaries  /content/CPB102/lab3a/taxi_trained/eval  --name loss error --step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is the RMSE on the training dataset; the error is the RMSE on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 22285. Click <a href=\"/_proxy/34890/\" target=\"_blank\">here</a> to access it.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorboard start --logdir /content/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%tensorboard stop --pid 22285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training on cloud </h2>\n",
    "\n",
    "First of all, we need to set permissions on our bucket so that Cloud ML can read/write to it.  In CloudShell, go to CPB102/lab3 and run ./get_service_account.sh.  Use that account in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No changes to gs://cloud-training-demos/\n",
      "No changes to gs://cloud-training-demos/\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "# you can find the Cloud ML service account by running ./get_service_account.sh\n",
    "# change the service account and bucket as appropriate\n",
    "gsutil acl ch -u cloud-ml-service@cml-663413318684.iam.gserviceaccount.com:WRITE gs://cloud-training-demos/\n",
    "gsutil defacl ch -u cloud-ml-service@cml-663413318684.iam.gserviceaccount.com:O gs://cloud-training-demos/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to copy the model and data to Google Cloud Storage (GCS).  Change bucket name as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/\n",
      "taxifare/PKG-INFO\n",
      "taxifare/setup.cfg\n",
      "taxifare/setup.py\n",
      "taxifare/trainer/\n",
      "taxifare/trainer/.taxifare.py.swp\n",
      "taxifare/trainer/__init__.py\n",
      "taxifare/trainer/task.py\n",
      "taxifare/trainer/taxifare.py\n",
      "taxifare/trainer.egg-info/\n",
      "taxifare/trainer.egg-info/dependency_links.txt\n",
      "taxifare/trainer.egg-info/PKG-INFO\n",
      "taxifare/trainer.egg-info/SOURCES.txt\n",
      "taxifare/trainer.egg-info/top_level.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://taxifare.tar.gz [Content-Type=application/x-tar]...\n",
      "Uploading   ...d-training-demos/taxifare/source/taxifare.tar.gz: 0 B/7.73 KiB    \r",
      "Uploading   ...d-training-demos/taxifare/source/taxifare.tar.gz: 7.73 KiB/7.73 KiB    \r\n",
      "Copying file://../lab1a/taxi-test.csv [Content-Type=text/csv]...\n",
      "Uploading   ...loud-training-demos/taxifare/input/taxi-test.csv: 0 B/79.68 KiB    \r",
      "Uploading   ...loud-training-demos/taxifare/input/taxi-test.csv: 71.48 KiB/79.68 KiB    \r",
      "Uploading   ...loud-training-demos/taxifare/input/taxi-test.csv: 79.68 KiB/79.68 KiB    \r\n",
      "Copying file://../lab1a/taxi-train.csv [Content-Type=text/csv]...\n",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 0 B/370.48 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 71.48 KiB/370.48 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 143.48 KiB/370.48 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 215.48 KiB/370.48 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 287.48 KiB/370.48 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 359.48 KiB/370.48 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 370.48 KiB/370.48 KiB    \r\n",
      "Copying file://../lab1a/taxi-valid.csv [Content-Type=text/csv]...\n",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-valid.csv: 0 B/79.46 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-valid.csv: 71.48 KiB/79.46 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-valid.csv: 79.46 KiB/79.46 KiB    \r\n",
      "CommandException: 1 files/objects could not be removed.\n",
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare\n",
    "gsutil cp taxifare.tar.gz gs://cloud-training-demos/taxifare/source/taxifare.tar.gz\n",
    "gsutil cp ../lab1a/*.csv  gs://cloud-training-demos/taxifare/input/\n",
    "gsutil -m rm -r -f gs://cloud-training-demos/taxifare/taxi_preproc\n",
    "gsutil -m rm -r -f gs://cloud-training-demos/taxifare/taxi_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run your preprocessor, you have to change the input and output to be on GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.direct_runner.DirectPipelineResult at 0x7f94bb35f810>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.dataflow.io.tfrecordio as tfrecordio\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "\n",
    "# defines\n",
    "feature_set = TaxifareFeatures()\n",
    "OUTPUT_DIR = 'gs://cloud-training-demos/taxifare/taxi_preproc'\n",
    "pipeline = beam.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "training_data = beam.io.TextFileSource(\n",
    "    'gs://cloud-training-demos/taxifare/input/taxi-train.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "train = pipeline | beam.Read('ReadTrainingData', training_data)\n",
    "eval_data = beam.io.TextFileSource(\n",
    "    'gs://cloud-training-demos/taxifare/input/taxi-valid.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "eval = pipeline | beam.Read('ReadEvalData', eval_data)\n",
    "(metadata, train_features, eval_features) = ((train, eval) |\n",
    "    ml.Preprocess('Preprocess', feature_set))\n",
    "train_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix=os.path.join(OUTPUT_DIR, 'features_train'),\n",
    "    file_name_suffix='',\n",
    "    shard_file=False,\n",
    "    compress_file=True)\n",
    "eval_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix=os.path.join(OUTPUT_DIR, 'features_eval'),\n",
    "    file_name_suffix='',\n",
    "    shard_file=False,\n",
    "    compress_file=True)\n",
    "(metadata, train_features, eval_features) | (\n",
    "    io.SavePreprocessed('SavingData', OUTPUT_DIR,\n",
    "                        file_parameters_list=[\n",
    "                            os.path.join(OUTPUT_DIR, 'metadata.yaml'),\n",
    "                            train_parameters, eval_parameters]))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos/taxifare/taxi_preproc/features_eval\n",
      "gs://cloud-training-demos/taxifare/taxi_preproc/features_train\n",
      "gs://cloud-training-demos/taxifare/taxi_preproc/info\n",
      "gs://cloud-training-demos/taxifare/taxi_preproc/metadata.yaml\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls gs://cloud-training-demos/taxifare/taxi_preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, submit the training job ot the cloud.  Note that unlike Dataflow jobs (which usually take minutes), Cloud ML jobs usually take hours and are, therefore, queued. It may be a couple of minutes before your job starts being executed. This being a small job, though, the task should complete a few seconds later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job \"trainer_task_160914_154501\" was submitted successfully.<br/>Run \"%ml jobs --name trainer_task_160914_154501\" to view the status of the job.</p><p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=cloud-training-demos&resource=ml.googleapis.com%2Fjob_id%2Ftrainer_task_160914_154501\" target=\"_blank\">here</a> to view cloud log. <br/>Start TensorBoard by running \"%tensorboard start --logdir=&lt;YourLogDir&gt;\".</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ml train --cloud\n",
    "package_uris: gs://cloud-training-demos/taxifare/source/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - gs://cloud-training-demos/taxifare/taxi_preproc/features_train\n",
    "  eval_data_paths:\n",
    "    - gs://cloud-training-demos/taxifare/taxi_preproc/features_eval\n",
    "  metadata_path: gs://cloud-training-demos/taxifare/taxi_preproc/metadata.yaml\n",
    "  output_path: gs://cloud-training-demos/taxifare/taxi_trained\n",
    "  max_steps: 1000\n",
    "  hidden1: 64\n",
    "  hidden2:  8\n",
    "  hidden3:  4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prediction </h2>\n",
    "\n",
    "Make sure that the training job has completed before proceeding to this step (check the log above)\n",
    "\n",
    "To predict the taxifare for new inputs, you first have to deploy the trained model (deleting a previous one if necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ml delete --name taxifare.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%ml deploy --name taxifare.v1 --path gs://cloud-training-demos/taxifare/taxi_trained/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No JSON object could be decoded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-50db2b3f2058>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m      'passenger_count': 2}]}\n\u001b[1;32m     18\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'projects/%s/models/%s/versions/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'cloud-training-demos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'taxifare'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"response={0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/oauth2client/util.pyc\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/googleapiclient/http.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/googleapiclient/model.pyc\u001b[0m in \u001b[0;36mresponse\u001b[0;34m(self, resp, content)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# to all the other success states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_content_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Content from bad request was: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/googleapiclient/model.pyc\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_wrapper\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'data'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m       \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \"\"\"\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No JSON object could be decoded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No JSON object could be decoded"
     ]
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "import google.cloud.ml.features as features\n",
    "from google.cloud.ml import session_bundle\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1beta1', credentials=credentials,\n",
    "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1beta1_discovery.json')\n",
    "#request = {'instances': ['-73.885262,40.773008,-73.987232,40.732403,2']}\n",
    "request = {'instances': [\n",
    "    {'pickup_longitude': -73.885262,\n",
    "     'pickup_latitude': 40.773008,\n",
    "     'dropoff_longitude': -73.987232,\n",
    "     'dropoff_latitude': 40.732403,\n",
    "     'passenger_count': 2}]}\n",
    "parent = 'projects/%s/models/%s/versions/%s' % ('cloud-training-demos', 'taxifare', 'v1')\n",
    "response = api.projects().predict(body=request, name=parent).execute()\n",
    "print \"response={0}\".format(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
