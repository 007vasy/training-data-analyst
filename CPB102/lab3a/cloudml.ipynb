{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Scaling up ML using Cloud ML </h1>\n",
    "\n",
    "This notebook is Lab3a of CPB 102, Google's course on Machine Learning using Cloud ML.\n",
    "\n",
    "In this notebook, we take a previously developed TensorFlow model to predict taxifare rides and package it up so that it can be run in Cloud ML. For now, we'll run this on a small dataset. The model that was developed is rather simplistic, and therefore, the accuracy of the model is not great either.  However, this notebook illustrates *how* to package up a TensorFlow model to run it within Cloud ML. \n",
    "\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./cloudml-0.1.3.latest.tar.gz\n",
      "Requirement already up-to-date: oauth2client==2.2.0 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: google-cloud-dataflow>=0.4.0 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: bs4>=0.0.1 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: numpy>=1.10.4 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: pillow>=3.2.0 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: dpkt>=1.8.7 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: nltk>=3.2.1 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: httplib2>=0.9.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client==2.2.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from oauth2client==2.2.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: pyasn1>=0.1.7 in /usr/local/lib/python2.7/dist-packages (from oauth2client==2.2.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: pyasn1-modules>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from oauth2client==2.2.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: google-apitools>=0.5.2 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: dill>=0.2.5 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: protorpc>=0.9.1 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: mock>=1.0.1 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: avro>=1.7.7 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: python-gflags>=2.0 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Collecting pyyaml>=3.10 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "  Downloading PyYAML-3.12.tar.gz (253kB)\n",
      "Requirement already up-to-date: beautifulsoup4 in /usr/local/lib/python2.7/dist-packages (from bs4>=0.0.1->cloudml==0.1.2)\n",
      "Collecting setuptools>=18.5 (from google-apitools>=0.5.2->google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "  Downloading setuptools-26.1.1-py2.py3-none-any.whl (464kB)\n",
      "Requirement already up-to-date: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=1.0.1->google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=1.0.1->google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Building wheels for collected packages: cloudml, pyyaml\n",
      "  Running setup.py bdist_wheel for cloudml: started\n",
      "  Running setup.py bdist_wheel for cloudml: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/e9/0d/f5/9930c2a12ec583cd5e771f885690a0da20c5d9a6df0d7660a6\n",
      "  Running setup.py bdist_wheel for pyyaml: started\n",
      "  Running setup.py bdist_wheel for pyyaml: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/f7/79/13f3a12cd723892437c0cfbde1230ab4d82947ff7b3839a4fc\n",
      "Successfully built cloudml pyyaml\n",
      "Installing collected packages: cloudml, pyyaml, setuptools\n",
      "  Found existing installation: cloudml 0.1.2\n",
      "    Uninstalling cloudml-0.1.2:\n",
      "      Successfully uninstalled cloudml-0.1.2\n",
      "  Found existing installation: PyYAML 3.11\n",
      "    Uninstalling PyYAML-3.11:\n",
      "      Successfully uninstalled PyYAML-3.11\n",
      "  Found existing installation: setuptools 25.2.0\n",
      "    Uninstalling setuptools-25.2.0:\n",
      "      Successfully uninstalled setuptools-25.2.0\n",
      "Successfully installed cloudml-0.1.2 pyyaml-3.12 setuptools-26.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-ml/sdk/cloudml-0.1.3.latest.tar.gz...\n",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                0 B/545.82 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                72 KiB/545.82 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                144 KiB/545.82 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                216 KiB/545.82 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                288 KiB/545.82 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                360 KiB/545.82 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                432 KiB/545.82 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                504 KiB/545.82 KiB    \r",
      "Downloading file://./cloudml-0.1.3.latest.tar.gz:                545.82 KiB/545.82 KiB    \r\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil cp gs://cloud-ml/sdk/cloudml-0.1.3.latest.tar.gz .\n",
    "pip install --upgrade cloudml-0.1.3.latest.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Write code for preprocessing and feature engineering </h2>\n",
    "\n",
    "Realistic ML models involve a fair bit of preprocessing and feature engineering. The standard Cloud ML pipeline expects this. We haven't covered this in class yet, so we'll just pull out the input variables and pass them through untransformed.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Datalab can generate the following template code for you.  Just type <b>%ml features</b> into an empty cell, and then fill out the path, headers, target, id.  Running that cell in turn will create Python code to define features. You can then edit it. (try it out by creating a new code block, and starting with %ml features in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import google.cloud.ml.features as features\n",
    "\n",
    "class TaxifareFeatures(object):\n",
    "  \"\"\"This class is generated from command line:\n",
    "        %ml features\n",
    "        path: ../lab1a/taxi-train.csv\n",
    "        headers: pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,fare_amount\n",
    "        target: fare_amount\n",
    "        Please modify it as appropriate!!!\n",
    "  \"\"\"\n",
    "  csv_columns = ('pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','fare_amount')\n",
    "  fare_amount = features.target('fare_amount').regression()\n",
    "  attrs = [\n",
    "      features.numeric('pickup_longitude').identity(),\n",
    "      features.numeric('dropoff_longitude').identity(),\n",
    "      features.numeric('passenger_count').identity(),\n",
    "      features.numeric('pickup_latitude').identity(),\n",
    "      features.numeric('dropoff_latitude').identity(),\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Dataflow pipeline for preprocessing </h2>\n",
    "\n",
    "Dataflow pipeline code can also be created using code generation in Datalab.  Type <b>%ml preprocess</b> into an empty cell, run it, fill in some params and execute it again. (create a new code cell and try it out!)\n",
    "\n",
    "Note that this code references the features class above (TaxifareFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.direct_runner.DirectPipelineResult at 0x7f7540df1050>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# header\n",
    "\"\"\"\n",
    "Following code is generated from command line:\n",
    "%%ml preprocess\n",
    "train_data_path: ../lab1a/taxi-train.csv\n",
    "eval_data_path: ../lab1a/taxi-valid.csv\n",
    "data_format: CSV\n",
    "output_dir: ./taxi_preproc\n",
    "feature_set_class_name: TaxifareFeatures\n",
    "\n",
    "Please modify as appropriate!!!\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.dataflow.io.tfrecordio as tfrecordio\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "\n",
    "# defines\n",
    "feature_set = TaxifareFeatures()\n",
    "OUTPUT_DIR = './taxi_preproc'\n",
    "pipeline = beam.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "training_data = beam.io.TextFileSource(\n",
    "    '../lab1a/taxi-train.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "train = pipeline | beam.Read('ReadTrainingData', training_data)\n",
    "eval_data = beam.io.TextFileSource(\n",
    "    '../lab1a/taxi-valid.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "eval = pipeline | beam.Read('ReadEvalData', eval_data)\n",
    "(metadata, train_features, eval_features) = ((train, eval) |\n",
    "    ml.Preprocess('Preprocess', feature_set))\n",
    "train_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix=os.path.join(OUTPUT_DIR, 'features_train'),\n",
    "    file_name_suffix='',\n",
    "    shard_file=False,\n",
    "    compress_file=True)\n",
    "eval_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix=os.path.join(OUTPUT_DIR, 'features_eval'),\n",
    "    file_name_suffix='',\n",
    "    shard_file=False,\n",
    "    compress_file=True)\n",
    "(metadata, train_features, eval_features) | (\n",
    "    io.SavePreprocessed('SavingData', OUTPUT_DIR,\n",
    "                        file_parameters_list=[\n",
    "                            os.path.join(OUTPUT_DIR, 'metadata.yaml'),\n",
    "                            train_parameters, eval_parameters]))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above preprocessing code creates TFRecords, an efficient compressed format that is suitable for repeated training and hyperparameter tuning. This is what our TensorFlow model receives. In addition, the preprocessing pipeline creates metadata.yaml, a set of statistics computed from the input data that is necessary for many of the input transformations covered in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_eval  features_train  info  metadata.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!ls taxi_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns:\r\n",
      "  dropoff_latitude:\r\n",
      "    max: 41.366138\r\n",
      "    mean: 40.751464661690754\r\n",
      "    min: 40.514429\r\n",
      "    name: dropoff_latitude\r\n",
      "    type: numeric\r\n",
      "  dropoff_longitude:\r\n",
      "    max: -73.137393\r\n",
      "    mean: -73.97474299191431\r\n",
      "    min: -74.417107\r\n",
      "    name: dropoff_longitude\r\n",
      "    type: numeric\r\n",
      "  fare_amount:\r\n",
      "    max: 88.0\r\n",
      "    mean: 11.195111969111972\r\n",
      "    min: 2.5\r\n",
      "    name: fare_amount\r\n",
      "    scenario: continuous\r\n",
      "    type: target\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 taxi_preproc/metadata.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Package up TensorFlow model </h2>\n",
    "\n",
    "The TensorFlow model needs to be packaged up into a Python module.  This has a very specific folder structure (you'd typically do this using Linux commands) or by dragging-and-dropping files into the appropriate folder structure. Then, you create an archive of it using the 'tar' command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/\n",
      "taxifare/PKG-INFO\n",
      "taxifare/setup.cfg\n",
      "taxifare/setup.py\n",
      "taxifare/trainer/\n",
      "taxifare/trainer/.taxifare.py.swp\n",
      "taxifare/trainer/__init__.py\n",
      "taxifare/trainer/task.py\n",
      "taxifare/trainer/taxifare.py\n",
      "taxifare/trainer.egg-info/\n",
      "taxifare/trainer.egg-info/dependency_links.txt\n",
      "taxifare/trainer.egg-info/PKG-INFO\n",
      "taxifare/trainer.egg-info/SOURCES.txt\n",
      "taxifare/trainer.egg-info/top_level.txt\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only three of those files ones that you would actually edit.\n",
    "\n",
    "The first is setup.py.  You would change it to reflect your module name, author, author_email and description. You might add Python packages that you depend upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "from setuptools import find_packages\r\n",
      "from setuptools import setup\r\n",
      "\r\n",
      "REQUIRED_PACKAGES = [\r\n",
      "]\r\n",
      "\r\n",
      "setup(\r\n",
      "    name='taxifare',\r\n",
      "    version='0.1',\r\n",
      "    author = 'Google',\r\n",
      "    author_email = 'training-feedback@cloud.google.com',\r\n",
      "    install_requires=REQUIRED_PACKAGES,\r\n",
      "    packages=find_packages(),\r\n",
      "    include_package_data=True,\r\n",
      "    description='CPB102 taxifare in Cloud ML',\r\n",
      "    requires=[]\r\n",
      ")\r\n"
     ]
    }
   ],
   "source": [
    "!grep -v \"^#\" taxifare/setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, the second is task.py. This file is canonical code to run your TensorFlow model by reading data in batches, setting up summary statistics, etc.  Most of this code will, in the future, move away from your Python module. For now, simply do a string-replace of 'taxifare' with the name of your Python module and add any hyperparameters you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import taxifare\r\n",
      "  parser.add_argument(\"--train_data_paths\", type=str, action='append')\r\n",
      "  parser.add_argument(\"--eval_data_paths\", type=str, action='append')\r\n",
      "  parser.add_argument(\"--metadata_path\", type=str)\r\n",
      "  parser.add_argument(\"--output_path\", type=str)\r\n",
      "  parser.add_argument(\"--max_steps\", type=int, default=2000)\r\n",
      "  parser.add_argument(\"--hidden1\", type=int, default=300)\r\n",
      "  parser.add_argument(\"--hidden2\", type=int, default=200)\r\n",
      "  parser.add_argument(\"--hidden3\", type=int, default=100)\r\n",
      "  \"\"\"Train taxifare for a number of steps.\"\"\"\r\n",
      "  # test on taxifare.\r\n",
      "      _, train_examples = taxifare.read_examples(\r\n",
      "          taxifare.create_inputs(metadata, input_data=train_examples))\r\n",
      "      output = taxifare.inference(inputs, metadata, layer_sizes)\r\n",
      "      loss = taxifare.loss(output, targets)\r\n",
      "      train_op, global_step = taxifare.training(loss,\r\n",
      "    placeholder, inputs, _, keys = taxifare.create_inputs(metadata)\r\n",
      "    output = taxifare.inference(inputs, metadata, layer_sizes)\r\n",
      "    _, examples = taxifare.read_examples(\r\n",
      "        taxifare.create_inputs(metadata, input_data=examples))\r\n",
      "    output = taxifare.inference(inputs, metadata, layer_sizes)\r\n",
      "    loss = taxifare.loss(output, targets)\r\n"
     ]
    }
   ],
   "source": [
    "!grep -E \"add_argument|taxifare\" taxifare/trainer/task.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains taxifare.py -- this is the real TensorFlow model and the only one for which you have work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Implementing TensorFlow model </h2>\n",
    "\n",
    "Here are the methods in taxifare.py that get called from task.py. It's your job to implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def read_examples(input_files, batch_size, shuffle, num_epochs=None):\r\n",
      "def create_inputs(metadata, input_data=None):\r\n",
      "def inference(inputs, metadata, hyperparams):\r\n",
      "def loss(output, targets):\r\n",
      "def training(loss, learning_rate):\r\n"
     ]
    }
   ],
   "source": [
    "!grep def taxifare/trainer/taxifare.py | grep -v \"def _\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the loss function for example.  This should feel familiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def loss(output, targets):\r\n",
      "  \"\"\"Calculates the loss from the output and the labels.\r\n",
      "  Args:\r\n",
      "    output: output layer tensor, float - [batch_size].\r\n",
      "    targets: Target value tensor, float - [batch_size].\r\n",
      "  Returns:\r\n",
      "    loss: Loss tensor of type float.\r\n",
      "  \"\"\"\r\n",
      "  loss = tf.reduce_mean(tf.abs(output - targets), name = 'loss') # Mean Absolute Error, not differentiable ...!\r\n",
      "  return loss\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep -A 10 \"def loss\" taxifare/trainer/taxifare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the cell above to look at the other functions.  Essentially, you'll take your TensorFlow model and separate out into modules and put the pieces in the right spots:\n",
    "<ol>\n",
    "<li> create_inputs will take the input data and do any input transformations that you want to do. </li>\n",
    "<li> inference will create the TensorFlow ML model i.e. the computational graph. </li>\n",
    "<li> loss will specify what you want to optimize </li>\n",
    "<li> training will implement the training loop. You typically don't have to change this </li>\n",
    "<li> read_examples as-is unless you want to change the way batching happens (you probably don't).\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running training locally </h2>\n",
    "\n",
    "Once you have a packaged TensorFlow model, you can run training by passing in the paths to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type %ml train into an empty cell, run it, fill in some params and execute it again. (create a new code cell and try it out!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>A training input template is created in next cell for you. See cell input instructions below.</p><table><tr><th>Parameters</th><th>Local Run Required</th><th>Cloud Run Required</th><th>Description</th></tr><tr><td>package_uris</td><td>True</td><td>True</td><td>A GCS or local (for local run only) path to your python training program package.</td></tr><tr><td>python_module</td><td>True</td><td>True</td><td>The module to run.</td></tr><tr><td>scale_tier</td><td>False</td><td>True</td><td>Type of resources requested for the job. On local run, BASIC means 1 master process only, and any other values mean 1 master 1 worker and 1 ps processes. But you can also override the values by setting worker_count and parameter_server_count. On cloud, see service definition for possible values.</td></tr><tr><td>region</td><td>False</td><td>True</td><td>Where the training job runs. For cloud run only.</td></tr><tr><td>args</td><td>False</td><td>False</td><td>Args that will be passed to your training program.</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ml train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%ml train [--cloud]\n",
    "package_uris: gs://your-bucket/my-training-package.tar.gz\n",
    "python_module: your_program.your_module\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  string_arg: value\n",
    "  int_arg: value\n",
    "  appendable_arg:\n",
    "    - value1\n",
    "    - value2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/\n",
      "taxifare/PKG-INFO\n",
      "taxifare/setup.cfg\n",
      "taxifare/setup.py\n",
      "taxifare/trainer/\n",
      "taxifare/trainer/.taxifare.py.swp\n",
      "taxifare/trainer/__init__.py\n",
      "taxifare/trainer/task.py\n",
      "taxifare/trainer/taxifare.py\n",
      "taxifare/trainer.egg-info/\n",
      "taxifare/trainer.egg-info/dependency_links.txt\n",
      "taxifare/trainer.egg-info/PKG-INFO\n",
      "taxifare/trainer.egg-info/SOURCES.txt\n",
      "taxifare/trainer.egg-info/top_level.txt\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master: Eval, step 223: error = 5.848<br/>master: Step 250: loss = 7.79 (0.043 sec)<br/>master: <br/>master: Step 300: loss = 8.63 (0.040 sec)<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: Final error after 300 steps = 5.534<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:master/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:master/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: inputs  =  [None, 5]<br/>master: Done training.<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%ml train\n",
    "package_uris: /content/CPB102/lab3a/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - /content/CPB102/lab3a/taxi_preproc/features_train\n",
    "  eval_data_paths:\n",
    "    - /content/CPB102/lab3a/taxi_preproc/features_eval\n",
    "  metadata_path: /content/CPB102/lab3a/taxi_preproc/metadata.yaml\n",
    "  output_path: /content/CPB102/lab3a/taxi_trained\n",
    "  max_steps: 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval  logdir  model  summaries\r\n"
     ]
    }
   ],
   "source": [
    "!ls /content/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"da8f7567-7a5c-4c37-9e09-838076604c9e\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"da8f7567-7a5c-4c37-9e09-838076604c9e\", [{\"y\": [], \"x\": [], \"type\": \"scatter\", \"name\": \"loss-/content/CPB102/lab3a/taxi_trained/eval\"}, {\"y\": [10.289995193481445, 9.290752410888672, 7.550495147705078, 9.472429275512695, 5.307887077331543, 9.933592796325684], \"x\": [50, 100, 150, 200, 250, 300], \"type\": \"scatter\", \"name\": \"loss-/content/CPB102/lab3a/taxi_trained/summaries\"}], {\"title\": \"loss\", \"xaxis\": {\"title\": \"step\"}, \"yaxis\": {\"title\": \"loss\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%ml summary --dir /content/CPB102/lab3a/taxi_trained/summaries  /content/CPB102/lab3a/taxi_trained/eval  --name loss --step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 22285. Click <a href=\"/_proxy/34890/\" target=\"_blank\">here</a> to access it.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorboard start --logdir /content/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%tensorboard stop --pid 22285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
