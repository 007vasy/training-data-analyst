{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Scaling up ML using Cloud ML </h1>\n",
    "\n",
    "This notebook is Lab3a of CPB 102, Google's course on Machine Learning using Cloud ML.\n",
    "\n",
    "In this notebook, we take a previously developed TensorFlow model to predict taxifare rides and package it up so that it can be run in Cloud ML. For now, we'll run this on a small dataset. The model that was developed is rather simplistic, and therefore, the accuracy of the model is not great either.  However, this notebook illustrates *how* to package up a TensorFlow model to run it within Cloud ML. \n",
    "\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "Later in the course, we will look at ways to make a more effective machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./cloudml-0.1.4.tar.gz\n",
      "Collecting oauth2client==2.2.0 (from cloudml==0.1.4)\n",
      "Collecting six>=1.10.0 (from cloudml==0.1.4)\n",
      "  Using cached six-1.10.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-dataflow>=0.4.0 (from cloudml==0.1.4)\n",
      "Collecting bs4>=0.0.1 (from cloudml==0.1.4)\n",
      "  Using cached bs4-0.0.1.tar.gz\n",
      "Collecting numpy>=1.10.4 (from cloudml==0.1.4)\n",
      "  Using cached numpy-1.11.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting pillow>=3.2.0 (from cloudml==0.1.4)\n",
      "  Using cached Pillow-3.3.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting dpkt>=1.8.7 (from cloudml==0.1.4)\n",
      "  Using cached dpkt-1.8.8-py2-none-any.whl\n",
      "Collecting nltk>=3.2.1 (from cloudml==0.1.4)\n",
      "  Using cached nltk-3.2.1.tar.gz\n",
      "Collecting httplib2>=0.9.1 (from oauth2client==2.2.0->cloudml==0.1.4)\n",
      "  Using cached httplib2-0.9.2.zip\n",
      "Collecting rsa>=3.1.4 (from oauth2client==2.2.0->cloudml==0.1.4)\n",
      "  Using cached rsa-3.4.2-py2.py3-none-any.whl\n",
      "Collecting pyasn1>=0.1.7 (from oauth2client==2.2.0->cloudml==0.1.4)\n",
      "  Using cached pyasn1-0.1.9-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.0.5 (from oauth2client==2.2.0->cloudml==0.1.4)\n",
      "  Using cached pyasn1_modules-0.0.8-py2.py3-none-any.whl\n",
      "Collecting google-apitools>=0.5.2 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Using cached google_apitools-0.5.4-py2-none-any.whl\n",
      "Collecting dill>=0.2.5 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "Collecting protorpc>=0.9.1 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Using cached protorpc-0.11.1-py2-none-any.whl\n",
      "Collecting mock>=1.0.1 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Using cached mock-2.0.0-py2.py3-none-any.whl\n",
      "Collecting avro>=1.7.7 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "Collecting python-gflags>=2.0 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Downloading python-gflags-3.0.7.tar.gz (50kB)\n",
      "Collecting pyyaml>=3.10 (from google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Downloading PyYAML-3.12.tar.gz (253kB)\n",
      "Collecting beautifulsoup4 (from bs4>=0.0.1->cloudml==0.1.4)\n",
      "  Using cached beautifulsoup4-4.5.1-py2-none-any.whl\n",
      "Collecting setuptools>=18.5 (from google-apitools>=0.5.2->google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Downloading setuptools-27.2.0-py2.py3-none-any.whl (465kB)\n",
      "Collecting funcsigs>=1; python_version < \"3.3\" (from mock>=1.0.1->google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Using cached funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting pbr>=0.11 (from mock>=1.0.1->google-cloud-dataflow>=0.4.0->cloudml==0.1.4)\n",
      "  Using cached pbr-1.10.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: cloudml, bs4, nltk, httplib2, python-gflags, pyyaml\n",
      "  Running setup.py bdist_wheel for cloudml: started\n",
      "  Running setup.py bdist_wheel for cloudml: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/db/25/59/9be335261e6e04d841c1e5958b404eb5de670de679fcfb15ba\n",
      "  Running setup.py bdist_wheel for bs4: started\n",
      "  Running setup.py bdist_wheel for bs4: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/84/67/d4/9e09d9d5adede2ee1c7b7e8775ba3fbb04d07c4f946f0e4f11\n",
      "  Running setup.py bdist_wheel for nltk: started\n",
      "  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/55/0b/ce/960dcdaec7c9af5b1f81d471a90c8dae88374386efe6e54a50\n",
      "  Running setup.py bdist_wheel for httplib2: started\n",
      "  Running setup.py bdist_wheel for httplib2: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/c7/67/60/e0be8ccfc1e08f8ff1f50d99ea5378e204580ea77b0169fb55\n",
      "  Running setup.py bdist_wheel for python-gflags: started\n",
      "  Running setup.py bdist_wheel for python-gflags: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/17/e1/820c9c1d289adbb466fc23ae3810045a31a566036db975b358\n",
      "  Running setup.py bdist_wheel for pyyaml: started\n",
      "  Running setup.py bdist_wheel for pyyaml: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/f7/79/13f3a12cd723892437c0cfbde1230ab4d82947ff7b3839a4fc\n",
      "Successfully built cloudml bs4 nltk httplib2 python-gflags pyyaml\n",
      "Installing collected packages: httplib2, pyasn1, rsa, pyasn1-modules, six, oauth2client, setuptools, google-apitools, dill, protorpc, funcsigs, pbr, mock, avro, python-gflags, pyyaml, google-cloud-dataflow, beautifulsoup4, bs4, numpy, pillow, dpkt, nltk, cloudml\n",
      "  Found existing installation: httplib2 0.9.2\n",
      "    Uninstalling httplib2-0.9.2:\n",
      "      Successfully uninstalled httplib2-0.9.2\n",
      "  Found existing installation: pyasn1 0.1.9\n",
      "    Uninstalling pyasn1-0.1.9:\n",
      "      Successfully uninstalled pyasn1-0.1.9\n",
      "  Found existing installation: rsa 3.4.2\n",
      "    Uninstalling rsa-3.4.2:\n",
      "      Successfully uninstalled rsa-3.4.2\n",
      "  Found existing installation: pyasn1-modules 0.0.8\n",
      "    Uninstalling pyasn1-modules-0.0.8:\n",
      "      Successfully uninstalled pyasn1-modules-0.0.8\n",
      "  Found existing installation: six 1.10.0\n",
      "    Uninstalling six-1.10.0:\n",
      "      Successfully uninstalled six-1.10.0\n",
      "  Found existing installation: oauth2client 2.0.2\n",
      "    Uninstalling oauth2client-2.0.2:\n",
      "      Successfully uninstalled oauth2client-2.0.2\n",
      "  Found existing installation: setuptools 25.2.0\n",
      "    Uninstalling setuptools-25.2.0:\n",
      "      Successfully uninstalled setuptools-25.2.0\n",
      "  Found existing installation: google-apitools 0.5.4\n",
      "    Uninstalling google-apitools-0.5.4:\n",
      "      Successfully uninstalled google-apitools-0.5.4\n",
      "  Found existing installation: dill 0.2.5\n",
      "    Uninstalling dill-0.2.5:\n",
      "      Successfully uninstalled dill-0.2.5\n",
      "  Found existing installation: protorpc 0.11.1\n",
      "    Uninstalling protorpc-0.11.1:\n",
      "      Successfully uninstalled protorpc-0.11.1\n",
      "  Found existing installation: funcsigs 1.0.2\n",
      "    Uninstalling funcsigs-1.0.2:\n",
      "      Successfully uninstalled funcsigs-1.0.2\n",
      "  Found existing installation: pbr 1.10.0\n",
      "    Uninstalling pbr-1.10.0:\n",
      "      Successfully uninstalled pbr-1.10.0\n",
      "  Found existing installation: mock 2.0.0\n",
      "    Uninstalling mock-2.0.0:\n",
      "      Successfully uninstalled mock-2.0.0\n",
      "  Found existing installation: avro 1.8.1\n",
      "    Uninstalling avro-1.8.1:\n",
      "      Successfully uninstalled avro-1.8.1\n",
      "  Found existing installation: python-gflags 3.0.6\n",
      "    Uninstalling python-gflags-3.0.6:\n",
      "      Successfully uninstalled python-gflags-3.0.6\n",
      "  Found existing installation: PyYAML 3.11\n",
      "    Uninstalling PyYAML-3.11:\n",
      "      Successfully uninstalled PyYAML-3.11\n",
      "  Found existing installation: google-cloud-dataflow 0.4.1\n",
      "    Uninstalling google-cloud-dataflow-0.4.1:\n",
      "      Successfully uninstalled google-cloud-dataflow-0.4.1\n",
      "  Found existing installation: beautifulsoup4 4.5.1\n",
      "    Uninstalling beautifulsoup4-4.5.1:\n",
      "      Successfully uninstalled beautifulsoup4-4.5.1\n",
      "  Found existing installation: bs4 0.0.1\n",
      "    Uninstalling bs4-0.0.1:\n",
      "      Successfully uninstalled bs4-0.0.1\n",
      "  Found existing installation: numpy 1.11.1\n",
      "    Uninstalling numpy-1.11.1:\n",
      "      Successfully uninstalled numpy-1.11.1\n",
      "  Found existing installation: Pillow 3.3.1\n",
      "    Uninstalling Pillow-3.3.1:\n",
      "      Successfully uninstalled Pillow-3.3.1\n",
      "  Found existing installation: dpkt 1.8.8\n",
      "    Uninstalling dpkt-1.8.8:\n",
      "      Successfully uninstalled dpkt-1.8.8\n",
      "  Found existing installation: nltk 3.2.1\n",
      "    Uninstalling nltk-3.2.1:\n",
      "      Successfully uninstalled nltk-3.2.1\n",
      "  Found existing installation: cloudml 0.1.2\n",
      "    Uninstalling cloudml-0.1.2:\n",
      "      Successfully uninstalled cloudml-0.1.2\n",
      "Successfully installed avro-1.8.1 beautifulsoup4-4.5.1 bs4-0.0.1 cloudml-0.1.4 dill-0.2.5 dpkt-1.8.8 funcsigs-1.0.2 google-apitools-0.5.4 google-cloud-dataflow-0.4.1 httplib2-0.9.2 mock-2.0.0 nltk-3.2.1 numpy-1.11.1 oauth2client-2.2.0 pbr-1.10.0 pillow-3.3.1 protorpc-0.11.1 pyasn1-0.1.9 pyasn1-modules-0.0.8 python-gflags-3.0.7 pyyaml-3.12 rsa-3.4.2 setuptools-27.2.0 six-1.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-ml/sdk/cloudml-0.1.4.tar.gz...\n",
      "Downloading file://./cloudml-0.1.4.tar.gz:                       0 B/545.79 KiB    \r",
      "Downloading file://./cloudml-0.1.4.tar.gz:                       72 KiB/545.79 KiB    \r",
      "Downloading file://./cloudml-0.1.4.tar.gz:                       144 KiB/545.79 KiB    \r",
      "Downloading file://./cloudml-0.1.4.tar.gz:                       216 KiB/545.79 KiB    \r",
      "Downloading file://./cloudml-0.1.4.tar.gz:                       288 KiB/545.79 KiB    \r",
      "Downloading file://./cloudml-0.1.4.tar.gz:                       360 KiB/545.79 KiB    \r",
      "Downloading file://./cloudml-0.1.4.tar.gz:                       432 KiB/545.79 KiB    \r",
      "Downloading file://./cloudml-0.1.4.tar.gz:                       504 KiB/545.79 KiB    \r",
      "Downloading file://./cloudml-0.1.4.tar.gz:                       545.79 KiB/545.79 KiB    \r\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "# remember to \"Reset Session\" if you execute this cell -- this is needed to restart the Python kernel with updated package\n",
    "gsutil cp gs://cloud-ml/sdk/cloudml-0.1.4.tar.gz .\n",
    "pip install --force-reinstall --upgrade cloudml-0.1.4.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Write code for preprocessing and feature engineering </h2>\n",
    "\n",
    "Realistic ML models involve a fair bit of preprocessing and feature engineering. The standard Cloud ML pipeline expects this. We haven't covered this in class yet, so we'll just pull out the input variables and pass them through untransformed (i.e. we will simply do identity() on the columns).\n",
    "\n",
    "<br/>\n",
    "\n",
    "Datalab can generate the following template code for you.  Just type <b>%ml features</b> into an empty cell, and then fill out the path, headers, target, id.  Running that cell in turn will create Python code to define features. You can then edit it. (try it out by creating a new code block, and starting with %ml features in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml/sdk/cloudml-0.1.4.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.ml.features as features\n",
    "\n",
    "#import google.cloud.ml as ml\n",
    "#print ml.sdk_location\n",
    "\n",
    "class TaxifareFeatures(object):\n",
    "  \"\"\"This class is generated from command line:\n",
    "        %ml features\n",
    "        path: ../lab1a/taxi-train.csv\n",
    "        headers: pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,fare_amount\n",
    "        target: fare_amount\n",
    "        Please modify it as appropriate!!!\n",
    "  \"\"\"\n",
    "  csv_columns = ('pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','fare_amount')\n",
    "  fare_amount = features.target('fare_amount').regression()\n",
    "  attrs = [\n",
    "      features.numeric('pickup_longitude').identity(),\n",
    "      features.numeric('dropoff_longitude').identity(),\n",
    "      features.numeric('passenger_count').identity(),\n",
    "      features.numeric('pickup_latitude').identity(),\n",
    "      features.numeric('dropoff_latitude').identity(),\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Dataflow pipeline for preprocessing </h2>\n",
    "\n",
    "Dataflow pipeline code can also be created using code generation in Datalab.  Type <b>%ml preprocess</b> (or <b>%ml preprocess --cloud</b> to create a template with a DataflowPipelineRunner and gs:// paths) into an empty cell, run it, fill in some params and execute it again. (create a new code cell and try it out!)\n",
    "\n",
    "Note that this code references the features class above (TaxifareFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.direct_runner.DirectPipelineResult at 0x7f969b23da50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# header\n",
    "\"\"\"\n",
    "Following code is generated from command line:\n",
    "%%ml preprocess\n",
    "train_data_path: ../lab1a/taxi-train.csv\n",
    "eval_data_path: ../lab1a/taxi-valid.csv\n",
    "data_format: CSV\n",
    "output_dir: ./taxi_preproc\n",
    "feature_set_class_name: TaxifareFeatures\n",
    "\n",
    "Please modify as appropriate!!!\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.dataflow.io.tfrecordio as tfrecordio\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "\n",
    "# defines\n",
    "feature_set = TaxifareFeatures()\n",
    "OUTPUT_DIR = './taxi_preproc'\n",
    "pipeline = beam.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "training_data = beam.io.TextFileSource(\n",
    "    '../lab1a/taxi-train.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "train = pipeline | beam.Read('ReadTrainingData', training_data)\n",
    "eval_data = beam.io.TextFileSource(\n",
    "    '../lab1a/taxi-valid.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "eval = pipeline | beam.Read('ReadEvalData', eval_data)\n",
    "(metadata, train_features, eval_features) = ((train, eval) |\n",
    "    ml.Preprocess('Preprocess', feature_set))\n",
    "train_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix=os.path.join(OUTPUT_DIR, 'features_train'),\n",
    "    file_name_suffix='',\n",
    "    shard_file=False,\n",
    "    compress_file=True)\n",
    "eval_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix=os.path.join(OUTPUT_DIR, 'features_eval'),\n",
    "    file_name_suffix='',\n",
    "    shard_file=False,\n",
    "    compress_file=True)\n",
    "(metadata, train_features, eval_features) | (\n",
    "    io.SavePreprocessed('SavingData', OUTPUT_DIR,\n",
    "                        file_parameters_list=[\n",
    "                            os.path.join(OUTPUT_DIR, 'metadata.yaml'),\n",
    "                            train_parameters, eval_parameters]))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above preprocessing code creates TFRecords, an efficient compressed format that is suitable for repeated training, distribution, and hyperparameter tuning. This is what our TensorFlow model receives. In addition, the preprocessing pipeline creates metadata.yaml, a set of statistics computed from the input data that is necessary for many of the input transformations covered in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_eval  features_train  info  metadata.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!ls taxi_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns:\r\n",
      "  dropoff_latitude:\r\n",
      "    max: 41.366138\r\n",
      "    mean: 40.751464661690754\r\n",
      "    min: 40.514429\r\n",
      "    name: dropoff_latitude\r\n",
      "    type: numeric\r\n",
      "  dropoff_longitude:\r\n",
      "    max: -73.137393\r\n",
      "    mean: -73.97474299191431\r\n",
      "    min: -74.417107\r\n",
      "    name: dropoff_longitude\r\n",
      "    type: numeric\r\n",
      "  fare_amount:\r\n",
      "    max: 88.0\r\n",
      "    mean: 11.195111969111972\r\n",
      "    min: 2.5\r\n",
      "    name: fare_amount\r\n",
      "    scenario: continuous\r\n",
      "    type: target\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 taxi_preproc/metadata.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Package up TensorFlow model </h2>\n",
    "\n",
    "The TensorFlow model needs to be packaged up into a Python module.  This has a very specific folder structure (you'd typically maintain this exact structure in your source repository). Then, you create an archive of it using the 'tar' command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/\n",
      "taxifare/PKG-INFO\n",
      "taxifare/setup.cfg\n",
      "taxifare/setup.py\n",
      "taxifare/trainer/\n",
      "taxifare/trainer/__init__.py\n",
      "taxifare/trainer/task.py\n",
      "taxifare/trainer/taxifare.py\n",
      "taxifare/trainer.egg-info/\n",
      "taxifare/trainer.egg-info/dependency_links.txt\n",
      "taxifare/trainer.egg-info/PKG-INFO\n",
      "taxifare/trainer.egg-info/SOURCES.txt\n",
      "taxifare/trainer.egg-info/top_level.txt\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only three of those files are ones that you would actually edit, and one of them (taxifare.py) has meaningful code associated with it.\n",
    "\n",
    "The first is setup.py.  You would change it to reflect your module name, author, author_email and description. You might also add Python packages that you depend upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "from setuptools import find_packages\r\n",
      "from setuptools import setup\r\n",
      "\r\n",
      "REQUIRED_PACKAGES = [\r\n",
      "]\r\n",
      "\r\n",
      "setup(\r\n",
      "    name='taxifare',\r\n",
      "    version='0.1',\r\n",
      "    author = 'Google',\r\n",
      "    author_email = 'training-feedback@cloud.google.com',\r\n",
      "    install_requires=REQUIRED_PACKAGES,\r\n",
      "    packages=find_packages(),\r\n",
      "    include_package_data=True,\r\n",
      "    description='CPB102 taxifare in Cloud ML',\r\n",
      "    requires=[]\r\n",
      ")\r\n"
     ]
    }
   ],
   "source": [
    "!grep -v \"^#\" taxifare/setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, the second is task.py. This file is canonical code to run your TensorFlow model by reading data in batches, setting up summary statistics, etc.  Most of this code will, in the future, move away from your Python module. For now, simply do a string-replace of 'taxifare' with the name of your Python module and add any hyperparameters you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import taxifare\r\n",
      "  parser.add_argument(\"--train_data_paths\", type=str, action='append')\r\n",
      "  parser.add_argument(\"--eval_data_paths\", type=str, action='append')\r\n",
      "  parser.add_argument(\"--metadata_path\", type=str)\r\n",
      "  parser.add_argument(\"--output_path\", type=str)\r\n",
      "  parser.add_argument(\"--max_steps\", type=int, default=2000)\r\n",
      "  parser.add_argument(\"--hidden1\", type=int, default=300)\r\n",
      "  parser.add_argument(\"--hidden2\", type=int, default=200)\r\n",
      "  parser.add_argument(\"--hidden3\", type=int, default=100)\r\n",
      "  \"\"\"Train taxifare for a number of steps.\"\"\"\r\n",
      "  # test on taxifare.\r\n",
      "      _, train_examples = taxifare.read_examples(\r\n",
      "          taxifare.create_inputs(metadata, input_data=train_examples))\r\n",
      "      output = taxifare.inference(inputs, metadata, layer_sizes)\r\n",
      "      loss = taxifare.loss(output, targets)\r\n",
      "      train_op, global_step = taxifare.training(loss,\r\n",
      "    placeholder, inputs, _, keys = taxifare.create_inputs(metadata)\r\n",
      "    output = taxifare.inference(inputs, metadata, layer_sizes)\r\n",
      "    _, examples = taxifare.read_examples(\r\n",
      "        taxifare.create_inputs(metadata, input_data=examples))\r\n",
      "    output = taxifare.inference(inputs, metadata, layer_sizes)\r\n",
      "    loss = taxifare.loss(output, targets)\r\n"
     ]
    }
   ],
   "source": [
    "!grep -E \"add_argument|taxifare\" taxifare/trainer/task.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third one is taxifare.py -- this is the real TensorFlow model and the only one for which you have work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Implementing TensorFlow model </h2>\n",
    "\n",
    "Here are the methods in taxifare.py that get called from task.py. It's your job to implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def read_examples(input_files, batch_size, shuffle, num_epochs=None):\r\n",
      "def create_inputs(metadata, input_data=None):\r\n",
      "def inference(inputs, metadata, hyperparams):\r\n",
      "def loss(output, targets):\r\n",
      "def training(loss, learning_rate):\r\n"
     ]
    }
   ],
   "source": [
    "!grep def taxifare/trainer/taxifare.py | grep -v \"def _\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the loss function for example.  This should feel familiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def loss(output, targets):\r\n",
      "  \"\"\"Calculates the loss from the output and the labels.\r\n",
      "  Args:\r\n",
      "    output: output layer tensor, float - [batch_size].\r\n",
      "    targets: Target value tensor, float - [batch_size].\r\n",
      "  Returns:\r\n",
      "    loss: Loss tensor of type float.\r\n",
      "  \"\"\"\r\n",
      "  loss = tf.sqrt(tf.reduce_mean(tf.square(output - targets)), name = 'loss') # RMSE\r\n",
      "  return loss\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep -A 10 \"def loss\" taxifare/trainer/taxifare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the cell above to look at the other functions.  Essentially, you'll implement your TensorFlow model in terms of these modules (or refactor an existing monolithic TensorFlow model into these modules) and put the pieces in the right spots:\n",
    "<ol>\n",
    "<li> create_inputs will take the input data and do any input transformations that you want to do. </li>\n",
    "<li> inference will create the TensorFlow ML model i.e. the computational graph. </li>\n",
    "<li> loss will specify what you want to optimize </li>\n",
    "<li> training will implement the training loop. You typically don't have to change this from the sample implementation </li>\n",
    "<li> read_examples can also be left as-is unless you want to change what preprocessing outputs or how batching happens (you probably don't). </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running training locally </h2>\n",
    "\n",
    "Once you have a packaged TensorFlow model, you can run training by passing in the paths to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type %ml train into an empty cell, run it, fill in some params and execute it again. (create a new code cell and try it out!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>A training input template is created in next cell for you. See cell input instructions below.</p><table><tr><th>Parameters</th><th>Local Run Required</th><th>Cloud Run Required</th><th>Description</th></tr><tr><td>package_uris</td><td>True</td><td>True</td><td>A GCS or local (for local run only) path to your python training program package.</td></tr><tr><td>python_module</td><td>True</td><td>True</td><td>The module to run.</td></tr><tr><td>scale_tier</td><td>False</td><td>True</td><td>Type of resources requested for the job. On local run, BASIC means 1 master process only, and any other values mean 1 master 1 worker and 1 ps processes. But you can also override the values by setting worker_count and parameter_server_count. On cloud, see service definition for possible values.</td></tr><tr><td>region</td><td>False</td><td>True</td><td>Where the training job runs. For cloud run only.</td></tr><tr><td>args</td><td>False</td><td>False</td><td>Args that will be passed to your training program.</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ml train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%ml train [--cloud]\n",
    "package_uris: gs://your-bucket/my-training-package.tar.gz\n",
    "python_module: your_program.your_module\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  string_arg: value\n",
    "  int_arg: value\n",
    "  appendable_arg:\n",
    "    - value1\n",
    "    - value2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf /content/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master: <br/>master: Step 550: loss = 16.42 (0.042 sec)<br/>master: <br/>master: Step 600: loss = 11.94 (0.042 sec)<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: Final error after 600 steps = 11.091<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:master/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:master/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: inputs  =  [None, 5]<br/>master: Done training.<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%ml train\n",
    "package_uris: /content/CPB102/lab3a/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - /content/CPB102/lab3a/taxi_preproc/features_train\n",
    "  eval_data_paths:\n",
    "    - /content/CPB102/lab3a/taxi_preproc/features_eval\n",
    "  metadata_path: /content/CPB102/lab3a/taxi_preproc/metadata.yaml\n",
    "  output_path: /content/CPB102/lab3a/taxi_trained\n",
    "  max_steps: 600\n",
    "  hidden1: 64\n",
    "  hidden2:  8\n",
    "  hidden3:  4\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval  logdir  model  summaries\r\n"
     ]
    }
   ],
   "source": [
    "!ls /content/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"ee1877d7-fb8c-495b-9c06-f13a21c0712e\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ee1877d7-fb8c-495b-9c06-f13a21c0712e\", [{\"y\": [], \"x\": [], \"type\": \"scatter\", \"name\": \"loss-/content/CPB102/lab3a/taxi_trained/eval\"}, {\"y\": [11.310486793518066, 11.081459999084473], \"x\": [326, 600], \"type\": \"scatter\", \"name\": \"error-/content/CPB102/lab3a/taxi_trained/eval\"}, {\"y\": [16.000383377075195, 14.468507766723633, 13.479462623596191, 15.069684028625488, 11.859210968017578, 15.489897727966309, 13.97226333618164, 15.945327758789062, 13.273407936096191, 13.955723762512207, 13.732272148132324, 13.676315307617188], \"x\": [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600], \"type\": \"scatter\", \"name\": \"loss-/content/CPB102/lab3a/taxi_trained/summaries\"}, {\"y\": [], \"x\": [], \"type\": \"scatter\", \"name\": \"error-/content/CPB102/lab3a/taxi_trained/summaries\"}], {\"title\": \"loss,error\", \"xaxis\": {\"title\": \"step\"}, \"yaxis\": {\"title\": \"loss,error\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%ml summary --dir /content/CPB102/lab3a/taxi_trained/summaries  /content/CPB102/lab3a/taxi_trained/eval  --name loss error --step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is the RMSE on the training dataset; the error is the RMSE on the validation dataset.  The loss is reported frequently since it is computed anyway, but we compute the error only once every 30s of training, so there won't be as many points associated with the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 16178. Click <a href=\"/_proxy/57657/\" target=\"_blank\">here</a> to access it.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorboard start --logdir /content/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%tensorboard stop --pid 16178"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training on cloud </h2>\n",
    "\n",
    "First of all, we need to set permissions on our bucket so that Cloud ML can read/write to it.  In CloudShell, go to CPB102/lab3a and run ./get_service_account.sh.  Use that account in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No changes to gs://cloud-training-demos/\n",
      "No changes to gs://cloud-training-demos/\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "# you can find the Cloud ML service account by running ./get_service_account.sh\n",
    "# change the service account and bucket as appropriate\n",
    "SVCACCT=cloud-ml-service@cml-663413318684.iam.gserviceaccount.com\n",
    "BUCKET=cloud-training-demos\n",
    "gsutil acl ch -u $SVCACCT:WRITE gs://$BUCKET/\n",
    "gsutil defacl ch -u $SVCACCT:O gs://$BUCKET/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to copy the model and data to Google Cloud Storage (GCS).  Change bucket name as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/\n",
      "taxifare/PKG-INFO\n",
      "taxifare/setup.cfg\n",
      "taxifare/setup.py\n",
      "taxifare/trainer/\n",
      "taxifare/trainer/__init__.py\n",
      "taxifare/trainer/task.py\n",
      "taxifare/trainer/taxifare.py\n",
      "taxifare/trainer.egg-info/\n",
      "taxifare/trainer.egg-info/dependency_links.txt\n",
      "taxifare/trainer.egg-info/PKG-INFO\n",
      "taxifare/trainer.egg-info/SOURCES.txt\n",
      "taxifare/trainer.egg-info/top_level.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://taxifare.tar.gz [Content-Type=application/x-tar]...\n",
      "Uploading   ...d-training-demos/taxifare/source/taxifare.tar.gz: 0 B/6.42 KiB    \r",
      "Uploading   ...d-training-demos/taxifare/source/taxifare.tar.gz: 6.42 KiB/6.42 KiB    \r\n",
      "Copying file://../lab1a/taxi-test.csv [Content-Type=text/csv]...\n",
      "Uploading   ...loud-training-demos/taxifare/input/taxi-test.csv: 0 B/79.68 KiB    \r",
      "Uploading   ...loud-training-demos/taxifare/input/taxi-test.csv: 71.48 KiB/79.68 KiB    \r",
      "Uploading   ...loud-training-demos/taxifare/input/taxi-test.csv: 79.68 KiB/79.68 KiB    \r\n",
      "Copying file://../lab1a/taxi-train.csv [Content-Type=text/csv]...\n",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 0 B/370.48 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 71.48 KiB/370.48 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 143.48 KiB/370.48 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 215.48 KiB/370.48 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 287.48 KiB/370.48 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 359.48 KiB/370.48 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-train.csv: 370.48 KiB/370.48 KiB    \r\n",
      "Copying file://../lab1a/taxi-valid.csv [Content-Type=text/csv]...\n",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-valid.csv: 0 B/79.46 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-valid.csv: 71.48 KiB/79.46 KiB    \r",
      "Uploading   ...oud-training-demos/taxifare/input/taxi-valid.csv: 79.46 KiB/79.46 KiB    \r\n",
      "CommandException: 1 files/objects could not be removed.\n",
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "BUCKET=cloud-training-demos\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare\n",
    "gsutil cp taxifare.tar.gz gs://$BUCKET/taxifare/source/taxifare.tar.gz\n",
    "gsutil cp ../lab1a/*.csv  gs://$BUCKET/taxifare/input/\n",
    "gsutil -m rm -r -f gs://$BUCKET/taxifare/taxi_preproc\n",
    "gsutil -m rm -r -f gs://$BUCKET/taxifare/taxi_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run your preprocessor, you have to change the input and output to be on GCS.  \n",
    "\n",
    "Using DirectPipelineRunner runs Dataflow locally, but the inputs & outputs are on the cloud. Using BlockingDataflowPipelineRunner will use Cloud Dataflow (and take much longer because of the overhead involved for such a small dataset). To see the status of your BlockingDataflowPipelineRunner job, visit https://console.cloud.google.com/dataflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Using fallback coder for typehint: Union[].\n",
      "WARNING:root:Using fallback coder for typehint: Union[].\n",
      "WARNING:root:Using fallback coder for typehint: Union[].\n",
      "WARNING:root:Using fallback coder for typehint: Union[].\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.dataflow.io.tfrecordio as tfrecordio\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "\n",
    "# Change as needed\n",
    "BUCKET = 'cloud-training-demos'\n",
    "PROJECT = 'cloud-training-demos'\n",
    "RUNNER = 'DirectPipelineRunner'  # RUNNER = 'BlockingDataflowPipelineRunner'\n",
    "\n",
    "# defines\n",
    "feature_set = TaxifareFeatures()\n",
    "OUTPUT_DIR = 'gs://{0}/taxifare/taxi_preproc'.format(BUCKET)\n",
    "\n",
    "pipeline = beam.Pipeline(argv=['--project', PROJECT,\n",
    "                               '--runner', RUNNER,\n",
    "                               '--job_name', 'lab3a',\n",
    "                               '--extra_package', ml.sdk_location,\n",
    "                               '--no_save_main_session', 'True',  # to prevent pickling and uploading Datalab itself!\n",
    "                               '--staging_location', 'gs://{0}/taxifare/staging'.format(BUCKET),\n",
    "                               '--temp_location', 'gs://{0}/taxifare/temp'.format(BUCKET)])\n",
    "\n",
    "# preprocessing\n",
    "training_data = beam.io.TextFileSource(\n",
    "    'gs://{0}/taxifare/input/taxi-train.csv'.format(BUCKET),\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "train = pipeline | beam.Read('ReadTrainingData', training_data)\n",
    "eval_data = beam.io.TextFileSource(\n",
    "    'gs://{0}/taxifare/input/taxi-valid.csv'.format(BUCKET),\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "eval = pipeline | beam.Read('ReadEvalData', eval_data)\n",
    "(metadata, train_features, eval_features) = ((train, eval) |\n",
    "    ml.Preprocess('Preprocess', feature_set))\n",
    "train_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix=os.path.join(OUTPUT_DIR, 'features_train'),\n",
    "    file_name_suffix='',\n",
    "    shard_file=False,\n",
    "    compress_file=True)\n",
    "eval_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix=os.path.join(OUTPUT_DIR, 'features_eval'),\n",
    "    file_name_suffix='',\n",
    "    shard_file=False,\n",
    "    compress_file=True)\n",
    "(metadata, train_features, eval_features) | (\n",
    "    io.SavePreprocessed('SavingData', OUTPUT_DIR,\n",
    "                        file_parameters_list=[\n",
    "                            os.path.join(OUTPUT_DIR, 'metadata.yaml'),\n",
    "                            train_parameters, eval_parameters]))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos/taxifare/taxi_preproc/features_eval\n",
      "gs://cloud-training-demos/taxifare/taxi_preproc/features_train\n",
      "gs://cloud-training-demos/taxifare/taxi_preproc/info\n",
      "gs://cloud-training-demos/taxifare/taxi_preproc/metadata.yaml\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls gs://cloud-training-demos/taxifare/taxi_preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, submit the training job to the cloud.  Note that unlike Dataflow jobs (which usually take minutes), Cloud ML jobs usually take hours and are, therefore, queued. It may be a couple of minutes before your job starts being executed. This being a small job, though, the task should complete a few seconds later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job \"trainer_task_160916_233834\" was submitted successfully.<br/>Run \"%ml jobs --name trainer_task_160916_233834\" to view the status of the job.</p><p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=cloud-training-demos&resource=ml.googleapis.com%2Fjob_id%2Ftrainer_task_160916_233834\" target=\"_blank\">here</a> to view cloud log. <br/>Start TensorBoard by running \"%tensorboard start --logdir=&lt;YourLogDir&gt;\".</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ml train --cloud\n",
    "package_uris: gs://cloud-training-demos/taxifare/source/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - gs://cloud-training-demos/taxifare/taxi_preproc/features_train\n",
    "  eval_data_paths:\n",
    "    - gs://cloud-training-demos/taxifare/taxi_preproc/features_eval\n",
    "  metadata_path: gs://cloud-training-demos/taxifare/taxi_preproc/metadata.yaml\n",
    "  output_path: gs://cloud-training-demos/taxifare/taxi_trained\n",
    "  max_steps: 1000\n",
    "  hidden1: 64\n",
    "  hidden2:  8\n",
    "  hidden3:  4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prediction </h2>\n",
    "\n",
    "Make sure that the training job has completed before proceeding to this step (check the log above)\n",
    "\n",
    "To predict the taxifare for new inputs, you first have to deploy the trained model (deleting a previous one if necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ml delete --name taxifare.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%ml deploy --name taxifare.v1 --path gs://cloud-training-demos/taxifare/taxi_trained/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response={u'error': u'Prediction failed: Unable to get element from the feed as bytes.'}\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "import google.cloud.ml.features as features\n",
    "from google.cloud.ml import session_bundle\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1beta1', credentials=credentials,\n",
    "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1beta1_discovery.json')\n",
    "#request = {'instances': ['-73.885262,40.773008,-73.987232,40.732403,2']}\n",
    "request = {'instances': [\n",
    "    {'pickup_longitude': -73.885262,\n",
    "     'pickup_latitude': 40.773008,\n",
    "     'dropoff_longitude': -73.987232,\n",
    "     'dropoff_latitude': 40.732403,\n",
    "     'passenger_count': 2}]}\n",
    "parent = 'projects/%s/models/%s/versions/%s' % ('cloud-training-demos', 'taxifare', 'v1')\n",
    "response = api.projects().predict(body=request, name=parent).execute()\n",
    "print \"response={0}\".format(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
